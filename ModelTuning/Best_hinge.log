Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Wed Sep 13 13:50:05 2017
Beta: 4
Random Seeds:
randForClassifier: 580
randForSplit: 41

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.87      0.98      0.93       440

avg / total       0.87      0.98      0.93       440

F4: 0.975

['yes', 'no']
[[432   8]
 [ 62 826]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.93      0.78       123

avg / total       0.68      0.93      0.78       123

F4: 0.907

['yes', 'no']
[[114   9]
 [ 54 156]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Wed Sep 13 13:51:44 2017
Beta: 4
Random Seeds:
randForClassifier: 866
randForSplit: 52

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.90      0.99      0.94       461

avg / total       0.90      0.99      0.94       461

Train F4: 0.981

['yes', 'no']
[[455   6]
 [ 53 814]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.85      0.70       102

avg / total       0.59      0.85      0.70       102

Test  F4: 0.831

['yes', 'no']
[[ 87  15]
 [ 61 170]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Wed Sep 13 15:34:37 2017
Beta: 4
Random Seeds:
randForClassifier: 587
randForSplit: 365

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.88      0.99      0.93       457

avg / total       0.88      0.99      0.93       457

Train F4: 0.980

['yes', 'no']
[[451   6]
 [ 59 812]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.63      0.93      0.75       106

avg / total       0.63      0.93      0.75       106

Test  F4: 0.908

['yes', 'no']
[[ 99   7]
 [ 59 168]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Wed Sep 13 15:39:13 2017
Beta: 4
Random Seeds:
randForClassifier: 748
randForSplit: 590

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.91      0.99      0.95       466

avg / total       0.91      0.99      0.95       466

Train F4: 0.983

['yes', 'no']
[[460   6]
 [ 43 819]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.87      0.72        97

avg / total       0.61      0.87      0.72        97

Test  F4: 0.845

['yes', 'no']
[[ 84  13]
 [ 53 183]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 15:40:58 2017
Beta: 4
Random Seeds:
randForClassifier: 978
randForSplit: 63

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.87      0.99      0.93       438

avg / total       0.87      0.99      0.93       438

Train F4: 0.983

['yes', 'no']
[[434   4]
 [ 66 824]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.70      0.95      0.80       125

avg / total       0.70      0.95      0.80       125

Test  F4: 0.932

['yes', 'no']
[[119   6]
 [ 52 156]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 15:43:43 2017
Beta: 4
Random Seeds:
randForClassifier: 349
randForSplit: 969

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.88      0.99      0.94       455

avg / total       0.88      0.99      0.94       455

Train F4: 0.986

['yes', 'no']
[[452   3]
 [ 59 814]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.70      0.88      0.78       108

avg / total       0.70      0.88      0.78       108

Test  F4: 0.867

['yes', 'no']
[[ 95  13]
 [ 40 185]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### Top positive features (20)
+0.00	miRNA seq
+0.00	microRNA seq
+0.00	terms
+0.00	conditions refer
+0.00	data usage
+0.00	data usage terms
+0.00	terms conditions
+0.00	terms conditions refer
+0.00	usage terms
+0.00	usage terms conditions
+0.00	refer
+0.00	day mouse
+0.00	embryonic day
+0.00	postnatal
+0.00	usage
+0.00	development
+0.00	miRNA seq embryonic
+0.00	seq embryonic day
+0.00	seq embryonic
+0.00	seq

### Top negative features (20)
-0.00	activated
-0.00	lines
-0.00	vitro
-0.00	hours
-0.00	following
-0.00	primary
-0.00	transgenic
-0.00	infection
-0.00	stem cells
-0.00	infected
-0.00	cell
-0.00	stem
-0.00	treatment
-0.00	induced
-0.00	mouse embryonic fibroblasts
-0.00	mouse embryonic
-0.00	treated
-0.00	embryonic fibroblasts
-0.00	fibroblasts
-0.00	cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=349, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x109e96de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['invscaling', 'optimal', 'constant']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20125
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability phosphorylate']

Middle 10 features: [u'lesions', u'lethal', u'lethal mice', u'lethal mouse', u'lethal mouse gene', u'lethality', u'leucine', u'leucine rich', u'leucine rich proteoglycan', u'leucine zipper']

Last 10 features: [u'zone mouse', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 40
E-GEOD-61205
E-SMDB-3976
E-MTAB-3112
E-CBIL-39
E-GEOD-51255

### False negatives: 13
E-GEOD-27309
E-MTAB-3707
E-GEOD-45278
E-GEOD-61582
E-GEOD-11484

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           455	Yes count:       108
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     32%

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 15:52:19 2017
Beta: 4
Random Seeds:
randForClassifier: 610
randForSplit: 176

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.96      0.85       440

avg / total       0.77      0.96      0.85       440

Train F4: 0.943

['yes', 'no']
[[421  19]
 [128 760]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.93      0.74       123

avg / total       0.62      0.93      0.74       123

Test  F4: 0.900

['yes', 'no']
[[114   9]
 [ 71 139]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 1e-05
classifier__learning_rate: 'constant'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### Top positive features (20)
+0.00	terms
+0.00	conditions refer
+0.00	data usage
+0.00	data usage terms
+0.00	terms conditions
+0.00	terms conditions refer
+0.00	usage terms
+0.00	usage terms conditions
+0.00	day mouse
+0.00	refer
+0.00	miRNA seq
+0.00	microRNA seq
+0.00	embryonic day mouse
+0.00	miRNA seq embryonic
+0.00	seq embryonic day
+0.00	development
+0.00	embryonic day
+0.00	seq
+0.00	microRNA
+0.00	postnatal

### Top negative features (20)
-0.00	shared including details
-0.00	use pre
-0.00	use pre publication
-0.00	pre publication
-0.00	treated
-0.00	data pre
-0.00	data pre publication
-0.00	pre publication release
-0.00	publication release
-0.00	unclear
-0.00	release
-0.00	profiling mouse embryonic
-0.00	cell
-0.00	induced
-0.00	stem
-0.00	embryonic fibroblasts
-0.00	mouse embryonic fibroblasts
-0.00	mouse embryonic
-0.00	fibroblasts
-0.00	cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=610, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10d85ede8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['invscaling', 'optimal', 'constant']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20907
First 10 features: [u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro']

Middle 10 features: [u'largely unexplored', u'largely unknown', u'largely unknown identify', u'larger', u'largest', u'laser', u'laser capture', u'laser capture microdissected', u'laser capture microdissection', u'laser scanner']

Last 10 features: [u'zone', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 71
E-CBIL-39
E-GEOD-51255
E-MTAB-2905
E-GEOD-57419
E-MTAB-3938

### False negatives: 9
E-GEOD-61582
E-GEOD-12008
E-GEOD-12618
E-ERAD-237
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           440	Yes count:       123
No  count:     1098	No  count:           888	No  count:       210
Percent Yes:    33%	Percent Yes:         33%	Percent Yes:     36%

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 16:48:41 2017
Beta: 4
Random Seeds:
randForClassifier: 567
randForSplit: 128

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.66      0.96      0.78       447

avg / total       0.66      0.96      0.78       447

Train F4: 0.939

['yes', 'no']
[[431  16]
 [223 658]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.52      0.93      0.66       116

avg / total       0.52      0.93      0.66       116

Test  F4: 0.889

['yes', 'no']
[[108   8]
 [101 116]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=567, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10aec1de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['invscaling', 'optimal', 'constant']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 16:54:11 2017
Beta: 4
Random Seeds:
randForClassifier: 564
randForSplit: 572

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.71      0.91      0.80       455

avg / total       0.71      0.91      0.80       455

Train F4: 0.899

['yes', 'no']
[[416  39]
 [174 699]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.83      0.71       108

avg / total       0.62      0.83      0.71       108

Test  F4: 0.817

['yes', 'no']
[[ 90  18]
 [ 54 171]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=564, shuffle=True,
       verbose=0, warm_start=False)

vectorizer:
CountVectorizer(analyzer=u'word', binary=True, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10c71dde8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['invscaling', 'optimal', 'constant']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 16 candidates, totalling 80 fits
### Tuning Report Wed Sep 13 17:03:43 2017
Beta: 4
Random Seeds:
randForClassifier: 309
randForSplit: 696

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.73      0.97      0.83       445

avg / total       0.73      0.97      0.83       445

Train F4: 0.952

['yes', 'no']
[[432  13]
 [162 721]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.94      0.71       118

avg / total       0.58      0.94      0.71       118

Test  F4: 0.907

['yes', 'no']
[[111   7]
 [ 82 133]]

### Best Pipeline Parameters:
classifier__alpha: 10
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=10, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=309, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x112c9ede8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[10, 100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 20 candidates, totalling 100 fits
### Tuning Report Wed Sep 13 17:05:26 2017
Beta: 4
Random Seeds:
randForClassifier: 991
randForSplit: 945

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.70      0.96      0.81       453

avg / total       0.70      0.96      0.81       453

Train F4: 0.940

['yes', 'no']
[[435  18]
 [187 688]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.54      0.95      0.69       110

avg / total       0.54      0.95      0.69       110

Test  F4: 0.905

['yes', 'no']
[[104   6]
 [ 89 134]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=991, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10cfc1de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1, 10, 100, 1000, 10000]
classifier__eta0:[1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 36 candidates, totalling 180 fits
### Tuning Report Wed Sep 13 17:10:09 2017
Beta: 4
Random Seeds:
randForClassifier: 463
randForSplit: 260

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.72      0.97      0.83       442

avg / total       0.72      0.97      0.83       442

Train F4: 0.947

['yes', 'no']
[[427  15]
 [165 721]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.95      0.71       121

avg / total       0.56      0.95      0.71       121

Test  F4: 0.913

['yes', 'no']
[[115   6]
 [ 90 122]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=463, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10b62ede8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1, 10, 100, 1000]
classifier__eta0:[1e-05, 0.0001, 0.001, 0.01, 0.1, 1]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 9 candidates, totalling 45 fits
### Tuning Report Wed Sep 13 17:16:18 2017
Beta: 4
Random Seeds:
randForClassifier: 403
randForSplit: 893

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.67      0.98      0.79       466

avg / total       0.67      0.98      0.79       466

Train F4: 0.952

['yes', 'no']
[[456  10]
 [229 633]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.46      0.97      0.62        97

avg / total       0.46      0.97      0.62        97

Test  F4: 0.910

['yes', 'no']
[[ 94   3]
 [111 125]]

### Best Pipeline Parameters:
classifier__alpha: 5
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=5, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=403, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x109aa5de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1, 5]
classifier__eta0:[0.005, 0.01, 0.5]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 9 candidates, totalling 45 fits
### Tuning Report Wed Sep 13 17:19:00 2017
Beta: 3
Random Seeds:
randForClassifier: 934
randForSplit: 9

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.67      0.96      0.79       456

avg / total       0.67      0.96      0.79       456

Train F3: 0.925

['yes', 'no']
[[440  16]
 [213 659]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.53      0.92      0.67       107

avg / total       0.53      0.92      0.67       107

Test  F3: 0.854

['yes', 'no']
[[ 98   9]
 [ 87 139]]

### Best Pipeline Parameters:
classifier__alpha: 5
classifier__eta0: 0.005
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=5, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.005, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=934, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10b8d7de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1, 5]
classifier__eta0:[0.005, 0.01, 0.5]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


Fitting 5 folds for each of 9 candidates, totalling 45 fits
### Tuning Report Wed Sep 13 17:21:22 2017
Beta: 3
Random Seeds:
randForClassifier: 337
randForSplit: 519

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.96      0.85       456

avg / total       0.76      0.96      0.85       456

Train F3: 0.935

['yes', 'no']
[[438  18]
 [142 730]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.55      0.91      0.68       107

avg / total       0.55      0.91      0.68       107

Test  F3: 0.851

['yes', 'no']
[[ 97  10]
 [ 80 146]]

### Best Pipeline Parameters:
classifier__alpha: 0.5
classifier__eta0: 0.005
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.5, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.005, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=337, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x1136efde8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1, 5]
classifier__eta0:[0.005, 0.01, 0.5]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.03	terms
+0.02	conditions refer
+0.02	data usage
+0.02	data usage terms
+0.02	terms conditions
+0.02	terms conditions refer
+0.02	usage terms
+0.02	usage terms conditions
+0.02	refer
+0.02	day mouse
+0.02	miRNA seq
+0.02	microRNA seq
+0.02	embryonic day mouse
+0.01	miRNA seq embryonic
+0.01	seq embryonic day
+0.01	development
+0.01	knockout
+0.01	seq
+0.01	embryonic day
+0.01	mice

### Top negative features (20)
-0.01	pre publication data
-0.01	pre publication release
-0.01	proper use pre
-0.01	publication data
-0.01	publication data shared
-0.01	publication moratoria
-0.01	publication release
-0.01	publication release information
-0.01	release information
-0.01	release information proper
-0.01	shared including details
-0.01	use pre
-0.01	use pre publication
-0.01	induced
-0.01	stem
-0.02	embryonic fibroblasts
-0.02	mouse embryonic fibroblasts
-0.02	mouse embryonic
-0.02	fibroblasts
-0.02	cells

### Vectorizer:   Number of Features: 20252
First 10 features: [u'a2', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate']

Middle 10 features: [u'leukemia', u'leukemia cells', u'leukemia clinically', u'leukemia clinically significant', u'leukemia elderly', u'leukemias', u'leukemic', u'leukemic cells', u'leukemic transformation', u'leukemic transformation data']

Last 10 features: [u'zone mouse', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 80
E-GEOD-50189
E-GEOD-6846
E-CBIL-19
E-GEOD-765
E-GEOD-78205

### False negatives: 10
E-GEOD-59777
E-ERAD-231
E-MTAB-5224
E-GEOD-11484
E-GEOD-15794

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           456	Yes count:       107
No  count:     1098	No  count:           872	No  count:       226
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     32%

Fitting 5 folds for each of 9 candidates, totalling 45 fits
### Tuning Report Wed Sep 13 17:22:55 2017
Beta: 4
Random Seeds:
randForClassifier: 21
randForSplit: 277

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.75      0.97      0.84       441

avg / total       0.75      0.97      0.84       441

Train F4: 0.952

['yes', 'no']
[[427  14]
 [143 744]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.96      0.75       122

avg / total       0.62      0.96      0.75       122

Test  F4: 0.929

['yes', 'no']
[[117   5]
 [ 71 140]]

### Best Pipeline Parameters:
classifier__alpha: 0.5
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.5, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=21, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10dd43de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1, 5]
classifier__eta0:[0.005, 0.01, 0.5]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.06	terms
+0.05	conditions refer
+0.05	data usage
+0.05	data usage terms
+0.05	terms conditions
+0.05	terms conditions refer
+0.05	usage terms
+0.05	usage terms conditions
+0.05	refer
+0.05	usage
+0.05	day mouse
+0.05	miRNA seq
+0.05	microRNA seq
+0.03	embryonic day mouse
+0.03	miRNA seq embryonic
+0.03	seq embryonic day
+0.03	seq
+0.02	development
+0.02	embryonic day
+0.02	seq embryonic

### Top negative features (20)
-0.01	use pre publication
-0.01	treated
-0.01	publication
-0.01	details
-0.01	cells treated
-0.01	profiling mouse embryonic
-0.01	pre publication
-0.02	data pre
-0.02	data pre publication
-0.02	pre publication release
-0.02	publication release
-0.02	cell
-0.02	unclear
-0.02	stem
-0.02	induced
-0.02	embryonic fibroblasts
-0.03	mouse embryonic fibroblasts
-0.03	mouse embryonic
-0.03	fibroblasts
-0.04	cells

### Vectorizer:   Number of Features: 20175
First 10 features: [u'aRNA', u'abdominal', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated', u'ablation']

Middle 10 features: [u'large number downregulated', u'large number genes', u'large scale', u'largely', u'largely conserved', u'largely restored', u'largely restored absence', u'largely unexplored', u'largely unknown', u'largely unknown identify']

Last 10 features: [u'zone', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 71
E-GEOD-50617
E-GEOD-56798
E-GEOD-64076
E-MARS-8
E-GEOD-49975

### False negatives: 5
E-GEOD-8360
E-ERAD-381
E-GEOD-6589
E-GEOD-69124
E-ERAD-433

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           441	Yes count:       122
No  count:     1098	No  count:           887	No  count:       211
Percent Yes:    33%	Percent Yes:         33%	Percent Yes:     36%

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Tuning Report Wed Sep 13 17:33:57 2017
Beta: 4
Random Seeds:
randForClassifier: 54
randForSplit: 740

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.64      0.97      0.77       452

avg / total       0.64      0.97      0.77       452

Train F4: 0.939

['yes', 'no']
[[437  15]
 [246 630]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.49      0.95      0.65       111

avg / total       0.49      0.95      0.65       111

Test  F4: 0.897

['yes', 'no']
[[105   6]
 [108 114]]

### Best Pipeline Parameters:
classifier__alpha: 5
classifier__eta0: 0.005
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=5, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.005, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=54, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x112996de8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1, 5]
classifier__eta0:[0.005, 0.01, 0.05]
classifier__learning_rate:['invscaling', 'optimal', 'constant']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.01	terms
+0.01	conditions refer
+0.01	data usage
+0.01	data usage terms
+0.01	terms conditions
+0.01	terms conditions refer
+0.01	usage terms
+0.01	usage terms conditions
+0.01	refer
+0.01	day mouse
+0.01	miRNA seq
+0.01	microRNA seq
+0.01	embryonic day
+0.01	embryonic day mouse
+0.01	miRNA seq embryonic
+0.01	seq embryonic day
+0.00	development
+0.00	seq
+0.00	tissue
+0.00	liver

### Top negative features (20)
-0.00	pre publication
-0.00	pre publication data
-0.00	pre publication release
-0.00	proper use pre
-0.00	publication data
-0.00	publication data shared
-0.00	publication moratoria
-0.00	publication release
-0.00	publication release information
-0.00	release information
-0.00	release information proper
-0.00	shared including details
-0.00	use pre
-0.00	use pre publication
-0.00	stem
-0.00	embryonic fibroblasts
-0.00	mouse embryonic
-0.00	mouse embryonic fibroblasts
-0.01	fibroblasts
-0.01	cells

### Vectorizer:   Number of Features: 19967
First 10 features: [u'a2', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate']

Middle 10 features: [u'known purified', u'known purified cells', u'known regulate', u'known regulated', u'known regulation', u'known role', u'known roles', u'ko', u'ko ko', u'ko mice']

Last 10 features: [u'zone mouse', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 108
E-MTAB-1391
E-JJRD-1
E-GEOD-59674
E-GEOD-19732
E-GEOD-22946

### False negatives: 6
E-GEOD-8360
E-GEOD-15794
E-GEOD-42688
E-ERAD-433
E-ERAD-169

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           452	Yes count:       111
No  count:     1098	No  count:           876	No  count:       222
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     33%

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Tuning Report Wed Sep 13 17:46:22 2017
Beta: 4
Random Seeds:
randForClassifier: 491
randForSplit: 310

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.98      0.81       446

avg / total       0.69      0.98      0.81       446

Train F4: 0.952

['yes', 'no']
[[435  11]
 [194 688]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.96      0.73       117

avg / total       0.60      0.96      0.73       117

Test  F4: 0.924

['yes', 'no']
[[112   5]
 [ 76 140]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=491, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10ba3dde8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1]
classifier__eta0:[0.01]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.03	terms
+0.03	conditions refer
+0.03	data usage
+0.03	data usage terms
+0.03	terms conditions
+0.03	terms conditions refer
+0.03	usage terms
+0.03	usage terms conditions
+0.03	refer
+0.03	usage
+0.03	day mouse
+0.03	miRNA seq
+0.03	microRNA seq
+0.02	embryonic day mouse
+0.02	miRNA seq embryonic
+0.02	seq embryonic day
+0.02	development
+0.02	embryonic day
+0.01	seq
+0.01	microRNA

### Top negative features (20)
-0.01	publication data
-0.01	publication data shared
-0.01	publication moratoria
-0.01	publication release
-0.01	publication release information
-0.01	release information
-0.01	release information proper
-0.01	shared including details
-0.01	use pre
-0.01	use pre publication
-0.01	details
-0.01	sorted
-0.01	induced
-0.02	treated
-0.02	stem
-0.02	embryonic fibroblasts
-0.02	mouse embryonic
-0.02	mouse embryonic fibroblasts
-0.03	cells
-0.03	fibroblasts

### Vectorizer:   Number of Features: 20674
First 10 features: [u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate']

Middle 10 features: [u'learning', u'learning memory', u'lectin', u'lectin staining', u'lectin staining used', u'led', u'led dramatic', u'led precocious', u'led precocious differentiation', u'left']

Last 10 features: [u'zone mouse', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 76
E-GEOD-77846
E-GEOD-765
E-GEOD-53760
E-GEOD-25828
E-GEOD-4664

### False negatives: 5
E-ERAD-433
E-GEOD-9954
E-GEOD-8969
E-GEOD-51932
E-GEOD-61499

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           446	Yes count:       117
No  count:     1098	No  count:           882	No  count:       216
Percent Yes:    33%	Percent Yes:         33%	Percent Yes:     35%

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Tuning Report Wed Sep 13 17:46:56 2017
Beta: 4
Random Seeds:
randForClassifier: 916
randForSplit: 408

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.97      0.78       453

avg / total       0.65      0.97      0.78       453

Train F4: 0.942

['yes', 'no']
[[439  14]
 [235 640]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.53      0.92      0.67       110

avg / total       0.53      0.92      0.67       110

Test  F4: 0.880

['yes', 'no']
[[101   9]
 [ 91 132]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=916, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10b0fade8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1]
classifier__eta0:[0.01]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.03	terms
+0.03	conditions refer
+0.03	data usage
+0.03	data usage terms
+0.03	terms conditions
+0.03	terms conditions refer
+0.03	usage terms
+0.03	usage terms conditions
+0.03	day mouse
+0.03	refer
+0.03	miRNA seq
+0.03	microRNA seq
+0.02	embryonic day mouse
+0.02	miRNA seq embryonic
+0.02	seq embryonic day
+0.02	development
+0.02	embryonic day
+0.01	seq
+0.01	profiling mouse
+0.01	postnatal day mouse

### Top negative features (20)
-0.01	publication release information
-0.01	release information
-0.01	release information proper
-0.01	shared including
-0.01	shared including details
-0.01	use pre
-0.01	use pre publication
-0.01	details
-0.01	pre publication
-0.01	data pre
-0.01	data pre publication
-0.01	pre publication release
-0.01	publication release
-0.01	induced
-0.02	embryonic fibroblasts
-0.02	stem
-0.02	mouse embryonic fibroblasts
-0.02	mouse embryonic
-0.02	fibroblasts
-0.02	cells

### Vectorizer:   Number of Features: 20549
First 10 features: [u'a2', u'aRNA', u'abdominal', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated']

Middle 10 features: [u'leaded', u'leaded removing', u'leaded removing growth', u'leading', u'leading cause', u'leading cause cancer', u'leading cytokine', u'leading cytokine dependent', u'leads', u'leads activation']

Last 10 features: [u'zipper', u'zona', u'zone', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic']

### False positives: 91
E-GEOD-4512
E-GEOD-15119
E-GEOD-66211
E-GEOD-4711
E-SMDB-3976

### False negatives: 9
E-ERAD-272
E-ERAD-169
E-GEOD-59127
E-ERAD-520
E-GEOD-12618

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           453	Yes count:       110
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     33%

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Tuning Report Wed Sep 13 17:47:44 2017
Beta: 4
Random Seeds:
randForClassifier: 551
randForSplit: 115

### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.96      0.80       447

avg / total       0.69      0.96      0.80       447

Train F4: 0.942

['yes', 'no']
[[431  16]
 [195 686]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.54      0.97      0.70       116

avg / total       0.54      0.97      0.70       116

Test  F4: 0.930

['yes', 'no']
[[113   3]
 [ 96 121]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=551, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3),
        preprocessor=<function vectorizer_preprocessor at 0x10f0dbde8>,
        stop_words='english', strip_accents='unicode',
        token_pattern=u'(?u)\\b([a-z_]\\w+)\\b', tokenizer=None,
        vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.5, 1]
classifier__eta0:[0.01]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]


