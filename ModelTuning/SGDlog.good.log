Fitting 5 folds for each of 24 candidates, totalling 120 fits
### Start Time Tue Sep 19 19:40:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=859	randForSplit=647	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.85       417

avg / total       0.76      0.98      0.85       417

Train F2: 0.926

['yes', 'no']
[[409   8]
 [132 743]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.92      0.65       100

avg / total       0.51      0.92      0.65       100

Test  F2: 0.792

['yes', 'no']
[[ 92   8]
 [ 89 134]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.1
classifier__learning_rate: 'invscaling'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.1, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=859, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0595	tissu
+0.0552	liver
+0.0523	mutant
+0.0502	defect
+0.0490	profil mous
+0.0480	knockout
+0.0446	development
+0.0442	genotyp
+0.0436	pool
+0.0436	mice
+0.0419	conclus
+0.0415	litterm
+0.0409	heart
+0.0407	brain
+0.0399	design
+0.0397	postnat
+0.0392	mice studi
+0.0386	embryon day
+0.0383	dissect

### Top negative features (20)
-0.0383	activ
-0.0405	embryon
-0.0411	cell treat
-0.0413	transgen
-0.0465	differenti
-0.0466	primari
-0.0470	treat
-0.0476	sort
-0.0478	inject
-0.0510	treatment
-0.0510	follow
-0.0619	stem
-0.0653	stem cell
-0.0829	cell
-0.0832	embryon fibroblast
-0.0909	induc
-0.0916	fibroblast mef
-0.0944	fibroblast
-0.0985	mous embryon
-0.1042	mef

### Vectorizer:   Number of Features: 16995
First 10 features: [u'a2', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr transcript', u'abil', u'abil activ']

Middle 10 features: [u'ko vs', u'ko wild', u'ko wildtyp', u'ko wt', u'kras', u'krasg12d', u'krasg12d activ', u'krt14', u'krt14 cre', u'kruppel']

Last 10 features: [u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zygot']

### False positives: 89
E-SMDB-3457
E-GEOD-29632
E-UMCU-22
E-GEOD-61205
E-CBIL-4

### False negatives: 8
E-GEOD-14605
E-GEOD-61582
E-GEOD-61367
E-GEOD-27309
E-ERAD-283

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:41:15 2017

### End Time Tue Sep 19 19:42:46 2017

Fitting 5 folds for each of 8 candidates, totalling 40 fits
### Start Time Tue Sep 19 19:48:09 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=452	randForSplit=189	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.73      0.98      0.84       413

avg / total       0.73      0.98      0.84       413

Train F2: 0.918

['yes', 'no']
[[405   8]
 [149 730]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.95      0.72       104

avg / total       0.58      0.95      0.72       104

Test  F2: 0.843

['yes', 'no']
[[ 99   5]
 [ 72 147]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=452, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0939	develop
+0.0707	tissu
+0.0649	genotyp
+0.0636	extens
+0.0628	mutant
+0.0590	dissect
+0.0568	kidney
+0.0543	conclus
+0.0529	pool
+0.0529	development
+0.0524	litterm
+0.0521	develop mous
+0.0519	brain
+0.0514	wild type
+0.0513	liver
+0.0509	wild
+0.0506	mous heart
+0.0480	knockout
+0.0468	defici mice
+0.0461	mutant embryo

### Top negative features (20)
-0.0453	embryon stem
-0.0497	activ
-0.0509	cell treat
-0.0528	unclear
-0.0545	inject
-0.0549	transgen
-0.0575	treat
-0.0585	sort
-0.0599	primari
-0.0618	infect
-0.0652	follow
-0.0776	stem
-0.0820	stem cell
-0.0865	cell
-0.0924	embryon fibroblast
-0.0945	fibroblast mef
-0.0980	induc
-0.1060	fibroblast
-0.1092	mous embryon
-0.1106	mef

### Vectorizer:   Number of Features: 16651
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene']

Middle 10 features: [u'ko condit', u'ko control', u'ko embryo', u'ko il6', u'ko ko', u'ko liver', u'ko mdr2', u'ko mef', u'ko mice', u'ko mous']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 72
E-GEOD-36899
E-GEOD-49122
E-GEOD-47395
E-GEOD-25506
E-GEOD-1052

### False negatives: 5
E-ERAD-231
E-ERAD-169
E-GEOD-27309
E-ERAD-237
E-ERAD-272

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           413	Yes count:       104
No  count:     1098	No  count:           879	No  count:       219
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Tue Sep 19 19:48:49 2017

Fitting 5 folds for each of 48 candidates, totalling 240 fits
### Start Time Tue Sep 19 19:50:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=441	randForSplit=609	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.98      0.78       407

avg / total       0.65      0.98      0.78       407

Train F2: 0.890

['yes', 'no']
[[400   7]
 [218 667]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.95      0.74       110

avg / total       0.60      0.95      0.74       110

Test  F2: 0.855

['yes', 'no']
[[105   5]
 [ 69 144]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=441, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.1028	develop
+0.0917	knockout
+0.0726	tissu
+0.0708	mous
+0.0641	mutant
+0.0633	liver
+0.0549	genotyp
+0.0548	overal
+0.0546	design
+0.0532	defect
+0.0501	brain
+0.0477	gene
+0.0472	litterm
+0.0469	development
+0.0459	exhibit
+0.0458	project
+0.0458	correspond
+0.0457	pool
+0.0456	dnase
+0.0448	dissect

### Top negative features (20)
-0.0482	line
-0.0483	fac
-0.0503	untreat
-0.0507	unclear
-0.0512	c2c12
-0.0513	infect
-0.0529	hour
-0.0537	follow
-0.0540	ani
-0.0543	inject
-0.0557	transgen
-0.0580	sort
-0.0586	cultur
-0.0620	primari
-0.0694	stem
-0.1089	treat
-0.1152	induc
-0.1213	fibroblast
-0.1354	mef
-0.1720	cell

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a10', u'a2', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'lcm', u'lcmv', u'ld', u'ldp', u'lead', u'lean', u'learn', u'leav', u'lectin', u'led']

Last 10 features: [u'yy1', u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 69
E-MTAB-4079
E-GEOD-6223
E-GEOD-70819
E-GEOD-60596
E-ERAD-201

### False negatives: 5
E-GEOD-43517
E-ERAD-72
E-MTAB-3707
E-ERAD-169
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           407	Yes count:       110
No  count:     1098	No  count:           885	No  count:       213
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     34%
### End Time Tue Sep 19 19:51:02 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 19:53:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=904	randForSplit=877	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.98      0.81       417

avg / total       0.69      0.98      0.81       417

Train F2: 0.900

['yes', 'no']
[[407  10]
 [186 689]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.92      0.69       100

avg / total       0.56      0.92      0.69       100

Test  F2: 0.814

['yes', 'no']
[[ 92   8]
 [ 73 150]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=904, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0919	develop
+0.0830	wild
+0.0821	tissu
+0.0749	mutant
+0.0620	knockout
+0.0615	conclus
+0.0606	male
+0.0594	genotyp
+0.0574	design
+0.0565	postnat
+0.0559	overal
+0.0548	defect
+0.0545	liver
+0.0538	litterm
+0.0506	mice
+0.0500	development
+0.0495	dissect
+0.0494	kidney
+0.0491	gene
+0.0485	seq

### Top negative features (20)
-0.0453	stimul
-0.0472	moratoria
-0.0488	unclear
-0.0501	infect
-0.0502	inject
-0.0520	differenti
-0.0526	ani
-0.0526	fac
-0.0551	activ
-0.0579	primari
-0.0595	treat
-0.0597	induct
-0.0610	sort
-0.0633	follow
-0.0660	transgen
-0.0723	stem
-0.0933	cell
-0.1050	induc
-0.1304	mef
-0.1311	fibroblast

### Vectorizer:   Number of Features: 4346
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'lef', u'left', u'leica', u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'leptin', u'lesion']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 73
E-ERAD-201
E-GEOD-5140
E-GEOD-40522
E-GEOD-8549
E-GEOD-21041

### False negatives: 8
E-GEOD-59777
E-GEOD-61582
E-GEOD-2362
E-ERAD-272
E-ERAD-169

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:54:37 2017

Fitting 5 folds for each of 108 candidates, totalling 540 fits
### Start Time Tue Sep 19 19:57:19 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=155	randForSplit=934	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.98      0.89       403

avg / total       0.81      0.98      0.89       403

Train F2: 0.942

['yes', 'no']
[[396   7]
 [ 95 794]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.92      0.74       114

avg / total       0.62      0.92      0.74       114

Test  F2: 0.840

['yes', 'no']
[[105   9]
 [ 64 145]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=155, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2', 'l1']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0658	embryon day
+0.0641	tissu
+0.0625	mutant
+0.0612	pool
+0.0611	liver
+0.0522	dissect
+0.0516	knockout
+0.0508	mous heart
+0.0508	genotyp
+0.0507	male
+0.0496	kidney
+0.0495	brain
+0.0494	postnat
+0.0494	development
+0.0493	mice studi
+0.0488	litterm
+0.0469	correspond
+0.0465	femal
+0.0463	profil mous

### Top negative features (20)
-0.0487	cell treat
-0.0498	inject
-0.0509	hour
-0.0528	transgen
-0.0528	embryon
-0.0530	unclear
-0.0560	primari
-0.0564	treatment
-0.0577	follow
-0.0602	sort
-0.0780	stem
-0.0789	stem cell
-0.0927	cell
-0.0983	induc
-0.1031	treat
-0.1088	fibroblast
-0.1112	fibroblast mef
-0.1187	mous embryon
-0.1200	embryon fibroblast
-0.1251	mef

### Vectorizer:   Number of Features: 16857
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'krt14 cre', u'kruppel', u'kruppel like', u'ksom', u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 64
E-GEOD-1482
E-GEOD-13691
E-GEOD-769
E-GEOD-2130
E-GEOD-66088

### False negatives: 9
E-GEOD-61367
E-GEOD-9913
E-GEOD-59777
E-ERAD-433
E-GEOD-45278

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 19:58:45 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 20:03:01 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=440	randForSplit=448	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.82      0.98      0.89       419

avg / total       0.82      0.98      0.89       419

Train F2: 0.946

['yes', 'no']
[[412   7]
 [ 90 783]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.90      0.75        98

avg / total       0.65      0.90      0.75        98

Test  F2: 0.833

['yes', 'no']
[[ 88  10]
 [ 48 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-06
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-06, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=440, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0889	develop
+0.0712	tissu
+0.0644	mutant
+0.0578	pool
+0.0535	postnat
+0.0525	mutant embryo
+0.0523	brain
+0.0515	genotyp
+0.0498	liver
+0.0497	dissect
+0.0486	extens
+0.0484	embryon day
+0.0476	design
+0.0471	kidney
+0.0459	e12
+0.0457	defici mice
+0.0452	litterm
+0.0448	development
+0.0442	knockout
+0.0441	wild type

### Top negative features (20)
-0.0482	activ
-0.0483	inject
-0.0508	untreat
-0.0510	transgen
-0.0512	cell treat
-0.0517	sort
-0.0552	unclear
-0.0585	treatment
-0.0590	stem
-0.0602	stem cell
-0.0603	primari
-0.0637	follow
-0.0982	induc
-0.1024	embryon fibroblast
-0.1043	treat
-0.1107	fibroblast mef
-0.1202	fibroblast
-0.1203	mous embryon
-0.1372	mef
-0.1613	cell

### Vectorizer:   Number of Features: 16902
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt', u'l5178i', u'l5178i cell', u'la', u'la jolla']

Last 10 features: [u'zipper', u'zona', u'zone', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot']

### False positives: 48
E-MARS-18
E-GEOD-3621
E-GEOD-54678
E-GEOD-57990
E-MTAB-3106

### False negatives: 10
E-GEOD-21860
E-GEOD-22125
E-MTAB-1404
E-ERAD-283
E-GEOD-7342

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:03:45 2017

### End Time Tue Sep 19 20:08:24 2017
### End Time Tue Sep 19 20:12:42 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:13:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=308	randForSplit=566	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.96      0.87       420

avg / total       0.80      0.96      0.87       420

Train F2: 0.926

['yes', 'no']
[[405  15]
 [101 771]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.89      0.76        97

avg / total       0.66      0.89      0.76        97

Test  F2: 0.830

['yes', 'no']
[[ 86  11]
 [ 44 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=308, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05, 0.0001, 0.001, 0.01]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3644	develop
+0.3073	tissu
+0.2563	liver
+0.2430	mutant
+0.2406	heart
+0.2390	postnat
+0.2240	knockout
+0.2125	litterm
+0.2048	kidney
+0.2047	dissect
+0.2019	development
+0.1998	wild
+0.1925	design
+0.1863	overal
+0.1841	e12
+0.1787	consist
+0.1724	bud
+0.1669	pool
+0.1662	genotyp
+0.1621	box

### Top negative features (20)
-0.1679	myoblast
-0.1920	treatment
-0.1925	stimul
-0.1960	infect
-0.2020	fac
-0.2047	unclear
-0.2064	differenti
-0.2181	activ
-0.2271	c2c12
-0.2337	inject
-0.2361	primari
-0.2579	treat
-0.2590	sort
-0.2698	transgen
-0.2764	follow
-0.2922	stem
-0.3963	induc
-0.4142	cell
-0.5183	fibroblast
-0.5604	mef

### Vectorizer:   Number of Features: 4417
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'ablat', u'abnorm']

Middle 10 features: [u'life', u'lifespan', u'lifetim', u'ligand', u'ligas', u'ligat', u'light', u'like', u'likewis', u'lim']

Last 10 features: [u'zero', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 44
E-GEOD-13691
E-GEOD-60780
E-GEOD-59848
E-GEOD-59463
E-MTAB-1569

### False negatives: 11
E-GEOD-59777
E-MTAB-5020
E-ERAD-237
E-GEOD-8969
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           420	Yes count:        97
No  count:     1098	No  count:           872	No  count:       226
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:13:41 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:15:31 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=728	randForSplit=842	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.74       403

avg / total       0.60      0.98      0.74       403

Train F2: 0.867

['yes', 'no']
[[393  10]
 [261 628]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.99      0.73       114

avg / total       0.58      0.99      0.73       114

Test  F2: 0.869

['yes', 'no']
[[113   1]
 [ 81 128]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=728, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1071	develop
+0.0769	tissu
+0.0738	mutant
+0.0695	knockout
+0.0644	defect
+0.0641	heart
+0.0625	exhibit
+0.0607	postnat
+0.0597	design
+0.0581	liver
+0.0575	kidney
+0.0566	conclus
+0.0560	pool
+0.0553	litterm
+0.0539	e12
+0.0536	overal
+0.0535	gene
+0.0532	genotyp
+0.0499	wild
+0.0493	development

### Top negative features (20)
-0.0441	fac
-0.0443	treatment
-0.0445	activ
-0.0455	hour
-0.0465	c2c12
-0.0486	infect
-0.0487	line
-0.0522	unclear
-0.0526	treat
-0.0552	transgen
-0.0567	primari
-0.0588	inject
-0.0608	sort
-0.0621	respons
-0.0770	follow
-0.0789	stem
-0.0955	cell
-0.1010	induc
-0.1263	fibroblast
-0.1423	mef

### Vectorizer:   Number of Features: 4349
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lead', u'lean', u'learn', u'leav', u'led', u'lef', u'left', u'leica', u'len', u'length']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 81
E-MTAB-2266
E-GEOD-12309
E-ERAD-201
E-GEOD-29632
E-CBIL-39

### False negatives: 1
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:15:37 2017

Interesting 60% precision, high recall
Try some more repetitions
Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:17:59 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=492	randForSplit=650	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.68      0.96      0.79       401

avg / total       0.68      0.96      0.79       401

Train F2: 0.884

['yes', 'no']
[[383  18]
 [180 711]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       116

avg / total       0.61      0.89      0.72       116

Test  F2: 0.814

['yes', 'no']
[[103  13]
 [ 66 141]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=492, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0987	develop
+0.0793	tissu
+0.0739	mutant
+0.0634	liver
+0.0617	mice
+0.0572	exhibit
+0.0563	genotyp
+0.0548	knockout
+0.0546	brain
+0.0542	appar
+0.0542	postnat
+0.0533	gene
+0.0520	conclus
+0.0515	development
+0.0512	litter
+0.0511	litterm
+0.0506	dissect
+0.0504	wild
+0.0491	defect
+0.0480	correspond

### Top negative features (20)
-0.0474	myoblast
-0.0497	untreat
-0.0500	fac
-0.0522	c2c12
-0.0523	hour
-0.0524	line
-0.0538	transgen
-0.0546	unclear
-0.0558	activ
-0.0582	follow
-0.0591	inject
-0.0606	sort
-0.0634	primari
-0.0647	ani
-0.0851	stem
-0.1005	cell
-0.1084	treat
-0.1116	induc
-0.1320	mef
-0.1414	fibroblast

### Vectorizer:   Number of Features: 4380
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukocyt', u'level', u'leverag', u'leydig', u'li', u'libitum']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 66
E-GEOD-20570
E-GEOD-62758
E-GEOD-57133
E-TABM-304
E-GEOD-55247

### False negatives: 13
E-ERAD-71
E-GEOD-8360
E-GEOD-15795
E-ERAD-381
E-GEOD-15794

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           401	Yes count:       116
No  count:     1098	No  count:           891	No  count:       207
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:18:05 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:18:58 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=362	randForSplit=345	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.86       409

avg / total       0.76      0.98      0.86       409

Train F2: 0.926

['yes', 'no']
[[401   8]
 [128 755]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.64      0.90      0.75       108

avg / total       0.64      0.90      0.75       108

Test  F2: 0.830

['yes', 'no']
[[ 97  11]
 [ 55 160]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=362, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3412	develop
+0.2805	tissu
+0.2515	liver
+0.2455	mutant
+0.2253	knockout
+0.2087	moe430
+0.2085	male
+0.2082	genotyp
+0.2067	postnat
+0.2031	litterm
+0.1961	pool
+0.1926	appar
+0.1916	kidney
+0.1912	dissect
+0.1888	femal
+0.1877	embryo
+0.1829	brain
+0.1776	development
+0.1678	gene
+0.1619	heart

### Top negative features (20)
-0.1963	myoblast
-0.1989	infect
-0.2000	treatment
-0.2001	unclear
-0.2026	embryon
-0.2031	differenti
-0.2040	fac
-0.2089	c2c12
-0.2458	cultur
-0.2464	inject
-0.2467	sort
-0.2503	transgen
-0.2544	treat
-0.2597	follow
-0.2740	primari
-0.3155	stem
-0.3595	cell
-0.4668	induc
-0.4799	fibroblast
-0.5304	mef

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin', u'lesion', u'let', u'lethal', u'leucin', u'leukaemia']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 55
E-MTAB-4349
E-GEOD-42389
E-GEOD-74317
E-GEOD-54924
E-MTAB-1569

### False negatives: 11
E-ERAD-278
E-GEOD-45278
E-TABM-280
E-ERAD-272
E-MTAB-3976

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Tue Sep 19 20:19:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:25:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=203	randForSplit=241	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       408

avg / total       0.79      0.97      0.87       408

Train F2: 0.924

['yes', 'no']
[[394  14]
 [106 778]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.87      0.76       109

avg / total       0.67      0.87      0.76       109

Test  F2: 0.823

['yes', 'no']
[[ 95  14]
 [ 46 168]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=203, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:25:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=50	randForSplit=953	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.83      0.97      0.89       426

avg / total       0.83      0.97      0.89       426

Train F2: 0.935

['yes', 'no']
[[412  14]
 [ 87 779]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.90      0.71        91

avg / total       0.59      0.90      0.71        91

Test  F2: 0.813

['yes', 'no']
[[ 82   9]
 [ 58 174]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=50, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=614	randForSplit=50	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.97      0.88       415

avg / total       0.81      0.97      0.88       415

Train F2: 0.931

['yes', 'no']
[[401  14]
 [ 93 784]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.81      0.73       102

avg / total       0.66      0.81      0.73       102

Test  F2: 0.779

['yes', 'no']
[[ 83  19]
 [ 42 179]]

### Best Pipeline Parameters:
classifier__alpha: 1e-05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1e-05, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=614, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:34 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:27:34 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=799	randForSplit=847	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.87       406

avg / total       0.78      0.97      0.87       406

Train F2: 0.928

['yes', 'no']
[[395  11]
 [109 777]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.71      0.93      0.80       111

avg / total       0.71      0.93      0.80       111

Test  F2: 0.874

['yes', 'no']
[[103   8]
 [ 42 170]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=799, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:27:35 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Wed Sep 20 11:28:00 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=428	randForSplit=34	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.95      0.80       415

avg / total       0.69      0.95      0.80       415

Train F2: 0.888

['yes', 'no']
[[396  19]
 [174 703]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.93      0.71       102

avg / total       0.58      0.93      0.71       102

Test  F2: 0.829

['yes', 'no']
[[ 95   7]
 [ 70 151]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=428, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:28:02 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### End Time Wed Sep 20 11:28:40 2017
### End Time Wed Sep 20 11:34:22 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:34:50 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=47	randForSplit=91	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.97      0.78       408

avg / total       0.65      0.97      0.78       408

Train F2: 0.886

['yes', 'no']
[[397  11]
 [212 672]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       109

avg / total       0.61      0.89      0.72       109

Test  F2: 0.815

['yes', 'no']
[[ 97  12]
 [ 62 152]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=47, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:34:52 2017
### End Time Wed Sep 20 11:42:38 2017

