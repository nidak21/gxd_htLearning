Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished
### Tuning Report Tue Sep 12 16:40:37 2017
Beta: 4
Random Seeds:
randForClassifier: 347
randForSplit: 461

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       451

avg / total       0.91      0.99      0.95       451

F4: 0.986

['yes', 'no']
[[447   4]
 [ 43 834]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.70      0.95      0.80       112

avg / total       0.70      0.95      0.80       112

F4: 0.927

['yes', 'no']
[[106   6]
 [ 46 175]]

### Top positive features (20)
+0.00   seq
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   usage
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   embryonic day
+0.00   day mouse
+0.00   development
+0.00   microRNA
+0.00   tissues
+0.00   seq embryonic
+0.00   postnatal

### Top negative features (20)
-0.00   differentiated
-0.00   transgenic mice
-0.00   sorted
-0.00   response
-0.00   stem cells
-0.00   transgenic
-0.00   differentiation
-0.00   infected
-0.00   stem
-0.00   infection
-0.00   treatment
-0.00   hours
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   embryonic fibroblasts
-0.00   cell
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=347, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x115bb8cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20430
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate']

Middle 10 features: [u'levels allele distinguished', u'levels analyzed', u'levels associated', u'levels cell', u'levels cell cycle', u'levels circulating', u'levels control', u'levels degradation', u'levels effect', u'levels expression']

Last 10 features: [u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 46
E-GEOD-45826
E-GEOD-4816
E-GEOD-42389
E-GEOD-21975
E-GEOD-65987

### False negatives: 6
E-GEOD-61404
E-GEOD-35594
E-GEOD-8106
E-GEOD-79929
E-GEOD-59127

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           451        Yes count:       112
No  count:     1098     No  count:           877        No  count:       221
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     33%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:41:35 2017
Beta: 4
Random Seeds:
randForClassifier: 869
randForSplit: 648

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       451

avg / total       0.92      0.99      0.95       451

F4: 0.986

['yes', 'no']
[[447   4]
 [ 40 837]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.63      0.89      0.74       112

avg / total       0.63      0.89      0.74       112

F4: 0.871

['yes', 'no']
[[100  12]
 [ 59 162]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   usage
+0.00   seq
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   day mouse
+0.00   embryonic day
+0.00   microRNA
+0.00   postnatal
+0.00   seq embryonic
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   bone marrow
-0.00   vitro
-0.00   pre
-0.00   differentiated
-0.00   bone
-0.00   cultured
-0.00   infected
-0.00   infection
-0.00   treatment
-0.00   stem cells
-0.00   hours
-0.00   stem
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   mouse embryonic
-0.00   embryonic fibroblasts
-0.00   treated
-0.00   fibroblasts
-0.00   cell
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=869, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x114e81cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 19853
First 10 features: [u'a2', u'aRNA', u'abdominal', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated']

Middle 10 features: [u'layers separated', u'lead', u'lead new', u'leaded', u'leaded removing', u'leaded removing growth', u'leading', u'leading cause', u'leading cause cancer', u'leading cytokine']

Last 10 features: [u'zona', u'zone', u'zone mouse', u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest']

### False positives: 59
E-GEOD-50950
E-GEOD-25469
E-GEOD-11419
E-GEOD-55340
E-GEOD-52809

### False negatives: 12
E-GEOD-51960
E-GEOD-7020
E-GEOD-67991
E-GEOD-11040
E-MTAB-867

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           451        Yes count:       112
No  count:     1098     No  count:           877        No  count:       221
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     33%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:42:04 2017
Beta: 4
Random Seeds:
randForClassifier: 622
randForSplit: 80

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       452

avg / total       0.91      0.99      0.95       452

F4: 0.984

['yes', 'no']
[[447   5]
 [ 46 830]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.91      0.77       111

avg / total       0.67      0.91      0.77       111

F4: 0.891

['yes', 'no']
[[101  10]
 [ 50 172]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   seq
+0.00   usage
+0.00   development
+0.00   postnatal
+0.00   day mouse
+0.00   embryonic day
+0.00   microRNA
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   transgenic mice
-0.00   derived
-0.00   sorted
-0.00   vitro
-0.00   transgenic
-0.00   differentiation
-0.00   stem cells
-0.00   infection
-0.00   infected
-0.00   stem
-0.00   treatment
-0.00   hours
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   fibroblasts
-0.00   cell
-0.00   treated
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=500, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=622, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10a49bcf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[500]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20293
First 10 features: [u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated']

Middle 10 features: [u'late stage', u'latency', u'latent', u'later', u'later stages', u'lateral', u'lavage', u'layer', u'layers', u'layers separated']

Last 10 features: [u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 50
E-GEOD-57990
E-GEOD-1143
E-GEOD-85855
E-GEOD-15574
E-GEOD-3486

### False negatives: 10
E-GEOD-55002
E-GEOD-54785
E-GEOD-17141
E-GEOD-73911
E-MTAB-3707

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           452        Yes count:       111
No  count:     1098     No  count:           876        No  count:       222
Percent Yes:    33%     Percent Yes:         34%        Percent Yes:     33%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:42:28 2017
Beta: 4
Random Seeds:
randForClassifier: 307
randForSplit: 891

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.95      0.99      0.97       442

avg / total       0.95      0.99      0.97       442

F4: 0.984

['yes', 'no']
[[436   6]
 [ 22 864]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.71      0.87      0.78       121

avg / total       0.71      0.87      0.78       121

F4: 0.857

['yes', 'no']
[[105  16]
 [ 42 170]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   seq
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   usage
+0.00   day mouse
+0.00   embryonic day
+0.00   postnatal
+0.00   microRNA
+0.00   development
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   derived
-0.00   differentiated
-0.00   transgenic
-0.00   infection
-0.00   sorted
-0.00   vitro
-0.00   differentiation
-0.00   infected
-0.00   stem cells
-0.00   treatment
-0.00   stem
-0.00   hours
-0.00   induced
-0.00   mouse embryonic fibroblasts
-0.00   cell
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 2000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=2000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=307, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10ab81cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[2000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 19901
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate']

Middle 10 features: [u'lacking gene', u'lacking gene required', u'lacks', u'lactamase', u'lactamase cassette', u'lactamase cassette site', u'lactogenic', u'lag', u'lamin', u'laminin']

Last 10 features: [u'zone', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 42
E-GEOD-4870
E-GEOD-41871
E-GEOD-54886
E-GEOD-7694
E-GEOD-66264

### False negatives: 16
E-GEOD-73911
E-GEOD-43517
E-GEOD-34210
E-GEOD-61582
E-GEOD-11484

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           442        Yes count:       121
No  count:     1098     No  count:           886        No  count:       212
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     36%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:44:06 2017
Beta: 4
Random Seeds:
randForClassifier: 788
randForSplit: 911

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.86      1.00      0.93       452

avg / total       0.86      1.00      0.93       452

F4: 0.989

['yes', 'no']
[[451   1]
 [ 72 804]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.56      0.97      0.71       111

avg / total       0.56      0.97      0.71       111

F4: 0.933

['yes', 'no']
[[108   3]
 [ 84 138]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   seq
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   usage
+0.00   embryonic day
+0.00   day mouse
+0.00   postnatal
+0.00   seq embryonic
+0.00   miRNA seq embryonic
+0.00   seq embryonic day
+0.00   microRNA

### Top negative features (20)
-0.00   derived
-0.00   vitro
-0.00   following
-0.00   culture
-0.00   pre
-0.00   transgenic
-0.00   infection
-0.00   infected
-0.00   stem cells
-0.00   hours
-0.00   treatment
-0.00   stem
-0.00   cell
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   fibroblasts
-0.00   treated
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=788, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10bbe3cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20186
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability phosphorylate']

Middle 10 features: [u'lanes platform data', u'lanes platform yielding', u'language', u'large', u'large calvarial', u'large calvarial defects', u'large intestine', u'large intestine differed', u'large nearly', u'large nearly unique']

Last 10 features: [u'zona', u'zone', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 84
E-MTAB-2184
E-GEOD-25506
E-GEOD-25257
E-GEOD-20570
E-GEOD-7150

### False negatives: 3
E-GEOD-8969
E-GEOD-27309
E-ERAD-520

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           452        Yes count:       111
No  count:     1098     No  count:           876        No  count:       222
Percent Yes:    33%     Percent Yes:         34%        Percent Yes:     33%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Traceback (most recent call last):
  File "TfidStdscalerSGD.py", line 4, in <module>
    import textTuningLib as tl
  File "../textTuningLib.py", line 267
    if ! verbose: return output
       ^
SyntaxError: invalid syntax
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished
### Tuning Report Tue Sep 12 16:55:35 2017
Beta: 4
Random Seeds:
randForClassifier: 869
randForSplit: 367

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.86      0.99      0.92       452

avg / total       0.86      0.99      0.92       452

F4: 0.983

['yes', 'no']
[[448   4]
 [ 71 805]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.60      0.94      0.73       111

avg / total       0.60      0.94      0.73       111

F4: 0.907

['yes', 'no']
[[104   7]
 [ 69 153]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### Top positive features (20)
+0.00	seq
+0.00	terms
+0.00	conditions refer
+0.00	data usage
+0.00	data usage terms
+0.00	terms conditions
+0.00	terms conditions refer
+0.00	usage terms
+0.00	usage terms conditions
+0.00	refer
+0.00	usage
+0.00	miRNA seq
+0.00	microRNA seq
+0.00	day mouse
+0.00	embryonic day
+0.00	postnatal
+0.00	microRNA
+0.00	miRNA seq embryonic
+0.00	seq embryonic day
+0.00	seq embryonic

### Top negative features (20)
-0.00	embryonic stem
-0.00	culture
-0.00	transgenic
-0.00	differentiated
-0.00	following
-0.00	response
-0.00	infected
-0.00	infection
-0.00	treatment
-0.00	hours
-0.00	stem cells
-0.00	mouse embryonic fibroblasts
-0.00	stem
-0.00	cell
-0.00	induced
-0.00	embryonic fibroblasts
-0.00	mouse embryonic
-0.00	fibroblasts
-0.00	treated
-0.00	cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=869, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10ac17c80>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 19978
First 10 features: [u'a2', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate']

Middle 10 features: [u'lens epithelial', u'lens epithelial cells', u'lenses', u'lentiviral', u'leptin', u'lesion', u'lesions', u'lethal', u'lethal mice', u'lethal mouse']

Last 10 features: [u'zone mouse', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 69
E-GEOD-33161
E-GEOD-1579
E-MTAB-27
E-GEOD-53156
E-GEOD-4928

### False negatives: 7
E-GEOD-27309
E-GEOD-63653
E-GEOD-15795
E-GEOD-12618
E-TABM-280

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           452	Yes count:       111
No  count:     1098	No  count:           876	No  count:       222
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     33%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:56:37 2017
Beta: 4
Random Seeds:
randForClassifier: 616
randForSplit: 79

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       441

avg / total       0.92      0.99      0.95       441

F4: 0.984

['yes', 'no']
[[436   5]
 [ 37 850]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.65      0.86      0.74       122

avg / total       0.65      0.86      0.74       122

F4: 0.844

['yes', 'no']
[[105  17]
 [ 57 154]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)


Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 16:57:59 2017
Beta: 4
Random Seeds:
randForClassifier: 101
randForSplit: 855

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.94      0.98      0.96       458

avg / total       0.94      0.98      0.96       458

F4: 0.982

['yes', 'no']
[[451   7]
 [ 31 839]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.64      0.91      0.75       105

avg / total       0.64      0.91      0.75       105

F4: 0.891

['yes', 'no']
[[ 96   9]
 [ 55 173]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished
### Tuning Report Tue Sep 12 16:59:24 2017
Beta: 4
Random Seeds:
randForClassifier: 590
randForSplit: 39

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.86      0.99      0.92       454


avg / total       0.86      0.99      0.92       454

F4: 0.984

['yes', 'no']
[[451   3]
 [ 76 798]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.54      0.96      0.69       109

avg / total       0.54      0.96      0.69       109

F4: 0.921

['yes', 'no']
[[105   4]
 [ 89 135]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:00:25 2017
Beta: 4
Random Seeds:
randForClassifier: 718
randForSplit: 8

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       450

avg / total       0.93      0.99      0.96       450

F4: 0.987

['yes', 'no']
[[446   4]
 [ 35 843]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.87      0.76       113

avg / total       0.67      0.87      0.76       113

F4: 0.853

['yes', 'no']
[[ 98  15]
 [ 48 172]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.01
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:00:48 2017
Beta: 4
Random Seeds:
randForClassifier: 257
randForSplit: 664

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.85      0.99      0.92       455

avg / total       0.85      0.99      0.92       455

F4: 0.982

['yes', 'no']
[[451   4]
 [ 78 795]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.53      0.90      0.67       108

avg / total       0.53      0.90      0.67       108

F4: 0.863

['yes', 'no']
[[ 97  11]
 [ 85 140]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished
### Tuning Report Tue Sep 12 17:01:05 2017
Beta: 4
Random Seeds:
randForClassifier: 100
randForSplit: 511

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.96       459

avg / total       0.92      0.99      0.96       459

F4: 0.987

['yes', 'no']
[[455   4]
 [ 38 831]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.89      0.77       104

avg / total       0.67      0.89      0.77       104

F4: 0.877

['yes', 'no']
[[ 93  11]
 [ 46 183]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:01:36 2017
Beta: 4
Random Seeds:
randForClassifier: 80
randForSplit: 402

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.33      1.00      0.50       441

avg / total       0.33      1.00      0.50       441

F4: 0.894

['yes', 'no']
[[441   0]
 [887   0]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.37      1.00      0.54       122

avg / total       0.37      1.00      0.54       122

F4: 0.908

['yes', 'no']
[[122   0]
 [211   0]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l1'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:01:56 2017
Beta: 4
Random Seeds:
randForClassifier: 550
randForSplit: 517

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.34      1.00      0.50       447

avg / total       0.34      1.00      0.50       447

F4: 0.896

['yes', 'no']
[[447   0]
 [881   0]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.35      1.00      0.52       116

avg / total       0.35      1.00      0.52       116

F4: 0.901

['yes', 'no']
[[116   0]
 [217   0]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l1'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished
### Tuning Report Tue Sep 12 17:02:35 2017
Beta: 4
Random Seeds:
randForClassifier: 962
randForSplit: 275

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       453

avg / total       0.93      0.99      0.96       453

F4: 0.989

['yes', 'no']
[[450   3]
 [ 34 841]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.88      0.76       110

avg / total       0.67      0.88      0.76       110

F4: 0.866

['yes', 'no']
[[ 97  13]
 [ 48 175]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:02:55 2017
Beta: 4
Random Seeds:
randForClassifier: 692
randForSplit: 746

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.88      0.99      0.93       453

avg / total       0.88      0.99      0.93       453

F4: 0.982

['yes', 'no']
[[448   5]
 [ 62 813]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.59      0.90      0.71       110

avg / total       0.59      0.90      0.71       110

F4: 0.872

['yes', 'no']
[[ 99  11]
 [ 70 153]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:03:47 2017
Beta: 4
Random Seeds:
randForClassifier: 260
randForSplit: 539

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       450

avg / total       0.93      0.99      0.96       450

F4: 0.987

['yes', 'no']
[[446   4]
 [ 36 842]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.69      0.91      0.78       113

avg / total       0.69      0.91      0.78       113

F4: 0.894

['yes', 'no']
[[103  10]
 [ 47 173]]

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:04:06 2017
Beta: 4
Random Seeds:
randForClassifier: 50
randForSplit: 203

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       452

avg / total       0.91      0.99      0.95       452

F4: 0.982

['yes', 'no']
[[446   6]
 [ 44 832]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.91      0.76       111

avg / total       0.66      0.91      0.76       111

F4: 0.890

['yes', 'no']
[[101  10]
 [ 53 169]]

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:04:26 2017
Beta: 4
Random Seeds:
randForClassifier: 23
randForSplit: 432

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       452

avg / total       0.93      0.99      0.96       452

F4: 0.989

['yes', 'no']
[[449   3]
 [ 35 841]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.70      0.94      0.80       111

avg / total       0.70      0.94      0.80       111

F4: 0.918

['yes', 'no']
[[104   7]
 [ 45 177]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished
### Tuning Report Tue Sep 12 17:04:39 2017
Beta: 4
Random Seeds:
randForClassifier: 224
randForSplit: 571

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       443

avg / total       0.92      0.99      0.95       443

F4: 0.984

['yes', 'no']
[[438   5]
 [ 38 847]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.88      0.76       120

avg / total       0.66      0.88      0.76       120

F4: 0.866

['yes', 'no']
[[106  14]
 [ 54 159]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:05:05 2017
Beta: 4
Random Seeds:
randForClassifier: 507
randForSplit: 278

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       443

avg / total       0.92      0.99      0.95       443

F4: 0.984

['yes', 'no']
[[438   5]
 [ 39 846]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.93      0.78       120

avg / total       0.67      0.93      0.78       120

F4: 0.905

['yes', 'no']
[[111   9]
 [ 54 159]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:05:32 2017
Beta: 4
Random Seeds:
randForClassifier: 765
randForSplit: 587

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.99      1.00      0.99       446

avg / total       0.99      1.00      0.99       446

F4: 0.997

['yes', 'no']
[[445   1]
 [  5 877]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.81      0.69      0.75       117

avg / total       0.81      0.69      0.75       117

F4: 0.698

['yes', 'no']
[[ 81  36]
 [ 19 197]]

### Best Pipeline Parameters:
classifier__alpha: 10
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/Users/jak/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
### Tuning Report Tue Sep 12 17:06:19 2017
Beta: 4
Random Seeds:
randForClassifier: 255
randForSplit: 991

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.00      0.00      0.00       436

avg / total       0.00      0.00      0.00       436

F4: 0.000

['yes', 'no']
[[  0 436]
 [  0 892]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.00      0.00      0.00       127

avg / total       0.00      0.00      0.00       127

F4: 0.000

['yes', 'no']
[[  0 127]
 [  0 206]]

### Best Pipeline Parameters:
classifier__alpha: 10
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l1'
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:06:50 2017
Beta: 4
Random Seeds:
randForClassifier: 285
randForSplit: 949

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.99      1.00      0.99       447

avg / total       0.99      1.00      0.99       447

F4: 0.997

['yes', 'no']
[[446   1]
 [  6 875]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.86      0.68      0.76       116

avg / total       0.86      0.68      0.76       116

F4: 0.689

['yes', 'no']
[[ 79  37]
 [ 13 204]]

### Best Pipeline Parameters:
classifier__alpha: 10
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:07:09 2017
Beta: 4
Random Seeds:
randForClassifier: 473
randForSplit: 915

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.94      0.99      0.96       456

avg / total       0.94      0.99      0.96       456

F4: 0.984

['yes', 'no']
[[450   6]
 [ 29 843]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.68      0.91      0.78       107

avg / total       0.68      0.91      0.78       107

F4: 0.889

['yes', 'no']
[[ 97  10]
 [ 45 181]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:07:33 2017
Beta: 4
Random Seeds:
randForClassifier: 310
randForSplit: 551

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.89      0.99      0.94       447

avg / total       0.89      0.99      0.94       447

F4: 0.986

['yes', 'no']
[[444   3]
 [ 57 824]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.62      0.92      0.74       116

avg / total       0.62      0.92      0.74       116

F4: 0.897

['yes', 'no']
[[107   9]
 [ 66 151]]

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:08:19 2017
Beta: 4
Random Seeds:
randForClassifier: 991
randForSplit: 521

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       457

avg / total       0.92      0.99      0.95       457

F4: 0.984

['yes', 'no']
[[452   5]
 [ 41 830]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.92      0.77       106

avg / total       0.66      0.92      0.77       106

F4: 0.895

['yes', 'no']
[[ 97   9]
 [ 49 178]]

### Best Pipeline Parameters:
classifier__alpha: 200
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: l
Tf_hinge.log            TfidStdscalerSGD.log    TuningTemplate.py
Tf_modified_huber.log   TfidStdscalerSGD.py     cache/
Local ~/work/gxd_htLearning/ModelTuning: echo "Note: this seems best" >>Tf_hinge.log 
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished
### Tuning Report Tue Sep 12 17:10:15 2017
Beta: 4
Random Seeds:
randForClassifier: 268
randForSplit: 838

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       441

avg / total       0.92      0.99      0.95       441

F4: 0.986

['yes', 'no']
[[437   4]
 [ 38 849]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.65      0.93      0.77       122

avg / total       0.65      0.93      0.77       122

F4: 0.904

['yes', 'no']
[[113   9]
 [ 60 151]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   day mouse
+0.00   usage
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   seq
+0.00   development
+0.00   embryonic day
+0.00   postnatal
+0.00   microRNA
+0.00   embryonic day mouse
+0.00   miRNA seq embryonic

### Top negative features (20)
-0.00   sorted
-0.00   differentiation
-0.00   activated
-0.00   following
-0.00   response
-0.00   pre
-0.00   infection
-0.00   infected
-0.00   stem cells
-0.00   hours
-0.00   treatment
-0.00   stem
-0.00   induced
-0.00   mouse embryonic fibroblasts
-0.00   cell
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=268, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10c866c08>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20155
First 10 features: [u'a2', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ablated', u'ablation']

Middle 10 features: [u'leucine rich', u'leucine rich proteoglycan', u'leucine zipper', u'leukaemia', u'leukemia', u'leukemia cells', u'leukemias', u'leukemic', u'leukemic cells', u'leukemogenesis']

Last 10 features: [u'zone', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 60
E-GEOD-6614
E-GEOD-37195
E-GEOD-14424
E-GEOD-40522
E-GEOD-13870

### False negatives: 9
E-GEOD-72165
E-GEOD-63653
E-GEOD-6770
E-ERAD-433
E-GEOD-44366

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           441        Yes count:       122
No  count:     1098     No  count:           887        No  count:       211
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     36%

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:11:05 2017
Beta: 4
Random Seeds:
randForClassifier: 371
randForSplit: 11

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.88      0.99      0.93       444

avg / total       0.88      0.99      0.93       444

F4: 0.986

['yes', 'no']
[[441   3]
 [ 61 823]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.95      0.78       119

avg / total       0.66      0.95      0.78       119

F4: 0.926

['yes', 'no']
[[113   6]
 [ 58 156]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:11:27 2017
Beta: 4
Random Seeds:
randForClassifier: 564
randForSplit: 799

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       438

avg / total       0.91      0.99      0.95       438

F4: 0.988

['yes', 'no']
[[435   3]
 [ 42 848]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.69      0.89      0.77       125

avg / total       0.69      0.89      0.77       125

F4: 0.873

['yes', 'no']
[[111  14]
 [ 51 157]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:11:40 2017
Beta: 4
Random Seeds:
randForClassifier: 556
randForSplit: 879

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       449

avg / total       0.92      0.99      0.95       449

F4: 0.988

['yes', 'no']
[[446   3]
 [ 41 838]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.89      0.76       114

avg / total       0.66      0.89      0.76       114

F4: 0.869

['yes', 'no']
[[101  13]
 [ 51 168]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.8s finished
### Tuning Report Tue Sep 12 17:12:14 2017
Beta: 4
Random Seeds:
randForClassifier: 791
randForSplit: 969

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       455

avg / total       0.92      0.99      0.95       455

F4: 0.989

['yes', 'no']
[[452   3]
 [ 41 832]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.72      0.86      0.78       108

avg / total       0.72      0.86      0.78       108

F4: 0.851

['yes', 'no']
[[ 93  15]
 [ 37 188]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:12:26 2017
Beta: 4
Random Seeds:
randForClassifier: 114
randForSplit: 59

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       456

avg / total       0.92      0.99      0.95       456

F4: 0.985

['yes', 'no']
[[451   5]
 [ 39 833]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.68      0.93      0.79       107

avg / total       0.68      0.93      0.79       107

F4: 0.914

['yes', 'no']
[[100   7]
 [ 47 179]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.6s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:12:44 2017
Beta: 4
Random Seeds:
randForClassifier: 667
randForSplit: 821

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       451

avg / total       0.93      0.99      0.96       451

F4: 0.983

['yes', 'no']
[[445   6]
 [ 35 842]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.69      0.90      0.78       112

avg / total       0.69      0.90      0.78       112

F4: 0.886

['yes', 'no']
[[101  11]
 [ 45 176]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.8s finished
### Tuning Report Tue Sep 12 17:12:53 2017
Beta: 4
Random Seeds:
randForClassifier: 493
randForSplit: 44

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       440

avg / total       0.91      0.99      0.95       440

F4: 0.984

['yes', 'no']
[[435   5]
 [ 41 847]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.73      0.93      0.81       123

avg / total       0.73      0.93      0.81       123

F4: 0.912

['yes', 'no']
[[114   9]
 [ 43 167]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:13:08 2017
Beta: 4
Random Seeds:
randForClassifier: 138
randForSplit: 312

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.90      0.99      0.94       454

avg / total       0.90      0.99      0.94       454

F4: 0.987

['yes', 'no']
[[451   3]
 [ 52 822]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.62      0.92      0.74       109

avg / total       0.62      0.92      0.74       109

F4: 0.892

['yes', 'no']
[[100   9]
 [ 61 163]]

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: l
Tf_hinge.log            TfidStdscalerSGD.log    TuningTemplate.py
Tf_modified_huber.log   TfidStdscalerSGD.py     cache/
Local ~/work/gxd_htLearning/ModelTuning: t Tf_hinge.log 
Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 16:40:37 2017
Beta: 4
Random Seeds:
randForClassifier: 347
randForSplit: 461

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       451

avg / total       0.91      0.99      0.95       451

F4: 0.986

['yes', 'no']
[[447   4]
 [ 43 834]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.70      0.95      0.80       112

avg / total       0.70      0.95      0.80       112

F4: 0.927

['yes', 'no']
[[106   6]
 [ 46 175]]

### Top positive features (20)
+0.00   seq
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   usage
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   embryonic day
+0.00   day mouse
+0.00   development
+0.00   microRNA
+0.00   tissues
+0.00   seq embryonic
+0.00   postnatal

### Top negative features (20)
-0.00   differentiated
-0.00   transgenic mice
-0.00   sorted
-0.00   response
-0.00   stem cells
-0.00   transgenic
-0.00   differentiation
-0.00   infected
-0.00   stem
-0.00   infection
-0.00   treatment
-0.00   hours
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   embryonic fibroblasts
-0.00   cell
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=347, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x115bb8cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20430
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate']

Middle 10 features: [u'levels allele distinguished', u'levels analyzed', u'levels associated', u'levels cell', u'levels cell cycle', u'levels circulating', u'levels control', u'levels degradation', u'levels effect', u'levels expression']

Last 10 features: [u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 46
E-GEOD-45826
E-GEOD-4816
E-GEOD-42389
E-GEOD-21975
E-GEOD-65987

### False negatives: 6
E-GEOD-61404
E-GEOD-35594
E-GEOD-8106
E-GEOD-79929
E-GEOD-59127

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           451        Yes count:       112
No  count:     1098     No  count:           877        No  count:       221
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     33%

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 16:41:35 2017
Beta: 4
Random Seeds:
randForClassifier: 869
randForSplit: 648

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.92      0.99      0.95       451

avg / total       0.92      0.99      0.95       451

F4: 0.986

['yes', 'no']
[[447   4]
 [ 40 837]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.63      0.89      0.74       112

avg / total       0.63      0.89      0.74       112

F4: 0.871

['yes', 'no']
[[100  12]
 [ 59 162]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   usage
+0.00   seq
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   day mouse
+0.00   embryonic day
+0.00   microRNA
+0.00   postnatal
+0.00   seq embryonic
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   bone marrow
-0.00   vitro
-0.00   pre
-0.00   differentiated
-0.00   bone
-0.00   cultured
-0.00   infected
-0.00   infection
-0.00   treatment
-0.00   stem cells
-0.00   hours
-0.00   stem
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   mouse embryonic
-0.00   embryonic fibroblasts
-0.00   treated
-0.00   fibroblasts
-0.00   cell
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=100, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=869, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x114e81cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 19853
First 10 features: [u'a2', u'aRNA', u'abdominal', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated']

Middle 10 features: [u'layers separated', u'lead', u'lead new', u'leaded', u'leaded removing', u'leaded removing growth', u'leading', u'leading cause', u'leading cause cancer', u'leading cytokine']

Last 10 features: [u'zona', u'zone', u'zone mouse', u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest']

### False positives: 59
E-GEOD-50950
E-GEOD-25469
E-GEOD-11419
E-GEOD-55340
E-GEOD-52809

### False negatives: 12
E-GEOD-51960
E-GEOD-7020
E-GEOD-67991
E-GEOD-11040
E-MTAB-867

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           451        Yes count:       112
No  count:     1098     No  count:           877        No  count:       221
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     33%

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 16:42:04 2017
Beta: 4
Random Seeds:
randForClassifier: 622
randForSplit: 80

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.99      0.95       452

avg / total       0.91      0.99      0.95       452

F4: 0.984

['yes', 'no']
[[447   5]
 [ 46 830]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.91      0.77       111

avg / total       0.67      0.91      0.77       111

F4: 0.891

['yes', 'no']
[[101  10]
 [ 50 172]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   seq
+0.00   usage
+0.00   development
+0.00   postnatal
+0.00   day mouse
+0.00   embryonic day
+0.00   microRNA
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   transgenic mice
-0.00   derived
-0.00   sorted
-0.00   vitro
-0.00   transgenic
-0.00   differentiation
-0.00   stem cells
-0.00   infection
-0.00   infected
-0.00   stem
-0.00   treatment
-0.00   hours
-0.00   mouse embryonic fibroblasts
-0.00   induced
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   fibroblasts
-0.00   cell
-0.00   treated
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=500, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=622, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10a49bcf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[500]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20293
First 10 features: [u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated']

Middle 10 features: [u'late stage', u'latency', u'latent', u'later', u'later stages', u'lateral', u'lavage', u'layer', u'layers', u'layers separated']

Last 10 features: [u'zone neural', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 50
E-GEOD-57990
E-GEOD-1143
E-GEOD-85855
E-GEOD-15574
E-GEOD-3486

### False negatives: 10
E-GEOD-55002
E-GEOD-54785
E-GEOD-17141
E-GEOD-73911
E-MTAB-3707

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           452        Yes count:       111
No  count:     1098     No  count:           876        No  count:       222
Percent Yes:    33%     Percent Yes:         34%        Percent Yes:     33%

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 16:42:28 2017
Beta: 4
Random Seeds:
randForClassifier: 307
randForSplit: 891

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.95      0.99      0.97       442

avg / total       0.95      0.99      0.97       442

F4: 0.984

['yes', 'no']
[[436   6]
 [ 22 864]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.71      0.87      0.78       121

avg / total       0.71      0.87      0.78       121

F4: 0.857

['yes', 'no']
[[105  16]
 [ 42 170]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   refer
+0.00   seq
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   usage
+0.00   day mouse
+0.00   embryonic day
+0.00   postnatal
+0.00   microRNA
+0.00   development
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   derived
-0.00   differentiated
-0.00   transgenic
-0.00   infection
-0.00   sorted
-0.00   vitro
-0.00   differentiation
-0.00   infected
-0.00   stem cells
-0.00   treatment
-0.00   stem
-0.00   hours
-0.00   induced
-0.00   mouse embryonic fibroblasts
-0.00   cell
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### Best Pipeline Parameters:
classifier__alpha: 2000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=2000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=307, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x10ab81cf8>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[2000]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 19901
First 10 features: [u'a2', u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate']

Middle 10 features: [u'lacking gene', u'lacking gene required', u'lacks', u'lactamase', u'lactamase cassette', u'lactamase cassette site', u'lactogenic', u'lag', u'lamin', u'laminin']

Last 10 features: [u'zone', u'zone neural', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 42
E-GEOD-4870
E-GEOD-41871
E-GEOD-54886
E-GEOD-7694
E-GEOD-66264

### False negatives: 16
E-GEOD-73911
E-GEOD-43517
E-GEOD-34210
E-GEOD-61582
E-GEOD-11484

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           442        Yes count:       121
No  count:     1098     No  count:           886        No  count:       212
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     36%

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 16:44:06 2017
Beta: 4
Random Seeds:
randForClassifier: 788
randForSplit: 911

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.86      1.00      0.93       452

avg / total       0.86      1.00      0.93       452

F4: 0.989

['yes', 'no']
[[451   1]
 [ 72 804]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.56      0.97      0.71       111

avg / total       0.56      0.97      0.71       111

F4: 0.933

['yes', 'no']
[[108   3]
 [ 84 138]]

### Top positive features (20)
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   seq
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   usage
+0.00   embryonic day
+0.00   day mouse
+0.00   postnatal
+0.00   seq embryonic
+0.00   miRNA seq embryonic
+0.00   seq embryonic day
+0.00   microRNA

### Top negative features (20)
-0.00   derived
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:17:07 2017
Beta: 4
Random Seeds:
randForClassifier: 297
randForSplit: 330

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.94      0.99      0.96       463

avg / total       0.94      0.99      0.96       463

F4: 0.986

['yes', 'no']
[[458   5]
 [ 30 835]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.70      0.92      0.80       100

avg / total       0.70      0.92      0.80       100

F4: 0.904

['yes', 'no']
[[ 92   8]
 [ 39 194]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:17:32 2017
Beta: 4
Random Seeds:
randForClassifier: 929
randForSplit: 16

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       460

avg / total       0.93      0.99      0.96       460

F4: 0.988

['yes', 'no']
[[456   4]
 [ 34 834]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.65      0.93      0.76       103

avg / total       0.65      0.93      0.76       103

F4: 0.909

['yes', 'no']
[[ 96   7]
 [ 52 178]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.6s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Tuning Report Tue Sep 12 17:17:49 2017
Beta: 4
Random Seeds:
randForClassifier: 594
randForSplit: 431

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.93      0.99      0.96       440

avg / total       0.93      0.99      0.96       440

F4: 0.989

['yes', 'no']
[[437   3]
 [ 35 853]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.70      0.85      0.77       123

avg / total       0.70      0.85      0.77       123

F4: 0.843

['yes', 'no']
[[105  18]
 [ 44 166]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:18:38 2017
Beta: 4
Random Seeds:
randForClassifier: 32
randForSplit: 798

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.40      0.85      0.54       454

avg / total       0.40      0.85      0.54       454

F4: 0.793

['yes', 'no']
[[384  70]
 [587 287]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.36      0.79      0.49       109

avg / total       0.36      0.79      0.49       109

F4: 0.736

['yes', 'no']
[[ 86  23]
 [156  68]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.0001
classifier__learning_rate: 'constant'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:19:30 2017
Beta: 4
Random Seeds:
randForClassifier: 325
randForSplit: 942

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.83      0.99      0.90       457

avg / total       0.83      0.99      0.90       457

F4: 0.976

['yes', 'no']
[[451   6]
 [ 95 776]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.57      0.92      0.71       106

avg / total       0.57      0.92      0.71       106

F4: 0.892

['yes', 'no']
[[ 98   8]
 [ 73 154]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.0001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:19:56 2017
Beta: 4
Random Seeds:
randForClassifier: 731
randForSplit: 909

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.87      0.99      0.92       455

avg / total       0.87      0.99      0.92       455

F4: 0.979

['yes', 'no']
[[449   6]
 [ 70 803]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.67      0.93      0.78       108

avg / total       0.67      0.93      0.78       108

F4: 0.905

['yes', 'no']
[[100   8]
 [ 50 175]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.0001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.7s remaining:    2.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished
### Tuning Report Tue Sep 12 17:20:19 2017
Beta: 4
Random Seeds:
randForClassifier: 506
randForSplit: 809

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.66      0.75      0.70       444

avg / total       0.66      0.75      0.70       444

F4: 0.744

['yes', 'no']
[[333 111]
 [168 716]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.61      0.62      0.62       119

avg / total       0.61      0.62      0.62       119

F4: 0.621

['yes', 'no']
[[ 74  45]
 [ 47 167]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:20:38 2017
Beta: 4
Random Seeds:
randForClassifier: 15
randForSplit: 668

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.47      0.92      0.62       435

avg / total       0.47      0.92      0.62       435

F4: 0.874

['yes', 'no']
[[402  33]
 [460 433]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.49      0.84      0.62       128

avg / total       0.49      0.84      0.62       128

F4: 0.810

['yes', 'no']
[[108  20]
 [111  94]]

### Best Pipeline Parameters:
classifier__alpha: 1500
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:21:03 2017
Beta: 4
Random Seeds:
randForClassifier: 125
randForSplit: 337

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.63      0.95      0.76       444

avg / total       0.63      0.95      0.76       444

F4: 0.925

['yes', 'no']
[[423  21]
 [251 633]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.52      0.85      0.65       119

avg / total       0.52      0.85      0.65       119

F4: 0.818

['yes', 'no']
[[101  18]
 [ 93 121]]

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:21:32 2017
Beta: 4
Random Seeds:
randForClassifier: 491
randForSplit: 226

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.90      0.99      0.94       451

avg / total       0.90      0.99      0.94       451

F4: 0.983

['yes', 'no']
[[446   5]
 [ 52 825]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.66      0.92      0.77       112

avg / total       0.66      0.92      0.77       112

F4: 0.898

['yes', 'no']
[[103   9]
 [ 54 167]]

### Best Pipeline Parameters:
classifier__alpha: 500
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:22:05 2017
Beta: 4
Random Seeds:
randForClassifier: 161
randForSplit: 671

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.90      0.99      0.94       444

avg / total       0.90      0.99      0.94       444

F4: 0.985

['yes', 'no']
[[440   4]
 [ 50 834]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.65      0.92      0.76       119

avg / total       0.65      0.92      0.76       119

F4: 0.902

['yes', 'no']
[[110   9]
 [ 60 154]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:22:37 2017
Beta: 4
Random Seeds:
randForClassifier: 680
randForSplit: 746

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.98      0.94       453

avg / total       0.91      0.98      0.94       453

F4: 0.980

['yes', 'no']
[[446   7]
 [ 45 830]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.68      0.87      0.76       110

avg / total       0.68      0.87      0.76       110

F4: 0.858

['yes', 'no']
[[ 96  14]
 [ 46 177]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished
### Tuning Report Tue Sep 12 17:23:02 2017
Beta: 4
Random Seeds:
randForClassifier: 137
randForSplit: 878

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.86      1.00      0.92       450

avg / total       0.86      1.00      0.92       450

F4: 0.987

['yes', 'no']
[[448   2]
 [ 72 806]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.61      0.96      0.74       113

avg / total       0.61      0.96      0.74       113

F4: 0.932

['yes', 'no']
[[109   4]
 [ 71 149]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished
### Tuning Report Tue Sep 12 17:23:25 2017
Beta: 4
Random Seeds:
randForClassifier: 892
randForSplit: 167

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.91      0.98      0.95       458

avg / total       0.91      0.98      0.95       458

F4: 0.980

['yes', 'no']
[[451   7]
 [ 44 826]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.63      0.96      0.76       105

avg / total       0.63      0.96      0.76       105

F4: 0.933

['yes', 'no']
[[101   4]
 [ 59 169]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 17:23:45 2017
Beta: 4
Random Seeds:
randForClassifier: 337
randForSplit: 34

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.88      0.99      0.93       454

avg / total       0.88      0.99      0.93       454

F4: 0.984

['yes', 'no']
[[450   4]
 [ 61 813]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.63      0.93      0.75       109

avg / total       0.63      0.93      0.75       109

F4: 0.901

['yes', 'no']
[[101   8]
 [ 60 164]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: ll -t
total 592
-rw-r--r--  1 jak  JAX\Domain Users   54182 Sep 12 17:23 Tf_hinge.log
-rw-r--r--@ 1 jak  JAX\Domain Users    2023 Sep 12 17:22 TfidStdscalerSGD.py
-rw-r--r--  1 jak  JAX\Domain Users  169952 Sep 12 16:37 Tf_modified_huber.log
-rw-r--r--@ 1 jak  JAX\Domain Users    2025 Sep 12 09:36 TuningTemplate.py
-rw-r--r--  1 jak  JAX\Domain Users   61926 Sep  8 17:08 TfidStdscalerSGD.log
drwxr-xr-x  3 jak  JAX\Domain Users     102 Aug 18 13:21 cache
Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 19:08:37 2017
Beta: 4
Random Seeds:
randForClassifier: 337
randForSplit: 564

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.90      0.99      0.94       443

avg / total       0.90      0.99      0.94       443

F4: 0.983

['yes', 'no']
[[438   5]
 [ 51 834]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.72      0.90      0.80       120

avg / total       0.72      0.90      0.80       120

F4: 0.887

['yes', 'no']
[[108  12]
 [ 41 172]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 19:08:56 2017
Beta: 4
Random Seeds:
randForClassifier: 649
randForSplit: 84

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.89      0.99      0.94       438

avg / total       0.89      0.99      0.94       438

F4: 0.986

['yes', 'no']
[[435   3]
 [ 54 836]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.74      0.90      0.81       125

avg / total       0.74      0.90      0.81       125

F4: 0.892

['yes', 'no']
[[113  12]
 [ 40 168]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.4s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished
### Tuning Report Tue Sep 12 19:09:22 2017
Beta: 4
Random Seeds:
randForClassifier: 565
randForSplit: 948

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.88      0.99      0.94       456

avg / total       0.88      0.99      0.94       456

F4: 0.986

['yes', 'no']
[[453   3]
 [ 59 813]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.57      0.93      0.70       107

avg / total       0.57      0.93      0.70       107

F4: 0.892

['yes', 'no']
[[ 99   8]
 [ 76 150]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Local ~/work/gxd_htLearning/ModelTuning: python TfidStdscalerSGD.py | tee -a Tf_hinge.log | more
Fitting 5 folds for each of 1 candidates, totalling 5 fits
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished
### Tuning Report Tue Sep 12 19:10:06 2017
Beta: 4
Random Seeds:
randForClassifier: 623
randForSplit: 170

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.89      0.99      0.94       450

avg / total       0.89      0.99      0.94       450

F4: 0.987

['yes', 'no']
[[447   3]
 [ 55 823]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.69      0.89      0.78       113

avg / total       0.69      0.89      0.78       113

F4: 0.878

['yes', 'no']
[[101  12]
 [ 46 174]]

### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### Top positive features (20)
+0.00   embryonic day
+0.00   seq
+0.00   terms
+0.00   conditions refer
+0.00   data usage
+0.00   data usage terms
+0.00   terms conditions
+0.00   terms conditions refer
+0.00   usage terms
+0.00   usage terms conditions
+0.00   day mouse
+0.00   refer
+0.00   miRNA seq
+0.00   microRNA seq
+0.00   usage
+0.00   postnatal
+0.00   seq embryonic
+0.00   embryonic day mouse
+0.00   miRNA seq embryonic
+0.00   seq embryonic day

### Top negative features (20)
-0.00   activated
-0.00   bone marrow
-0.00   derived
-0.00   infected
-0.00   response
-0.00   transgenic
-0.00   infection
-0.00   vitro
-0.00   treatment
-0.00   hours
-0.00   stem cells
-0.00   induced
-0.00   stem
-0.00   mouse embryonic fibroblasts
-0.00   cell
-0.00   embryonic fibroblasts
-0.00   mouse embryonic
-0.00   treated
-0.00   fibroblasts
-0.00   cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=10000, average=False, class_weight='balanced',
       epsilon=0.1, eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=623, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x113668c08>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[10000]
classifier__eta0:[1e-05]
classifier__learning_rate:['invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 21260
First 10 features: [u'aRNA', u'abdominal', u'abdominal obesity', u'abdominal obesity genes', u'abdominal obesity overlapped', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate']

Middle 10 features: [u'lethality', u'leucine', u'leucine zipper', u'leukemia', u'leukemia cells', u'leukemia clinically', u'leukemia clinically significant', u'leukemia elderly', u'leukemia inhibitory', u'leukemia inhibitory factor']

Last 10 features: [u'zone mouse', u'zones', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 46
E-GEOD-6466
E-MTAB-5112
E-GEOD-14249
E-GEOD-11139
E-GEOD-7887

### False negatives: 12
E-MEXP-486
E-GEOD-35322
E-GEOD-27720
E-MTAB-2328
E-GEOD-43517

### Train Test Split Report, test % = 0.20
All Samples:   1661     Training Samples:   1328        Test Samples:    333
Yes count:      563     Yes count:           450        Yes count:       113
No  count:     1098     No  count:           878        No  count:       220
Percent Yes:    33%     Percent Yes:         33%        Percent Yes:     33%
Note: trying more exhaustive grid search
Trying exhaustive gridsearch for huber
Fitting 5 folds for each of 72 candidates, totalling 360 fits
### Tuning Report Wed Sep 13 11:23:15 2017
Beta: 4
Random Seeds:
randForClassifier: 124
randForSplit: 559

### Metrics: Training Set
             precision    recall  f1-score   support

        yes       0.89      0.99      0.94       457

avg / total       0.89      0.99      0.94       457

F4: 0.985

['yes', 'no']
[[453   4]
 [ 57 814]]

### Metrics: Test Set
             precision    recall  f1-score   support

        yes       0.59      0.91      0.71       106

avg / total       0.59      0.91      0.71       106

F4: 0.877

['yes', 'no']
[[ 96  10]
 [ 68 159]]

### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)
### Top positive features (20)
+0.00	dissected
+0.00	postnatal
+0.00	terms
+0.00	seq
+0.00	conditions refer
+0.00	data usage
+0.00	data usage terms
+0.00	terms conditions
+0.00	terms conditions refer
+0.00	usage terms
+0.00	usage terms conditions
+0.00	refer
+0.00	embryonic day
+0.00	miRNA seq
+0.00	microRNA seq
+0.00	usage
+0.00	day mouse
+0.00	development
+0.00	microRNA
+0.00	tissues

### Top negative features (20)
-0.00	transgenic
-0.00	vitro
-0.00	induce
-0.00	following
-0.00	response
-0.00	differentiated
-0.00	infection
-0.00	infected
-0.00	stem
-0.00	stem cells
-0.00	hours
-0.00	treatment
-0.00	cell
-0.00	mouse embryonic fibroblasts
-0.00	induced
-0.00	mouse embryonic
-0.00	embryonic fibroblasts
-0.00	fibroblasts
-0.00	treated
-0.00	cells

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1000, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=124, shuffle=True,
       verbose=0, warm_start=False)

scaler:
StandardScaler(copy=True, with_mean=False, with_std=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2',
        preprocessor=<function vectorizer_preprocessor at 0x111fa6c08>,
        smooth_idf=True, stop_words='english', strip_accents='unicode',
        sublinear_tf=False, token_pattern=u'(?u)\\b([a-z_]\\w+)\\b',
        tokenizer=None, use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[100, 1000, 10000, 20000]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['hinge']
classifier__penalty:['l2', 'l1']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Vectorizer:   Number of Features: 20532
First 10 features: [u'a2', u'abdominal', u'aberrant', u'aberrations', u'ability', u'ability differentiate', u'ability phosphorylate', u'ability phosphorylate vitro', u'ablated', u'ablation']

Middle 10 features: [u'lamina propria', u'laminin', u'landmarks', u'landscape', u'landscapes', u'landscapes polycomb', u'landscapes polycomb repressive', u'lane', u'lane platform', u'lanes']

Last 10 features: [u'zone', u'zone mouse', u'zooepidemicus', u'zooepidemicus colony', u'zooepidemicus colony forming', u'zooepidemicus strongly', u'zooepidemicus strongly suggest', u'zygotic', u'zygotic genome', u'zygotic genome activation']

### False positives: 68
E-GEOD-33161
E-GEOD-46151
E-GEOD-15729
E-GEOD-19517
E-GEOD-47019

### False negatives: 10
E-GEOD-39524
E-GEOD-34388
E-GEOD-33091
E-MTAB-3707
E-GEOD-46515

### Train Test Split Report, test % = 0.20
All Samples:   1661	Training Samples:   1328	Test Samples:    333
Yes count:      563	Yes count:           457	Yes count:       106
No  count:     1098	No  count:           871	No  count:       227
Percent Yes:    33%	Percent Yes:         34%	Percent Yes:     31%

