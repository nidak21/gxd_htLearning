### Some Lessons Learned

* My version of sklearn is 0.18.1
* Vectorizing:
    * WHOA, the default preprocessor converts to lower case, not the tokenizer
    * SO if you provide your own preprocessor, you need .lower() there yourself
* Randomness in training.
    * most is in train_test_split and in the classifier (e.g., SGD)
    * as far as I can tell, there is none caused by the gridsearch CV fold
    partitioning
    * so fix a random_state for train_test_split and the classifier, give
    random state for gridsearch StratifiedKfold, now change the StratifiedKfold
    random state, run again. Identical results.
    * BUT if you change the random state for anything else, you get different 
    training results.
    * this seems very weird.

### Stemming is weird
* First, note that stemming typically doesn't improve models very much, but
    if you want to continue.... here is what I've learned
* Stemming is not built into sklearn. you must combine w/ nltk yourself.
    * import nltk.stem.snowball as nltk,  nltk.EnglishStemmer()
	(same for lemmatization - haven't tried this directly)
* OPTIONS:
    * Preprocess the data files beforehand (outside of Vectorizer)
	* leads to fastest tuning runs as you don't have to keep stemming
	    on each run.
	* cannot tune by comparing stemming to non-stemming
    * Create StemmingVectorizer subclass
	* see https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn
	* In subclass approach, you cannot easily test stemming vs. non-stemming
	    as you tune
	* Things to know:
	    * the preprocessor() takes a doc (string),returns a modified doc.
		* the default preprocessor() does .lower() if lowercase=True
	    * the tokenizer() takes a doc, returns list of tokens. Includes:
		* reg-ex tokenizing, n-gramming, stopword processing,
		    min_df, max_df processing
	    * Not sure where unicode, accent handling happens
	    * the analyzer() function runs both preprocessor() and then
		tokenizer(). If you want to do something in between of after
		these, customize this.
	* override build_analyzer() (and ultimately the analyzer() function) by
	    taking output of the super.build_analyzer() and stemming each token
	    that comes out.
	    * only stems the last word in each n-gram
	* OR override build_preprocessor() (and ultimately preprocessor()) by
	    taking output of the super.build_preprocessor(), tokenizing words,
	    and stemming each word, putting words back into a string to be
	    passed to the tokenizer.
	    * you don't HAVE to combine stemming w/ other preprocessing
	    * redundant tokenizing, 2 passes over the raw document (if 
		the default preprocessor() does anything)
	* OR override build_preprocessor() by providing your own preprocessor()
	    function.
	    * you must combine stemming w/ any other preprocessing you're doing 
	    * remember to .lower() if desired
	    * likely faster than making 2 passes
	* OR override build_tokenizer() (and ultimately tokenizer())
	    * you'd have to figure out where to stick stemming into the process
	    * seems painful as the tokenizer does a lot (I haven't tried this) 
    * Pass your own preprocessor() function at Vectorizer instantiation.
	    * equivalent to the last option above
	    * BUT you can include/exclude this preprocessor() option to test
		stemming vs. non-stemming as you tune
	    * you must combine stemming w/ any other preprocessing you're doing 
	    * remember to .lower() if desired
	    * likely faster than making 2 passes
    * Pass your own analyzer() function at Vectorizer instantiation.
	    * same issues as above
    * Pass your own tokenizer() function at Vectorizer instantiation.
	    * seems painful as the tokenizer does a lot (I haven't tried this)
    * STEMMING AND STOPWORDS. Note the stopword list is NOT
	stemmed, e.g.,  "become", "becomes", "becoming" are stopwords.
	But in the input doc, these get stemmed to "becom" which is not removed
	during stopword processing. Sigh. So you might want to enhance the
	stopword list by stemming its elements. Fortunately words removed by 
	min_df and max_df have already been stemmed (if you are stemming).
	
### Model Tuning for this application
    * SGD with 'log' seems hard to get reasonable recall on test set no matter
    what I try.
    * I cannot reproduce Yasmine's results for  SGD with 'log', even using
    the same params she used.
    * I cannot reproduce Yasmine's results for SGD and 'modified_huber'
    * I get a little better results with 'modified_huber', (can get recall
    up to .9), but not great
    * SGD with 'hinge' and 'invscaling' is pretty good. Recall 90-94 range
    and precision in 65-70 range

### Good params:
### Best Pipeline Parameters:
classifier__alpha: 10000
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Also these seem comparable:
### Best Pipeline Parameters:
classifier__alpha: 1000
classifier__eta0: 0.0001
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Tried CountVectorizer and MaxAbsScaler with hinge, worked comparably.
(not CountVectorizer with StandardScaler throws int to float warnings)

### Best Pipeline Parameters:
classifier__alpha: 100
classifier__eta0: 1e-05
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Tried Binary Vectorizer - not good.

CountVectorizer + MaxAbsScaler is getting 95% recall with:
### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'invscaling'
classifier__loss: 'hinge'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

Sep21: Seems like
hinge.good is the best yet. P ~ 60, R in 90's

Tried hinge w/ binary vectorizer w/ (1,1), (1,2), (1,3), various params.
no good.

Tried modified_huber, count vs. tfidf, (1,1), (1,2), (1,3) etc. doesnt
seem any better than hinge - and in most configs, it's worse.

### Odd positively/negatively highly weighted features
* a lot of odd highly weighted features come from the Encode experiments that
    are almost all "yes" (at least for now)
* the std text for these are:
    "microRNA-seq from limb (ENCSR070GAZ) ---- miRNA-seq on embryonic 15.5 day
    mouse limb  For data usage terms and conditions, please refer to"
* so this leads to highly weighted freatures: "conditions refer", "data usage",
    "data usage terms", "terms conditions", "refer" (I don't get this), etc.
* Other weird examples that I don't get: I guess it depends on how training
    and test sets are split
** sometimes "mice" is positively weighted but it is in 508 "no" and 328 "yes"
** similar "seq" is positively weighted but is in more "no" than "yes

### Naive Bayes Classifiers
There is a lot I don't understand about these (like about everything!)
I don't know what to do with the "priors" parameters, so I just went with the
defaults.

GaussianNB - couldn't use GridSearch on it because matrix type that comes out
of the vectorizer and which goes into the GaussianNB.fit() method don't match.
GaussianNB.fit() wants a "dense" matrix rather than a sparse one.
I spent some time trying to figure out how to get the CountVectorizer 
give a different output type (dtype parameter), but couldn't figure it out.
I briefly tried creating a "ToArray" class with a transform() method that
converted the input using .toarray().
Idea was to add this class to the Pipeline.
I could probably make this work, but I gave up after a bit.
So I skipped the GridSearch and ran it a few times. Pretty poor performance.

MultinomialNB - GridSearch was ok on this, a little better performance, but
not that good. Could get in the 80's for Recall and Precision.

BernoulliNB - Gridsearch ok, but worse performance.

----------Balancing yes and no's  Nov 2017 -------------------

After talking with Hagit and Xiangying doing the GXD reference relevance work
at U Delaware, they were very concerned about the imbalance between positive
and negative samples. They are trying multiple things to train/test on a
balanced set.

The simplest thing is to use the class_weight='balanced' parameter in the 
classifier. (see the documentation, I'm still a bit fuzzy on this, but have
been using it from the beginning)

Next simple thing to do is to randomly select a number of negative samples
from the total set of negative samples - select the same number as the positive
samples. Train/test on this pos + neg subset. Was hoping this would improve
precision.

I tried this. Here is what I did:

* Organizing the training/test data (in Data/balanceTesting)
    Start with 3248 total training set,
    Separated out all 1157 yes and 2071 no's into separate files.

    NOTE I did this via grep. Grep seems to skip lines that have non-ascii
    characters, so these counts are a bit lower from the total sample set
    (skipped experiments with non-ascii chrs in their title or desc).
    But not too many experiments were skipped. Turns out 19 were skipped
    by grep. See grep discussion below...
    
* Try1
    randomly pulled out 1157 no's from the total set of 2071
    (Wrote getRandomSubset.py to do this random selection)

    Combined these into a single training set file with the 1111 yes's
    (46 of the yes's were skipped Encode experiments).
    Call this the "balanced training set".
    Ran populateTrainingData.py to generate no/yes directories.

    Also pulled out the 913 "no"s that were not used for training. 
    Call these the "leftovers".

    Retuned LinearSVC using the Bal* tuning file names and output.
    Retuned parameters:  max_df=.7, min_df=.05, C=0.00001,
			class_weight='balanced' (still)
    Test set: P ~72, R ~94
    74 FP, 148 TN. True Negative ratio: 148/222 = .66

    Using these parameters, retrained a classifier on the whole balanced
    training set.

    On the 913 leftovers:
    409 FP and 504 TN. this is 504/913 = .55 true negative rate

    Difference in true negative rates on test set vs. leftovers is a bit
    concerning. Seems like the classification of negatives doesn't generalize
    too well.

* Try 2
    Same things, but with slightly different balanced training set size and
    new random subset.

    Random subset of no's: 1112 to go with 1111 yes's,
    total balanced test set: 2223,   958 leftovers.

    Retuned parameters:  max_df=.8, min_df=.01, C=0.00001,
			class_weight=None

    Test set: P .78, R .94,
    59 FP, and 160 TN. this is 160/129 = .69 true negative rate

    On 958 leftovers:
    388 FP, 570 TN. True negative ratio:  570/958 = .59
    
    In general these new parameters give a little better results all around,
    but the difference in true negative ratios is something to ponder.

lessons/thoughts:
    * Is this balance approach better than the previous unbalanced?
	need to decide...
    * grep's behavior w/ non-ascii is pain. need to remove non-ascii
	from exported sample TSV file
	sed -i 's/[\d128-\d255]//g' FILENAME
	    # doesn't work on Mac, need to find correct byte code syntax
	tr -cd '\11\12\15\40-\176' <input >output
	    # (Unix bash shell note:  $'\11' means to shell, 11 in octal)
	    # works ok except on some experiment descriptions
	    # E-GEOD-41246 has a weird char just before "UTRs was widespread"
	    # This char sequence is e2 27 in hex.
	    # This char seq kills cut and tr, and grep doesn't match anything
	    #   on that line after the char sequence.
	    # (vi is fine and it lets you delete the offending chars)
	    # Will need to investigate how to get rid of these types of chars.
	    # 19 experiments from the training set have some
	    #   of these weird chars.
