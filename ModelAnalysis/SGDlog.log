Fitting 5 folds for each of 24 candidates, totalling 120 fits
### Start Time Tue Sep 19 19:40:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=859	randForSplit=647	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.85       417

avg / total       0.76      0.98      0.85       417

Train F2: 0.926

['yes', 'no']
[[409   8]
 [132 743]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.92      0.65       100

avg / total       0.51      0.92      0.65       100

Test  F2: 0.792

['yes', 'no']
[[ 92   8]
 [ 89 134]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.1
classifier__learning_rate: 'invscaling'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.1, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=859, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0595	tissu
+0.0552	liver
+0.0523	mutant
+0.0502	defect
+0.0490	profil mous
+0.0480	knockout
+0.0446	development
+0.0442	genotyp
+0.0436	pool
+0.0436	mice
+0.0419	conclus
+0.0415	litterm
+0.0409	heart
+0.0407	brain
+0.0399	design
+0.0397	postnat
+0.0392	mice studi
+0.0386	embryon day
+0.0383	dissect

### Top negative features (20)
-0.0383	activ
-0.0405	embryon
-0.0411	cell treat
-0.0413	transgen
-0.0465	differenti
-0.0466	primari
-0.0470	treat
-0.0476	sort
-0.0478	inject
-0.0510	treatment
-0.0510	follow
-0.0619	stem
-0.0653	stem cell
-0.0829	cell
-0.0832	embryon fibroblast
-0.0909	induc
-0.0916	fibroblast mef
-0.0944	fibroblast
-0.0985	mous embryon
-0.1042	mef

### Vectorizer:   Number of Features: 16995
First 10 features: [u'a2', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr transcript', u'abil', u'abil activ']

Middle 10 features: [u'ko vs', u'ko wild', u'ko wildtyp', u'ko wt', u'kras', u'krasg12d', u'krasg12d activ', u'krt14', u'krt14 cre', u'kruppel']

Last 10 features: [u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zygot']

### False positives: 89
E-SMDB-3457
E-GEOD-29632
E-UMCU-22
E-GEOD-61205
E-CBIL-4

### False negatives: 8
E-GEOD-14605
E-GEOD-61582
E-GEOD-61367
E-GEOD-27309
E-ERAD-283

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:41:15 2017

### End Time Tue Sep 19 19:42:46 2017

Fitting 5 folds for each of 8 candidates, totalling 40 fits
### Start Time Tue Sep 19 19:48:09 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=452	randForSplit=189	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.73      0.98      0.84       413

avg / total       0.73      0.98      0.84       413

Train F2: 0.918

['yes', 'no']
[[405   8]
 [149 730]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.95      0.72       104

avg / total       0.58      0.95      0.72       104

Test  F2: 0.843

['yes', 'no']
[[ 99   5]
 [ 72 147]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=452, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0939	develop
+0.0707	tissu
+0.0649	genotyp
+0.0636	extens
+0.0628	mutant
+0.0590	dissect
+0.0568	kidney
+0.0543	conclus
+0.0529	pool
+0.0529	development
+0.0524	litterm
+0.0521	develop mous
+0.0519	brain
+0.0514	wild type
+0.0513	liver
+0.0509	wild
+0.0506	mous heart
+0.0480	knockout
+0.0468	defici mice
+0.0461	mutant embryo

### Top negative features (20)
-0.0453	embryon stem
-0.0497	activ
-0.0509	cell treat
-0.0528	unclear
-0.0545	inject
-0.0549	transgen
-0.0575	treat
-0.0585	sort
-0.0599	primari
-0.0618	infect
-0.0652	follow
-0.0776	stem
-0.0820	stem cell
-0.0865	cell
-0.0924	embryon fibroblast
-0.0945	fibroblast mef
-0.0980	induc
-0.1060	fibroblast
-0.1092	mous embryon
-0.1106	mef

### Vectorizer:   Number of Features: 16651
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene']

Middle 10 features: [u'ko condit', u'ko control', u'ko embryo', u'ko il6', u'ko ko', u'ko liver', u'ko mdr2', u'ko mef', u'ko mice', u'ko mous']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 72
E-GEOD-36899
E-GEOD-49122
E-GEOD-47395
E-GEOD-25506
E-GEOD-1052

### False negatives: 5
E-ERAD-231
E-ERAD-169
E-GEOD-27309
E-ERAD-237
E-ERAD-272

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           413	Yes count:       104
No  count:     1098	No  count:           879	No  count:       219
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Tue Sep 19 19:48:49 2017

Fitting 5 folds for each of 48 candidates, totalling 240 fits
### Start Time Tue Sep 19 19:50:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=441	randForSplit=609	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.98      0.78       407

avg / total       0.65      0.98      0.78       407

Train F2: 0.890

['yes', 'no']
[[400   7]
 [218 667]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.95      0.74       110

avg / total       0.60      0.95      0.74       110

Test  F2: 0.855

['yes', 'no']
[[105   5]
 [ 69 144]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=441, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.1028	develop
+0.0917	knockout
+0.0726	tissu
+0.0708	mous
+0.0641	mutant
+0.0633	liver
+0.0549	genotyp
+0.0548	overal
+0.0546	design
+0.0532	defect
+0.0501	brain
+0.0477	gene
+0.0472	litterm
+0.0469	development
+0.0459	exhibit
+0.0458	project
+0.0458	correspond
+0.0457	pool
+0.0456	dnase
+0.0448	dissect

### Top negative features (20)
-0.0482	line
-0.0483	fac
-0.0503	untreat
-0.0507	unclear
-0.0512	c2c12
-0.0513	infect
-0.0529	hour
-0.0537	follow
-0.0540	ani
-0.0543	inject
-0.0557	transgen
-0.0580	sort
-0.0586	cultur
-0.0620	primari
-0.0694	stem
-0.1089	treat
-0.1152	induc
-0.1213	fibroblast
-0.1354	mef
-0.1720	cell

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a10', u'a2', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'lcm', u'lcmv', u'ld', u'ldp', u'lead', u'lean', u'learn', u'leav', u'lectin', u'led']

Last 10 features: [u'yy1', u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 69
E-MTAB-4079
E-GEOD-6223
E-GEOD-70819
E-GEOD-60596
E-ERAD-201

### False negatives: 5
E-GEOD-43517
E-ERAD-72
E-MTAB-3707
E-ERAD-169
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           407	Yes count:       110
No  count:     1098	No  count:           885	No  count:       213
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     34%
### End Time Tue Sep 19 19:51:02 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 19:53:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=904	randForSplit=877	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.98      0.81       417

avg / total       0.69      0.98      0.81       417

Train F2: 0.900

['yes', 'no']
[[407  10]
 [186 689]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.92      0.69       100

avg / total       0.56      0.92      0.69       100

Test  F2: 0.814

['yes', 'no']
[[ 92   8]
 [ 73 150]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=904, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0919	develop
+0.0830	wild
+0.0821	tissu
+0.0749	mutant
+0.0620	knockout
+0.0615	conclus
+0.0606	male
+0.0594	genotyp
+0.0574	design
+0.0565	postnat
+0.0559	overal
+0.0548	defect
+0.0545	liver
+0.0538	litterm
+0.0506	mice
+0.0500	development
+0.0495	dissect
+0.0494	kidney
+0.0491	gene
+0.0485	seq

### Top negative features (20)
-0.0453	stimul
-0.0472	moratoria
-0.0488	unclear
-0.0501	infect
-0.0502	inject
-0.0520	differenti
-0.0526	ani
-0.0526	fac
-0.0551	activ
-0.0579	primari
-0.0595	treat
-0.0597	induct
-0.0610	sort
-0.0633	follow
-0.0660	transgen
-0.0723	stem
-0.0933	cell
-0.1050	induc
-0.1304	mef
-0.1311	fibroblast

### Vectorizer:   Number of Features: 4346
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'lef', u'left', u'leica', u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'leptin', u'lesion']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 73
E-ERAD-201
E-GEOD-5140
E-GEOD-40522
E-GEOD-8549
E-GEOD-21041

### False negatives: 8
E-GEOD-59777
E-GEOD-61582
E-GEOD-2362
E-ERAD-272
E-ERAD-169

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:54:37 2017

Fitting 5 folds for each of 108 candidates, totalling 540 fits
### Start Time Tue Sep 19 19:57:19 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=155	randForSplit=934	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.98      0.89       403

avg / total       0.81      0.98      0.89       403

Train F2: 0.942

['yes', 'no']
[[396   7]
 [ 95 794]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.92      0.74       114

avg / total       0.62      0.92      0.74       114

Test  F2: 0.840

['yes', 'no']
[[105   9]
 [ 64 145]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=155, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2', 'l1']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0658	embryon day
+0.0641	tissu
+0.0625	mutant
+0.0612	pool
+0.0611	liver
+0.0522	dissect
+0.0516	knockout
+0.0508	mous heart
+0.0508	genotyp
+0.0507	male
+0.0496	kidney
+0.0495	brain
+0.0494	postnat
+0.0494	development
+0.0493	mice studi
+0.0488	litterm
+0.0469	correspond
+0.0465	femal
+0.0463	profil mous

### Top negative features (20)
-0.0487	cell treat
-0.0498	inject
-0.0509	hour
-0.0528	transgen
-0.0528	embryon
-0.0530	unclear
-0.0560	primari
-0.0564	treatment
-0.0577	follow
-0.0602	sort
-0.0780	stem
-0.0789	stem cell
-0.0927	cell
-0.0983	induc
-0.1031	treat
-0.1088	fibroblast
-0.1112	fibroblast mef
-0.1187	mous embryon
-0.1200	embryon fibroblast
-0.1251	mef

### Vectorizer:   Number of Features: 16857
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'krt14 cre', u'kruppel', u'kruppel like', u'ksom', u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 64
E-GEOD-1482
E-GEOD-13691
E-GEOD-769
E-GEOD-2130
E-GEOD-66088

### False negatives: 9
E-GEOD-61367
E-GEOD-9913
E-GEOD-59777
E-ERAD-433
E-GEOD-45278

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 19:58:45 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 20:03:01 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=440	randForSplit=448	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.82      0.98      0.89       419

avg / total       0.82      0.98      0.89       419

Train F2: 0.946

['yes', 'no']
[[412   7]
 [ 90 783]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.90      0.75        98

avg / total       0.65      0.90      0.75        98

Test  F2: 0.833

['yes', 'no']
[[ 88  10]
 [ 48 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-06
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-06, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=440, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0889	develop
+0.0712	tissu
+0.0644	mutant
+0.0578	pool
+0.0535	postnat
+0.0525	mutant embryo
+0.0523	brain
+0.0515	genotyp
+0.0498	liver
+0.0497	dissect
+0.0486	extens
+0.0484	embryon day
+0.0476	design
+0.0471	kidney
+0.0459	e12
+0.0457	defici mice
+0.0452	litterm
+0.0448	development
+0.0442	knockout
+0.0441	wild type

### Top negative features (20)
-0.0482	activ
-0.0483	inject
-0.0508	untreat
-0.0510	transgen
-0.0512	cell treat
-0.0517	sort
-0.0552	unclear
-0.0585	treatment
-0.0590	stem
-0.0602	stem cell
-0.0603	primari
-0.0637	follow
-0.0982	induc
-0.1024	embryon fibroblast
-0.1043	treat
-0.1107	fibroblast mef
-0.1202	fibroblast
-0.1203	mous embryon
-0.1372	mef
-0.1613	cell

### Vectorizer:   Number of Features: 16902
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt', u'l5178i', u'l5178i cell', u'la', u'la jolla']

Last 10 features: [u'zipper', u'zona', u'zone', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot']

### False positives: 48
E-MARS-18
E-GEOD-3621
E-GEOD-54678
E-GEOD-57990
E-MTAB-3106

### False negatives: 10
E-GEOD-21860
E-GEOD-22125
E-MTAB-1404
E-ERAD-283
E-GEOD-7342

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:03:45 2017

### End Time Tue Sep 19 20:08:24 2017
### End Time Tue Sep 19 20:12:42 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:13:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=308	randForSplit=566	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.96      0.87       420

avg / total       0.80      0.96      0.87       420

Train F2: 0.926

['yes', 'no']
[[405  15]
 [101 771]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.89      0.76        97

avg / total       0.66      0.89      0.76        97

Test  F2: 0.830

['yes', 'no']
[[ 86  11]
 [ 44 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=308, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05, 0.0001, 0.001, 0.01]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3644	develop
+0.3073	tissu
+0.2563	liver
+0.2430	mutant
+0.2406	heart
+0.2390	postnat
+0.2240	knockout
+0.2125	litterm
+0.2048	kidney
+0.2047	dissect
+0.2019	development
+0.1998	wild
+0.1925	design
+0.1863	overal
+0.1841	e12
+0.1787	consist
+0.1724	bud
+0.1669	pool
+0.1662	genotyp
+0.1621	box

### Top negative features (20)
-0.1679	myoblast
-0.1920	treatment
-0.1925	stimul
-0.1960	infect
-0.2020	fac
-0.2047	unclear
-0.2064	differenti
-0.2181	activ
-0.2271	c2c12
-0.2337	inject
-0.2361	primari
-0.2579	treat
-0.2590	sort
-0.2698	transgen
-0.2764	follow
-0.2922	stem
-0.3963	induc
-0.4142	cell
-0.5183	fibroblast
-0.5604	mef

### Vectorizer:   Number of Features: 4417
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'ablat', u'abnorm']

Middle 10 features: [u'life', u'lifespan', u'lifetim', u'ligand', u'ligas', u'ligat', u'light', u'like', u'likewis', u'lim']

Last 10 features: [u'zero', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 44
E-GEOD-13691
E-GEOD-60780
E-GEOD-59848
E-GEOD-59463
E-MTAB-1569

### False negatives: 11
E-GEOD-59777
E-MTAB-5020
E-ERAD-237
E-GEOD-8969
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           420	Yes count:        97
No  count:     1098	No  count:           872	No  count:       226
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:13:41 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:15:31 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=728	randForSplit=842	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.74       403

avg / total       0.60      0.98      0.74       403

Train F2: 0.867

['yes', 'no']
[[393  10]
 [261 628]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.99      0.73       114

avg / total       0.58      0.99      0.73       114

Test  F2: 0.869

['yes', 'no']
[[113   1]
 [ 81 128]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=728, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1071	develop
+0.0769	tissu
+0.0738	mutant
+0.0695	knockout
+0.0644	defect
+0.0641	heart
+0.0625	exhibit
+0.0607	postnat
+0.0597	design
+0.0581	liver
+0.0575	kidney
+0.0566	conclus
+0.0560	pool
+0.0553	litterm
+0.0539	e12
+0.0536	overal
+0.0535	gene
+0.0532	genotyp
+0.0499	wild
+0.0493	development

### Top negative features (20)
-0.0441	fac
-0.0443	treatment
-0.0445	activ
-0.0455	hour
-0.0465	c2c12
-0.0486	infect
-0.0487	line
-0.0522	unclear
-0.0526	treat
-0.0552	transgen
-0.0567	primari
-0.0588	inject
-0.0608	sort
-0.0621	respons
-0.0770	follow
-0.0789	stem
-0.0955	cell
-0.1010	induc
-0.1263	fibroblast
-0.1423	mef

### Vectorizer:   Number of Features: 4349
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lead', u'lean', u'learn', u'leav', u'led', u'lef', u'left', u'leica', u'len', u'length']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 81
E-MTAB-2266
E-GEOD-12309
E-ERAD-201
E-GEOD-29632
E-CBIL-39

### False negatives: 1
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:15:37 2017

Interesting 60% precision, high recall
Try some more repetitions
Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:17:59 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=492	randForSplit=650	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.68      0.96      0.79       401

avg / total       0.68      0.96      0.79       401

Train F2: 0.884

['yes', 'no']
[[383  18]
 [180 711]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       116

avg / total       0.61      0.89      0.72       116

Test  F2: 0.814

['yes', 'no']
[[103  13]
 [ 66 141]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=492, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0987	develop
+0.0793	tissu
+0.0739	mutant
+0.0634	liver
+0.0617	mice
+0.0572	exhibit
+0.0563	genotyp
+0.0548	knockout
+0.0546	brain
+0.0542	appar
+0.0542	postnat
+0.0533	gene
+0.0520	conclus
+0.0515	development
+0.0512	litter
+0.0511	litterm
+0.0506	dissect
+0.0504	wild
+0.0491	defect
+0.0480	correspond

### Top negative features (20)
-0.0474	myoblast
-0.0497	untreat
-0.0500	fac
-0.0522	c2c12
-0.0523	hour
-0.0524	line
-0.0538	transgen
-0.0546	unclear
-0.0558	activ
-0.0582	follow
-0.0591	inject
-0.0606	sort
-0.0634	primari
-0.0647	ani
-0.0851	stem
-0.1005	cell
-0.1084	treat
-0.1116	induc
-0.1320	mef
-0.1414	fibroblast

### Vectorizer:   Number of Features: 4380
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukocyt', u'level', u'leverag', u'leydig', u'li', u'libitum']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 66
E-GEOD-20570
E-GEOD-62758
E-GEOD-57133
E-TABM-304
E-GEOD-55247

### False negatives: 13
E-ERAD-71
E-GEOD-8360
E-GEOD-15795
E-ERAD-381
E-GEOD-15794

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           401	Yes count:       116
No  count:     1098	No  count:           891	No  count:       207
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:18:05 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:18:58 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=362	randForSplit=345	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.86       409

avg / total       0.76      0.98      0.86       409

Train F2: 0.926

['yes', 'no']
[[401   8]
 [128 755]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.64      0.90      0.75       108

avg / total       0.64      0.90      0.75       108

Test  F2: 0.830

['yes', 'no']
[[ 97  11]
 [ 55 160]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=362, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3412	develop
+0.2805	tissu
+0.2515	liver
+0.2455	mutant
+0.2253	knockout
+0.2087	moe430
+0.2085	male
+0.2082	genotyp
+0.2067	postnat
+0.2031	litterm
+0.1961	pool
+0.1926	appar
+0.1916	kidney
+0.1912	dissect
+0.1888	femal
+0.1877	embryo
+0.1829	brain
+0.1776	development
+0.1678	gene
+0.1619	heart

### Top negative features (20)
-0.1963	myoblast
-0.1989	infect
-0.2000	treatment
-0.2001	unclear
-0.2026	embryon
-0.2031	differenti
-0.2040	fac
-0.2089	c2c12
-0.2458	cultur
-0.2464	inject
-0.2467	sort
-0.2503	transgen
-0.2544	treat
-0.2597	follow
-0.2740	primari
-0.3155	stem
-0.3595	cell
-0.4668	induc
-0.4799	fibroblast
-0.5304	mef

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin', u'lesion', u'let', u'lethal', u'leucin', u'leukaemia']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 55
E-MTAB-4349
E-GEOD-42389
E-GEOD-74317
E-GEOD-54924
E-MTAB-1569

### False negatives: 11
E-ERAD-278
E-GEOD-45278
E-TABM-280
E-ERAD-272
E-MTAB-3976

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Tue Sep 19 20:19:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:25:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=203	randForSplit=241	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       408

avg / total       0.79      0.97      0.87       408

Train F2: 0.924

['yes', 'no']
[[394  14]
 [106 778]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.87      0.76       109

avg / total       0.67      0.87      0.76       109

Test  F2: 0.823

['yes', 'no']
[[ 95  14]
 [ 46 168]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=203, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:25:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=50	randForSplit=953	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.83      0.97      0.89       426

avg / total       0.83      0.97      0.89       426

Train F2: 0.935

['yes', 'no']
[[412  14]
 [ 87 779]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.90      0.71        91

avg / total       0.59      0.90      0.71        91

Test  F2: 0.813

['yes', 'no']
[[ 82   9]
 [ 58 174]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=50, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=614	randForSplit=50	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.97      0.88       415

avg / total       0.81      0.97      0.88       415

Train F2: 0.931

['yes', 'no']
[[401  14]
 [ 93 784]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.81      0.73       102

avg / total       0.66      0.81      0.73       102

Test  F2: 0.779

['yes', 'no']
[[ 83  19]
 [ 42 179]]

### Best Pipeline Parameters:
classifier__alpha: 1e-05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1e-05, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=614, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:34 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:27:34 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=799	randForSplit=847	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.87       406

avg / total       0.78      0.97      0.87       406

Train F2: 0.928

['yes', 'no']
[[395  11]
 [109 777]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.71      0.93      0.80       111

avg / total       0.71      0.93      0.80       111

Test  F2: 0.874

['yes', 'no']
[[103   8]
 [ 42 170]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=799, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:27:35 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Wed Sep 20 11:28:00 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=428	randForSplit=34	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.95      0.80       415

avg / total       0.69      0.95      0.80       415

Train F2: 0.888

['yes', 'no']
[[396  19]
 [174 703]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.93      0.71       102

avg / total       0.58      0.93      0.71       102

Test  F2: 0.829

['yes', 'no']
[[ 95   7]
 [ 70 151]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=428, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:28:02 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### End Time Wed Sep 20 11:28:40 2017
### End Time Wed Sep 20 11:34:22 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:34:50 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=47	randForSplit=91	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.97      0.78       408

avg / total       0.65      0.97      0.78       408

Train F2: 0.886

['yes', 'no']
[[397  11]
 [212 672]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       109

avg / total       0.61      0.89      0.72       109

Test  F2: 0.815

['yes', 'no']
[[ 97  12]
 [ 62 152]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=47, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:34:52 2017
### End Time Wed Sep 20 11:42:38 2017

Fitting 5 folds for each of 24 candidates, totalling 120 fits
### Start Time Tue Sep 19 19:40:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=859	randForSplit=647	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.85       417

avg / total       0.76      0.98      0.85       417

Train F2: 0.926

['yes', 'no']
[[409   8]
 [132 743]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.92      0.65       100

avg / total       0.51      0.92      0.65       100

Test  F2: 0.792

['yes', 'no']
[[ 92   8]
 [ 89 134]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.1
classifier__learning_rate: 'invscaling'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.1, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=859, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0595	tissu
+0.0552	liver
+0.0523	mutant
+0.0502	defect
+0.0490	profil mous
+0.0480	knockout
+0.0446	development
+0.0442	genotyp
+0.0436	pool
+0.0436	mice
+0.0419	conclus
+0.0415	litterm
+0.0409	heart
+0.0407	brain
+0.0399	design
+0.0397	postnat
+0.0392	mice studi
+0.0386	embryon day
+0.0383	dissect

### Top negative features (20)
-0.0383	activ
-0.0405	embryon
-0.0411	cell treat
-0.0413	transgen
-0.0465	differenti
-0.0466	primari
-0.0470	treat
-0.0476	sort
-0.0478	inject
-0.0510	treatment
-0.0510	follow
-0.0619	stem
-0.0653	stem cell
-0.0829	cell
-0.0832	embryon fibroblast
-0.0909	induc
-0.0916	fibroblast mef
-0.0944	fibroblast
-0.0985	mous embryon
-0.1042	mef

### Vectorizer:   Number of Features: 16995
First 10 features: [u'a2', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr transcript', u'abil', u'abil activ']

Middle 10 features: [u'ko vs', u'ko wild', u'ko wildtyp', u'ko wt', u'kras', u'krasg12d', u'krasg12d activ', u'krt14', u'krt14 cre', u'kruppel']

Last 10 features: [u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zygot']

### False positives: 89
E-SMDB-3457
E-GEOD-29632
E-UMCU-22
E-GEOD-61205
E-CBIL-4

### False negatives: 8
E-GEOD-14605
E-GEOD-61582
E-GEOD-61367
E-GEOD-27309
E-ERAD-283

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:41:15 2017

Fitting 5 folds for each of 16 candidates, totalling 80 fits
### Start Time Tue Sep 19 19:42:32 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=150	randForSplit=878	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.57      0.96      0.72       418

avg / total       0.57      0.96      0.72       418

Train F2: 0.846

['yes', 'no']
[[401  17]
 [297 577]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.45      0.96      0.62        99

avg / total       0.45      0.96      0.62        99

Test  F2: 0.785

['yes', 'no']
[[ 95   4]
 [114 110]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.1
classifier__learning_rate: 'invscaling'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.1, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=150, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1, 10]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0122	develop
+0.0090	conclus
+0.0087	differenti express
+0.0086	tissu
+0.0077	liver
+0.0077	mice
+0.0077	mutant
+0.0075	knockout
+0.0072	insight
+0.0072	gene
+0.0072	genotyp
+0.0070	heart
+0.0068	seq
+0.0066	rna seq
+0.0066	defect
+0.0062	design
+0.0062	express
+0.0060	postnat
+0.0059	wild
+0.0058	analys

### Top negative features (20)
-0.0056	pre public
-0.0056	inject
-0.0058	data pre
-0.0058	public releas
-0.0059	follow
-0.0059	primari
-0.0059	sort
-0.0059	cell line
-0.0063	cell treat
-0.0068	unclear
-0.0075	treat
-0.0075	stem
-0.0079	stem cell
-0.0091	induc
-0.0118	embryon fibroblast
-0.0130	fibroblast mef
-0.0130	fibroblast
-0.0138	mous embryon
-0.0149	mef
-0.0186	cell

### Vectorizer:   Number of Features: 16808
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'label accord', u'label affymetrix', u'label biotin', u'label cdna', u'label complementari', u'label crna', u'label crnas', u'label cy3', u'label cy5', u'label fragment']

Last 10 features: [u'zone', u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 114
E-GEOD-14727
E-GEOD-16992
E-GEOD-26064
E-GEOD-47735
E-GEOD-52564

### False negatives: 4
E-GEOD-6589
E-ERAD-381
E-GEOD-12618
E-GEOD-20639

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           418	Yes count:        99
No  count:     1098	No  count:           874	No  count:       224
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:42:46 2017

Fitting 5 folds for each of 8 candidates, totalling 40 fits
### Start Time Tue Sep 19 19:46:02 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=940	randForSplit=319	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.49      0.97      0.65       423

avg / total       0.49      0.97      0.65       423

Train F2: 0.809

['yes', 'no']
[[409  14]
 [426 443]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.38      0.97      0.54        94

avg / total       0.38      0.97      0.54        94

Test  F2: 0.737

['yes', 'no']
[[ 91   3]
 [150  79]]

### Best Pipeline Parameters:
classifier__alpha: 10
classifier__eta0: 0.001
classifier__learning_rate: 'invscaling'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=10, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=940, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[10]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0012	knockout
+0.0010	develop
+0.0008	tissu
+0.0008	array
+0.0007	pool
+0.0007	profil
+0.0007	gene
+0.0007	mutant
+0.0006	wild
+0.0006	wild type
+0.0006	dissect
+0.0006	mice
+0.0006	mutant embryo
+0.0006	design
+0.0006	profil mous
+0.0006	genotyp
+0.0006	postnat
+0.0006	litterm
+0.0006	overal
+0.0006	knockout mice

### Top negative features (20)
-0.0005	ani public
-0.0005	data pre
-0.0005	includ ani
-0.0005	moratoria
-0.0005	moratoria pleas
-0.0005	pre public
-0.0005	public data
-0.0005	public moratoria
-0.0005	public releas
-0.0005	releas inform
-0.0005	use pre
-0.0007	stem
-0.0007	stem cell
-0.0008	cell
-0.0009	induc
-0.0010	embryon fibroblast
-0.0011	fibroblast
-0.0011	fibroblast mef
-0.0012	mous embryon
-0.0013	mef

### Vectorizer:   Number of Features: 16816
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'knockdown pgc', u'knockdown prdm2', u'knockdown sirna', u'knockin', u'knockin ki', u'knockout', u'knockout allel', u'knockout anim', u'knockout background', u'knockout caus']

Last 10 features: [u'zipper', u'zn', u'zona', u'zone', u'zone neural', u'zone svz', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 150
E-MIMR-2
E-GEOD-58086
E-MTAB-2354
E-TABM-304
E-GEOD-13870

### False negatives: 3
E-GEOD-6290
E-GEOD-17141
E-ERAD-169

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           423	Yes count:        94
No  count:     1098	No  count:           869	No  count:       229
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     29%
### End Time Tue Sep 19 19:46:10 2017

Fitting 5 folds for each of 8 candidates, totalling 40 fits
### Start Time Tue Sep 19 19:47:06 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=385	randForSplit=835	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.51      0.98      0.67       419

avg / total       0.51      0.98      0.67       419

Train F2: 0.823

['yes', 'no']
[[409  10]
 [399 474]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.41      0.97      0.58        98

avg / total       0.41      0.97      0.58        98

Test  F2: 0.765

['yes', 'no']
[[ 95   3]
 [134  91]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=385, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0123	develop
+0.0102	tissu
+0.0089	wild
+0.0084	mutant
+0.0081	gene
+0.0077	mice
+0.0077	pool
+0.0075	conclus
+0.0070	genotyp
+0.0069	knockout
+0.0069	postnat
+0.0069	dissect
+0.0067	liver
+0.0065	kidney
+0.0064	brain
+0.0064	litterm
+0.0063	litter
+0.0061	design
+0.0060	correspond
+0.0058	overal

### Top negative features (20)
-0.0054	unclear
-0.0055	inject
-0.0056	public
-0.0057	c2c12
-0.0061	line
-0.0062	transgen
-0.0063	moratoria
-0.0063	trust
-0.0063	wellcom
-0.0066	sort
-0.0067	treat
-0.0068	follow
-0.0069	pleas
-0.0069	treatment
-0.0070	primari
-0.0082	stem
-0.0102	cell
-0.0110	induc
-0.0133	fibroblast
-0.0150	mef

### Vectorizer:   Number of Features: 4327
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'lens', u'lentivir', u'lentivirus', u'leptin', u'lesion', u'let', u'lethal', u'leucin', u'leukem', u'leukemia']

Last 10 features: [u'young', u'yy1', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 134
E-GEOD-11887
E-GEOD-47735
E-CBIL-37
E-GEOD-7887
E-GEOD-30459

### False negatives: 3
E-GEOD-50119
E-ERAD-72
E-ERAD-520

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:47:13 2017

Fitting 5 folds for each of 48 candidates, totalling 240 fits
### Start Time Tue Sep 19 19:48:09 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=452	randForSplit=189	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.73      0.98      0.84       413

avg / total       0.73      0.98      0.84       413

Train F2: 0.918

['yes', 'no']
[[405   8]
 [149 730]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.95      0.72       104

avg / total       0.58      0.95      0.72       104

Test  F2: 0.843

['yes', 'no']
[[ 99   5]
 [ 72 147]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=452, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1, 1, 10]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0939	develop
+0.0707	tissu
+0.0649	genotyp
+0.0636	extens
+0.0628	mutant
+0.0590	dissect
+0.0568	kidney
+0.0543	conclus
+0.0529	pool
+0.0529	development
+0.0524	litterm
+0.0521	develop mous
+0.0519	brain
+0.0514	wild type
+0.0513	liver
+0.0509	wild
+0.0506	mous heart
+0.0480	knockout
+0.0468	defici mice
+0.0461	mutant embryo

### Top negative features (20)
-0.0453	embryon stem
-0.0497	activ
-0.0509	cell treat
-0.0528	unclear
-0.0545	inject
-0.0549	transgen
-0.0575	treat
-0.0585	sort
-0.0599	primari
-0.0618	infect
-0.0652	follow
-0.0776	stem
-0.0820	stem cell
-0.0865	cell
-0.0924	embryon fibroblast
-0.0945	fibroblast mef
-0.0980	induc
-0.1060	fibroblast
-0.1092	mous embryon
-0.1106	mef

### Vectorizer:   Number of Features: 16651
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene']

Middle 10 features: [u'ko condit', u'ko control', u'ko embryo', u'ko il6', u'ko ko', u'ko liver', u'ko mdr2', u'ko mef', u'ko mice', u'ko mous']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 72
E-GEOD-36899
E-GEOD-49122
E-GEOD-47395
E-GEOD-25506
E-GEOD-1052

### False negatives: 5
E-ERAD-231
E-ERAD-169
E-GEOD-27309
E-ERAD-237
E-ERAD-272

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           413	Yes count:       104
No  count:     1098	No  count:           879	No  count:       219
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Tue Sep 19 19:48:49 2017

Fitting 5 folds for each of 48 candidates, totalling 240 fits
### Start Time Tue Sep 19 19:50:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=441	randForSplit=609	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.98      0.78       407

avg / total       0.65      0.98      0.78       407

Train F2: 0.890

['yes', 'no']
[[400   7]
 [218 667]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.95      0.74       110

avg / total       0.60      0.95      0.74       110

Test  F2: 0.855

['yes', 'no']
[[105   5]
 [ 69 144]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=441, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.1028	develop
+0.0917	knockout
+0.0726	tissu
+0.0708	mous
+0.0641	mutant
+0.0633	liver
+0.0549	genotyp
+0.0548	overal
+0.0546	design
+0.0532	defect
+0.0501	brain
+0.0477	gene
+0.0472	litterm
+0.0469	development
+0.0459	exhibit
+0.0458	project
+0.0458	correspond
+0.0457	pool
+0.0456	dnase
+0.0448	dissect

### Top negative features (20)
-0.0482	line
-0.0483	fac
-0.0503	untreat
-0.0507	unclear
-0.0512	c2c12
-0.0513	infect
-0.0529	hour
-0.0537	follow
-0.0540	ani
-0.0543	inject
-0.0557	transgen
-0.0580	sort
-0.0586	cultur
-0.0620	primari
-0.0694	stem
-0.1089	treat
-0.1152	induc
-0.1213	fibroblast
-0.1354	mef
-0.1720	cell

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a10', u'a2', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'lcm', u'lcmv', u'ld', u'ldp', u'lead', u'lean', u'learn', u'leav', u'lectin', u'led']

Last 10 features: [u'yy1', u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 69
E-MTAB-4079
E-GEOD-6223
E-GEOD-70819
E-GEOD-60596
E-ERAD-201

### False negatives: 5
E-GEOD-43517
E-ERAD-72
E-MTAB-3707
E-ERAD-169
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           407	Yes count:       110
No  count:     1098	No  count:           885	No  count:       213
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     34%
### End Time Tue Sep 19 19:51:02 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 19:53:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=904	randForSplit=877	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.98      0.81       417

avg / total       0.69      0.98      0.81       417

Train F2: 0.900

['yes', 'no']
[[407  10]
 [186 689]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.92      0.69       100

avg / total       0.56      0.92      0.69       100

Test  F2: 0.814

['yes', 'no']
[[ 92   8]
 [ 73 150]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=904, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0919	develop
+0.0830	wild
+0.0821	tissu
+0.0749	mutant
+0.0620	knockout
+0.0615	conclus
+0.0606	male
+0.0594	genotyp
+0.0574	design
+0.0565	postnat
+0.0559	overal
+0.0548	defect
+0.0545	liver
+0.0538	litterm
+0.0506	mice
+0.0500	development
+0.0495	dissect
+0.0494	kidney
+0.0491	gene
+0.0485	seq

### Top negative features (20)
-0.0453	stimul
-0.0472	moratoria
-0.0488	unclear
-0.0501	infect
-0.0502	inject
-0.0520	differenti
-0.0526	ani
-0.0526	fac
-0.0551	activ
-0.0579	primari
-0.0595	treat
-0.0597	induct
-0.0610	sort
-0.0633	follow
-0.0660	transgen
-0.0723	stem
-0.0933	cell
-0.1050	induc
-0.1304	mef
-0.1311	fibroblast

### Vectorizer:   Number of Features: 4346
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'lef', u'left', u'leica', u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'leptin', u'lesion']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 73
E-ERAD-201
E-GEOD-5140
E-GEOD-40522
E-GEOD-8549
E-GEOD-21041

### False negatives: 8
E-GEOD-59777
E-GEOD-61582
E-GEOD-2362
E-ERAD-272
E-ERAD-169

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           417	Yes count:       100
No  count:     1098	No  count:           875	No  count:       223
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 19:54:37 2017

Fitting 5 folds for each of 108 candidates, totalling 540 fits
### Start Time Tue Sep 19 19:57:19 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=155	randForSplit=934	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.98      0.89       403

avg / total       0.81      0.98      0.89       403

Train F2: 0.942

['yes', 'no']
[[396   7]
 [ 95 794]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.92      0.74       114

avg / total       0.62      0.92      0.74       114

Test  F2: 0.840

['yes', 'no']
[[105   9]
 [ 64 145]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=155, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2', 'l1']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0658	embryon day
+0.0641	tissu
+0.0625	mutant
+0.0612	pool
+0.0611	liver
+0.0522	dissect
+0.0516	knockout
+0.0508	mous heart
+0.0508	genotyp
+0.0507	male
+0.0496	kidney
+0.0495	brain
+0.0494	postnat
+0.0494	development
+0.0493	mice studi
+0.0488	litterm
+0.0469	correspond
+0.0465	femal
+0.0463	profil mous

### Top negative features (20)
-0.0487	cell treat
-0.0498	inject
-0.0509	hour
-0.0528	transgen
-0.0528	embryon
-0.0530	unclear
-0.0560	primari
-0.0564	treatment
-0.0577	follow
-0.0602	sort
-0.0780	stem
-0.0789	stem cell
-0.0927	cell
-0.0983	induc
-0.1031	treat
-0.1088	fibroblast
-0.1112	fibroblast mef
-0.1187	mous embryon
-0.1200	embryon fibroblast
-0.1251	mef

### Vectorizer:   Number of Features: 16857
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'krt14 cre', u'kruppel', u'kruppel like', u'ksom', u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 64
E-GEOD-1482
E-GEOD-13691
E-GEOD-769
E-GEOD-2130
E-GEOD-66088

### False negatives: 9
E-GEOD-61367
E-GEOD-9913
E-GEOD-59777
E-ERAD-433
E-GEOD-45278

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 19:58:45 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 20:03:01 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=440	randForSplit=448	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.82      0.98      0.89       419

avg / total       0.82      0.98      0.89       419

Train F2: 0.946

['yes', 'no']
[[412   7]
 [ 90 783]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.90      0.75        98

avg / total       0.65      0.90      0.75        98

Test  F2: 0.833

['yes', 'no']
[[ 88  10]
 [ 48 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-06
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-06, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=440, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0889	develop
+0.0712	tissu
+0.0644	mutant
+0.0578	pool
+0.0535	postnat
+0.0525	mutant embryo
+0.0523	brain
+0.0515	genotyp
+0.0498	liver
+0.0497	dissect
+0.0486	extens
+0.0484	embryon day
+0.0476	design
+0.0471	kidney
+0.0459	e12
+0.0457	defici mice
+0.0452	litterm
+0.0448	development
+0.0442	knockout
+0.0441	wild type

### Top negative features (20)
-0.0482	activ
-0.0483	inject
-0.0508	untreat
-0.0510	transgen
-0.0512	cell treat
-0.0517	sort
-0.0552	unclear
-0.0585	treatment
-0.0590	stem
-0.0602	stem cell
-0.0603	primari
-0.0637	follow
-0.0982	induc
-0.1024	embryon fibroblast
-0.1043	treat
-0.1107	fibroblast mef
-0.1202	fibroblast
-0.1203	mous embryon
-0.1372	mef
-0.1613	cell

### Vectorizer:   Number of Features: 16902
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt', u'l5178i', u'l5178i cell', u'la', u'la jolla']

Last 10 features: [u'zipper', u'zona', u'zone', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot']

### False positives: 48
E-MARS-18
E-GEOD-3621
E-GEOD-54678
E-GEOD-57990
E-MTAB-3106

### False negatives: 10
E-GEOD-21860
E-GEOD-22125
E-MTAB-1404
E-ERAD-283
E-GEOD-7342

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:03:45 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 20:04:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=908	randForSplit=339	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.50      0.97      0.66       400

avg / total       0.50      0.97      0.66       400

Train F4: 0.918

['yes', 'no']
[[387  13]
 [382 510]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.50      0.95      0.65       117

avg / total       0.50      0.95      0.65       117

Test  F4: 0.900

['yes', 'no']
[[111   6]
 [113  93]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.0001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=908, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0086	develop
+0.0080	mice
+0.0064	tissu
+0.0061	gene
+0.0054	conclus
+0.0048	profil mous
+0.0048	mutant
+0.0047	knockout
+0.0046	genotyp
+0.0046	litterm
+0.0045	wild
+0.0044	wild type
+0.0043	appar
+0.0043	express
+0.0043	male
+0.0043	development
+0.0042	liver
+0.0042	mice studi
+0.0042	kidney
+0.0042	brain

### Top negative features (20)
-0.0044	includ ani
-0.0044	moratoria
-0.0044	moratoria pleas
-0.0044	public data
-0.0044	public moratoria
-0.0044	releas inform
-0.0044	use pre
-0.0045	pre public
-0.0046	data pre
-0.0046	public releas
-0.0047	primari
-0.0052	stem
-0.0052	stem cell
-0.0061	cell
-0.0069	induc
-0.0078	embryon fibroblast
-0.0079	fibroblast mef
-0.0084	fibroblast
-0.0090	mous embryon
-0.0092	mef

### Vectorizer:   Number of Features: 16778
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'knowledg pathogenesi', u'known', u'known essenti', u'known express', u'known gene', u'known housekeep', u'known howev', u'known involv', u'known molecular', u'known pathway']

Last 10 features: [u'zone', u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 113
E-GEOD-21003
E-MIMR-7
E-GEOD-7781
E-GEOD-49200
E-GEOD-53244

### False negatives: 6
E-GEOD-8360
E-GEOD-15795
E-GEOD-50119
E-GEOD-15794
E-ERAD-283

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           400	Yes count:       117
No  count:     1098	No  count:           892	No  count:       206
Percent Yes:    32%	Percent Yes:         30%	Percent Yes:     36%
### End Time Tue Sep 19 20:05:05 2017

Fitting 5 folds for each of 64 candidates, totalling 320 fits
### Start Time Tue Sep 19 20:07:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=124	randForSplit=246	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.50      0.98      0.66       410

avg / total       0.50      0.98      0.66       410

Train F4: 0.924

['yes', 'no']
[[400  10]
 [396 486]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.49      0.99      0.66       107

avg / total       0.49      0.99      0.66       107

Test  F4: 0.935

['yes', 'no']
[[106   1]
 [109 107]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=124, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0707	develop
+0.0607	knockout
+0.0582	tissu
+0.0514	litterm
+0.0504	mice
+0.0502	mutant
+0.0472	design
+0.0467	gene
+0.0446	genotyp
+0.0439	overal
+0.0425	wild
+0.0414	postnat
+0.0405	femal
+0.0391	male
+0.0389	development
+0.0379	e14
+0.0378	testi
+0.0371	appar
+0.0369	exhibit
+0.0364	liver

### Top negative features (20)
-0.0321	public
-0.0332	pre
-0.0336	transgen
-0.0338	fac
-0.0341	vitro
-0.0345	inject
-0.0357	trust
-0.0357	wellcom
-0.0370	follow
-0.0389	treat
-0.0398	moratoria
-0.0403	primari
-0.0432	pleas
-0.0511	infect
-0.0544	sort
-0.0620	cell
-0.0654	induc
-0.0716	stem
-0.0859	fibroblast
-0.0995	mef

### Vectorizer:   Number of Features: 4367
First 10 features: [u'a1', u'a10', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'learn', u'leav', u'lectin', u'led', u'lef', u'left', u'leica', u'len', u'length', u'lens']

Last 10 features: [u'zebrafish', u'zero', u'zfp36', u'zinc', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 109
E-GEOD-9740
E-GEOD-26764
E-GEOD-70562
E-GEOD-38001
E-GEOD-10235

### False negatives: 1
E-ERAD-72

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           410	Yes count:       107
No  count:     1098	No  count:           882	No  count:       216
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Tue Sep 19 20:08:24 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:10:49 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=941	randForSplit=470	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.52      0.98      0.68       418

avg / total       0.52      0.98      0.68       418

Train F4: 0.929

['yes', 'no']
[[408  10]
 [372 502]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.50      1.00      0.67        99

avg / total       0.50      1.00      0.67        99

Test  F4: 0.945

['yes', 'no']
[[ 99   0]
 [ 98 126]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=941, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001, 0.001, 0.01, 0.1]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0666	knockout
+0.0642	develop
+0.0597	tissu
+0.0490	mutant
+0.0426	litterm
+0.0424	genotyp
+0.0413	wild
+0.0409	liver
+0.0408	mice
+0.0395	goal
+0.0390	gene
+0.0385	kidney
+0.0372	development
+0.0369	dissect
+0.0365	appar
+0.0363	postnat
+0.0355	defect
+0.0354	identifi
+0.0347	brain
+0.0345	analys

### Top negative features (20)
-0.0297	vitro
-0.0306	pre
-0.0311	fluoresc
-0.0313	treatment
-0.0323	primari
-0.0334	unclear
-0.0339	infect
-0.0345	transgen
-0.0350	line
-0.0352	c2c12
-0.0358	fac
-0.0383	inject
-0.0402	follow
-0.0425	treat
-0.0473	sort
-0.0502	stem
-0.0639	cell
-0.0683	induc
-0.0816	fibroblast
-0.0914	mef

### Vectorizer:   Number of Features: 4354
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'left', u'leica', u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin', u'lesion']

Last 10 features: [u'young', u'yy1', u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 98
E-ERAD-230
E-BAIR-13
E-ERAD-407
E-GEOD-53480
E-AFMX-12

### False negatives: 0

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           418	Yes count:        99
No  count:     1098	No  count:           874	No  count:       224
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:11:08 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:12:23 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=692	randForSplit=965	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.44      0.98      0.61       412

avg / total       0.44      0.98      0.61       412

Train F4: 0.915

['yes', 'no']
[[404   8]
 [513 367]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.43      0.96      0.59       105

avg / total       0.43      0.96      0.59       105

Test  F4: 0.896

['yes', 'no']
[[101   4]
 [136  82]]

### Best Pipeline Parameters:
classifier__alpha: 1e-05
classifier__eta0: 1e-06
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1e-05, average=False, class_weight='balanced',
       epsilon=0.1, eta0=1e-06, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=692, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05, 0.0001, 0.001, 0.01]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0001	develop
+0.0001	tissu
+0.0001	mutant
+0.0001	mice
+0.0001	express
+0.0001	gene
+0.0000	genotyp
+0.0000	old
+0.0000	knockout
+0.0000	litterm
+0.0000	liver
+0.0000	wild
+0.0000	kidney
+0.0000	defect
+0.0000	pool
+0.0000	e14
+0.0000	development
+0.0000	dissect
+0.0000	heart
+0.0000	litter

### Top negative features (20)
-0.0000	infect
-0.0000	c2c12
-0.0000	inject
-0.0000	fac
-0.0000	transgen
-0.0000	line
-0.0000	sort
-0.0000	public
-0.0000	primari
-0.0000	follow
-0.0000	treat
-0.0000	pleas
-0.0000	moratoria
-0.0000	trust
-0.0000	wellcom
-0.0001	stem
-0.0001	cell
-0.0001	induc
-0.0001	fibroblast
-0.0001	mef

### Vectorizer:   Number of Features: 4331
First 10 features: [u'a2', u'aa', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat', u'abnorm']

Middle 10 features: [u'ligat', u'light', u'like', u'likewis', u'lim', u'lim1', u'limb', u'limit', u'limma', u'lin']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 136
E-AFMX-13
E-GEOD-12073
E-GEOD-46298
E-GEOD-31945
E-GEOD-49975

### False negatives: 4
E-ERAD-283
E-ERAD-237
E-ERAD-520
E-ERAD-71

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           412	Yes count:       105
No  count:     1098	No  count:           880	No  count:       218
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Tue Sep 19 20:12:42 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:13:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=308	randForSplit=566	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.96      0.87       420

avg / total       0.80      0.96      0.87       420

Train F2: 0.926

['yes', 'no']
[[405  15]
 [101 771]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.89      0.76        97

avg / total       0.66      0.89      0.76        97

Test  F2: 0.830

['yes', 'no']
[[ 86  11]
 [ 44 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=308, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05, 0.0001, 0.001, 0.01]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3644	develop
+0.3073	tissu
+0.2563	liver
+0.2430	mutant
+0.2406	heart
+0.2390	postnat
+0.2240	knockout
+0.2125	litterm
+0.2048	kidney
+0.2047	dissect
+0.2019	development
+0.1998	wild
+0.1925	design
+0.1863	overal
+0.1841	e12
+0.1787	consist
+0.1724	bud
+0.1669	pool
+0.1662	genotyp
+0.1621	box

### Top negative features (20)
-0.1679	myoblast
-0.1920	treatment
-0.1925	stimul
-0.1960	infect
-0.2020	fac
-0.2047	unclear
-0.2064	differenti
-0.2181	activ
-0.2271	c2c12
-0.2337	inject
-0.2361	primari
-0.2579	treat
-0.2590	sort
-0.2698	transgen
-0.2764	follow
-0.2922	stem
-0.3963	induc
-0.4142	cell
-0.5183	fibroblast
-0.5604	mef

### Vectorizer:   Number of Features: 4417
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'ablat', u'abnorm']

Middle 10 features: [u'life', u'lifespan', u'lifetim', u'ligand', u'ligas', u'ligat', u'light', u'like', u'likewis', u'lim']

Last 10 features: [u'zero', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 44
E-GEOD-13691
E-GEOD-60780
E-GEOD-59848
E-GEOD-59463
E-MTAB-1569

### False negatives: 11
E-GEOD-59777
E-MTAB-5020
E-ERAD-237
E-GEOD-8969
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           420	Yes count:        97
No  count:     1098	No  count:           872	No  count:       226
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:13:41 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:15:31 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=728	randForSplit=842	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.74       403

avg / total       0.60      0.98      0.74       403

Train F2: 0.867

['yes', 'no']
[[393  10]
 [261 628]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.99      0.73       114

avg / total       0.58      0.99      0.73       114

Test  F2: 0.869

['yes', 'no']
[[113   1]
 [ 81 128]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=728, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1071	develop
+0.0769	tissu
+0.0738	mutant
+0.0695	knockout
+0.0644	defect
+0.0641	heart
+0.0625	exhibit
+0.0607	postnat
+0.0597	design
+0.0581	liver
+0.0575	kidney
+0.0566	conclus
+0.0560	pool
+0.0553	litterm
+0.0539	e12
+0.0536	overal
+0.0535	gene
+0.0532	genotyp
+0.0499	wild
+0.0493	development

### Top negative features (20)
-0.0441	fac
-0.0443	treatment
-0.0445	activ
-0.0455	hour
-0.0465	c2c12
-0.0486	infect
-0.0487	line
-0.0522	unclear
-0.0526	treat
-0.0552	transgen
-0.0567	primari
-0.0588	inject
-0.0608	sort
-0.0621	respons
-0.0770	follow
-0.0789	stem
-0.0955	cell
-0.1010	induc
-0.1263	fibroblast
-0.1423	mef

### Vectorizer:   Number of Features: 4349
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lead', u'lean', u'learn', u'leav', u'led', u'lef', u'left', u'leica', u'len', u'length']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 81
E-MTAB-2266
E-GEOD-12309
E-ERAD-201
E-GEOD-29632
E-CBIL-39

### False negatives: 1
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:15:37 2017

Interesting 60% precision, high recall
Try some more repetitions
Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:17:59 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=492	randForSplit=650	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.68      0.96      0.79       401

avg / total       0.68      0.96      0.79       401

Train F2: 0.884

['yes', 'no']
[[383  18]
 [180 711]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       116

avg / total       0.61      0.89      0.72       116

Test  F2: 0.814

['yes', 'no']
[[103  13]
 [ 66 141]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=492, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0987	develop
+0.0793	tissu
+0.0739	mutant
+0.0634	liver
+0.0617	mice
+0.0572	exhibit
+0.0563	genotyp
+0.0548	knockout
+0.0546	brain
+0.0542	appar
+0.0542	postnat
+0.0533	gene
+0.0520	conclus
+0.0515	development
+0.0512	litter
+0.0511	litterm
+0.0506	dissect
+0.0504	wild
+0.0491	defect
+0.0480	correspond

### Top negative features (20)
-0.0474	myoblast
-0.0497	untreat
-0.0500	fac
-0.0522	c2c12
-0.0523	hour
-0.0524	line
-0.0538	transgen
-0.0546	unclear
-0.0558	activ
-0.0582	follow
-0.0591	inject
-0.0606	sort
-0.0634	primari
-0.0647	ani
-0.0851	stem
-0.1005	cell
-0.1084	treat
-0.1116	induc
-0.1320	mef
-0.1414	fibroblast

### Vectorizer:   Number of Features: 4380
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukocyt', u'level', u'leverag', u'leydig', u'li', u'libitum']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 66
E-GEOD-20570
E-GEOD-62758
E-GEOD-57133
E-TABM-304
E-GEOD-55247

### False negatives: 13
E-ERAD-71
E-GEOD-8360
E-GEOD-15795
E-ERAD-381
E-GEOD-15794

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           401	Yes count:       116
No  count:     1098	No  count:           891	No  count:       207
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:18:05 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:18:58 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=362	randForSplit=345	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.86       409

avg / total       0.76      0.98      0.86       409

Train F2: 0.926

['yes', 'no']
[[401   8]
 [128 755]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.64      0.90      0.75       108

avg / total       0.64      0.90      0.75       108

Test  F2: 0.830

['yes', 'no']
[[ 97  11]
 [ 55 160]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=362, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3412	develop
+0.2805	tissu
+0.2515	liver
+0.2455	mutant
+0.2253	knockout
+0.2087	moe430
+0.2085	male
+0.2082	genotyp
+0.2067	postnat
+0.2031	litterm
+0.1961	pool
+0.1926	appar
+0.1916	kidney
+0.1912	dissect
+0.1888	femal
+0.1877	embryo
+0.1829	brain
+0.1776	development
+0.1678	gene
+0.1619	heart

### Top negative features (20)
-0.1963	myoblast
-0.1989	infect
-0.2000	treatment
-0.2001	unclear
-0.2026	embryon
-0.2031	differenti
-0.2040	fac
-0.2089	c2c12
-0.2458	cultur
-0.2464	inject
-0.2467	sort
-0.2503	transgen
-0.2544	treat
-0.2597	follow
-0.2740	primari
-0.3155	stem
-0.3595	cell
-0.4668	induc
-0.4799	fibroblast
-0.5304	mef

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin', u'lesion', u'let', u'lethal', u'leucin', u'leukaemia']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 55
E-MTAB-4349
E-GEOD-42389
E-GEOD-74317
E-GEOD-54924
E-MTAB-1569

### False negatives: 11
E-ERAD-278
E-GEOD-45278
E-TABM-280
E-ERAD-272
E-MTAB-3976

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Tue Sep 19 20:19:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:25:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=203	randForSplit=241	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       408

avg / total       0.79      0.97      0.87       408

Train F2: 0.924

['yes', 'no']
[[394  14]
 [106 778]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.87      0.76       109

avg / total       0.67      0.87      0.76       109

Test  F2: 0.823

['yes', 'no']
[[ 95  14]
 [ 46 168]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=203, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:25:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=50	randForSplit=953	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.83      0.97      0.89       426

avg / total       0.83      0.97      0.89       426

Train F2: 0.935

['yes', 'no']
[[412  14]
 [ 87 779]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.90      0.71        91

avg / total       0.59      0.90      0.71        91

Test  F2: 0.813

['yes', 'no']
[[ 82   9]
 [ 58 174]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=50, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=614	randForSplit=50	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.97      0.88       415

avg / total       0.81      0.97      0.88       415

Train F2: 0.931

['yes', 'no']
[[401  14]
 [ 93 784]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.81      0.73       102

avg / total       0.66      0.81      0.73       102

Test  F2: 0.779

['yes', 'no']
[[ 83  19]
 [ 42 179]]

### Best Pipeline Parameters:
classifier__alpha: 1e-05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1e-05, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=614, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:34 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:27:34 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=799	randForSplit=847	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.87       406

avg / total       0.78      0.97      0.87       406

Train F2: 0.928

['yes', 'no']
[[395  11]
 [109 777]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.71      0.93      0.80       111

avg / total       0.71      0.93      0.80       111

Test  F2: 0.874

['yes', 'no']
[[103   8]
 [ 42 170]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=799, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:27:35 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Wed Sep 20 11:28:00 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=428	randForSplit=34	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.95      0.80       415

avg / total       0.69      0.95      0.80       415

Train F2: 0.888

['yes', 'no']
[[396  19]
 [174 703]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.93      0.71       102

avg / total       0.58      0.93      0.71       102

Test  F2: 0.829

['yes', 'no']
[[ 95   7]
 [ 70 151]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=428, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:28:02 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:28:38 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=531	randForSplit=263	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.75       414

avg / total       0.60      0.98      0.75       414

Train F2: 0.869

['yes', 'no']
[[404  10]
 [265 613]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.94      0.66       103

avg / total       0.51      0.94      0.66       103

Test  F2: 0.806

['yes', 'no']
[[ 97   6]
 [ 93 127]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=531, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:28:40 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:29:11 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=92	randForSplit=537	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.56      0.86      0.68       403

avg / total       0.56      0.86      0.68       403

Train F2: 0.775

['yes', 'no']
[[345  58]
 [270 619]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.77      0.68       114

avg / total       0.61      0.77      0.68       114

Test  F2: 0.732

['yes', 'no']
[[ 88  26]
 [ 57 152]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=92, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:29:12 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:29:42 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=628	randForSplit=666	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.59      0.98      0.74       404

avg / total       0.59      0.98      0.74       404

Train F2: 0.868

['yes', 'no']
[[397   7]
 [274 614]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.93      0.71       113

avg / total       0.58      0.93      0.71       113

Test  F2: 0.828

['yes', 'no']
[[105   8]
 [ 77 133]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=628, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:29:43 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:30:05 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=338	randForSplit=378	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.42      0.99      0.58       402

avg / total       0.42      0.99      0.58       402

Train F2: 0.774

['yes', 'no']
[[397   5]
 [559 331]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.43      0.98      0.60       115

avg / total       0.43      0.98      0.60       115

Test  F2: 0.781

['yes', 'no']
[[113   2]
 [150  58]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=338, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:30:07 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Wed Sep 20 11:30:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=285	randForSplit=986	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.57      0.97      0.72       420

avg / total       0.57      0.97      0.72       420

Train F2: 0.849

['yes', 'no']
[[406  14]
 [305 567]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.48      0.94      0.64        97

avg / total       0.48      0.94      0.64        97

Test  F2: 0.789

['yes', 'no']
[[ 91   6]
 [ 98 128]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=285, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:30:35 2017

Fitting 5 folds for each of 8 candidates, totalling 40 fits
### Start Time Wed Sep 20 11:31:59 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=961	randForSplit=225	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.34      0.99      0.51       394

avg / total       0.34      0.99      0.51       394

Train F2: 0.719

['yes', 'no']
[[389   5]
 [739 159]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.41      0.99      0.59       123

avg / total       0.41      0.99      0.59       123

Test  F2: 0.776

['yes', 'no']
[[122   1]
 [172  28]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=961, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:32:03 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Wed Sep 20 11:32:46 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=908	randForSplit=686	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.48      0.97      0.64       423

avg / total       0.48      0.97      0.64       423

Train F2: 0.808

['yes', 'no']
[[412  11]
 [445 424]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.41      0.97      0.58        94

avg / total       0.41      0.97      0.58        94

Test  F2: 0.762

['yes', 'no']
[[ 91   3]
 [130  99]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=908, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:32:52 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:34:20 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=341	randForSplit=707	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.89      0.97      0.93       406

avg / total       0.89      0.97      0.93       406

Train F2: 0.954

['yes', 'no']
[[394  12]
 [ 48 838]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.72      0.80      0.76       111

avg / total       0.72      0.80      0.76       111

Test  F2: 0.783

['yes', 'no']
[[ 89  22]
 [ 35 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=341, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:34:22 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:34:50 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=47	randForSplit=91	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.97      0.78       408

avg / total       0.65      0.97      0.78       408

Train F2: 0.886

['yes', 'no']
[[397  11]
 [212 672]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       109

avg / total       0.61      0.89      0.72       109

Test  F2: 0.815

['yes', 'no']
[[ 97  12]
 [ 62 152]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=47, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:34:52 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:35:17 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=651	randForSplit=170	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.51      0.98      0.67       415

avg / total       0.51      0.98      0.67       415

Train F3: 0.896

['yes', 'no']
[[406   9]
 [389 488]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.45      0.96      0.61       102

avg / total       0.45      0.96      0.61       102

Test  F3: 0.861

['yes', 'no']
[[ 98   4]
 [122  99]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=651, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:35:19 2017

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Start Time Wed Sep 20 11:36:40 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=4	randForSplit=444	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.74       416

avg / total       0.60      0.98      0.74       416

Train F3: 0.919

['yes', 'no']
[[407   9]
 [277 599]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.49      0.98      0.65       101

avg / total       0.49      0.98      0.65       101

Test  F3: 0.890

['yes', 'no']
[[ 99   2]
 [104 118]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=4, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:36:53 2017

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Start Time Wed Sep 20 11:37:40 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=141	randForSplit=180	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.53      0.98      0.68       413

avg / total       0.53      0.98      0.68       413

Train F3: 0.899

['yes', 'no']
[[403  10]
 [361 518]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.48      0.96      0.64       104

avg / total       0.48      0.96      0.64       104

Test  F3: 0.873

['yes', 'no']
[[100   4]
 [110 109]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=141, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.5, 1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:37:53 2017

Fitting 5 folds for each of 27 candidates, totalling 135 fits
### Start Time Wed Sep 20 11:41:11 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=146	randForSplit=52	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.52      0.98      0.68       421

avg / total       0.52      0.98      0.68       421

Train F3: 0.901

['yes', 'no']
[[413   8]
 [380 491]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.47      0.98      0.64        96

avg / total       0.47      0.98      0.64        96

Test  F3: 0.883

['yes', 'no']
[[ 94   2]
 [106 121]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=146, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:41:24 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Wed Sep 20 11:42:32 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=548	randForSplit=487	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.52      0.97      0.68       419

avg / total       0.52      0.97      0.68       419

Train F3: 0.895

['yes', 'no']
[[407  12]
 [372 501]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.43      0.92      0.59        98

avg / total       0.43      0.92      0.59        98

Test  F3: 0.826

['yes', 'no']
[[ 90   8]
 [117 108]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=548, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001, 0.001]
classifier__eta0:[0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:42:38 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:19:49 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=589	randForSplit=542	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       416

avg / total       0.79      0.97      0.87       416

Train F2: 0.930

['yes', 'no']
[[405  11]
 [109 767]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.88      0.76       101

avg / total       0.67      0.88      0.76       101

Test  F2: 0.829

['yes', 'no']
[[ 89  12]
 [ 44 178]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=589, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3810	develop
+0.3597	tissu
+0.2681	mutant
+0.2388	femal
+0.2346	male
+0.2319	pool
+0.2291	insight
+0.2286	extens
+0.2212	litterm
+0.2107	knockout
+0.2045	genotyp
+0.1997	dnase
+0.1985	postnat
+0.1976	mice
+0.1964	brain
+0.1963	liver
+0.1943	development
+0.1936	wild
+0.1872	testi
+0.1854	overal

### Top negative features (20)
-0.1819	myoblast
-0.1844	unclear
-0.1846	infect
-0.1851	ani
-0.2027	differenti
-0.2061	fac
-0.2140	activ
-0.2187	c2c12
-0.2222	sort
-0.2405	primari
-0.2417	inject
-0.2457	treatment
-0.2499	treat
-0.2620	follow
-0.2644	transgen
-0.2885	stem
-0.4336	induc
-0.5131	fibroblast
-0.5166	mef
-0.7319	cell

### Vectorizer:   Number of Features: 4375
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lc', u'lck', u'lcm', u'lcmv', u'ldlr', u'lead', u'lean', u'learn', u'leav', u'lectin']

Last 10 features: [u'yy1', u'zebrafish', u'zfp36', u'zinc', u'zipper', u'zn', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 44
E-GEOD-35989
E-GEOD-2917
E-GEOD-28207
E-GEOD-5583
E-GEOD-760

### False negatives: 12
E-MNIA-66
E-ERAD-433
E-GEOD-52118
E-TABM-426
E-GEOD-10498

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           416	Yes count:       101
No  count:     1098	No  count:           876	No  count:       222
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     31%
### End Time Wed Sep 20 12:19:51 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:20:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=444	randForSplit=744	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.82      0.97      0.89       422

avg / total       0.82      0.97      0.89       422

Train F2: 0.934

['yes', 'no']
[[408  14]
 [ 89 781]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.85      0.75        95

avg / total       0.67      0.85      0.75        95

Test  F2: 0.808

['yes', 'no']
[[ 81  14]
 [ 40 188]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=444, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:20:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:20:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=777	randForSplit=411	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.98      0.86       414

avg / total       0.77      0.98      0.86       414

Train F2: 0.929

['yes', 'no']
[[405   9]
 [118 760]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.63      0.90      0.74       103

avg / total       0.63      0.90      0.74       103

Test  F2: 0.830

['yes', 'no']
[[ 93  10]
 [ 55 165]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=777, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:20:25 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:20:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=996	randForSplit=493	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.98      0.88       412

avg / total       0.80      0.98      0.88       412

Train F2: 0.934

['yes', 'no']
[[402  10]
 [103 777]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.91      0.78       105

avg / total       0.68      0.91      0.78       105

Test  F2: 0.854

['yes', 'no']
[[ 96   9]
 [ 46 172]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=996, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:20:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:21:01 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=782	randForSplit=467	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.97      0.88       419

avg / total       0.80      0.97      0.88       419

Train F2: 0.933

['yes', 'no']
[[408  11]
 [102 771]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.88      0.73        98

avg / total       0.62      0.88      0.73        98

Test  F2: 0.810

['yes', 'no']
[[ 86  12]
 [ 53 172]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=782, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:21:03 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:21:29 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=785	randForSplit=312	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.98      0.87       415

avg / total       0.78      0.98      0.87       415

Train F2: 0.935

['yes', 'no']
[[408   7]
 [114 763]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.64      0.91      0.75       102

avg / total       0.64      0.91      0.75       102

Test  F2: 0.841

['yes', 'no']
[[ 93   9]
 [ 52 169]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=785, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:21:30 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:23:02 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=632	randForSplit=197	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.95      0.87       418

avg / total       0.80      0.95      0.87       418

Train F2: 0.919

['yes', 'no']
[[399  19]
 [100 774]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.91      0.78        99

avg / total       0.68      0.91      0.78        99

Test  F2: 0.852

['yes', 'no']
[[ 90   9]
 [ 42 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=632, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:23:03 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:23:16 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=82	randForSplit=649	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.96      0.87       400

avg / total       0.79      0.96      0.87       400

Train F2: 0.923

['yes', 'no']
[[385  15]
 [100 792]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.74      0.85      0.79       117

avg / total       0.74      0.85      0.79       117

Test  F2: 0.824

['yes', 'no']
[[ 99  18]
 [ 34 172]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=82, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:23:17 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:23:29 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=80	randForSplit=751	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.97      0.86       406

avg / total       0.77      0.97      0.86       406

Train F2: 0.919

['yes', 'no']
[[392  14]
 [116 770]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.92      0.77       111

avg / total       0.67      0.92      0.77       111

Test  F2: 0.854

['yes', 'no']
[[102   9]
 [ 51 161]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=80, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:23:30 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:23:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=545	randForSplit=146	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       413

avg / total       0.79      0.97      0.87       413

Train F2: 0.930

['yes', 'no']
[[402  11]
 [107 772]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.90      0.77       104

avg / total       0.68      0.90      0.77       104

Test  F2: 0.847

['yes', 'no']
[[ 94  10]
 [ 45 174]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=545, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:23:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:24:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=67	randForSplit=960	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       422

avg / total       0.79      0.97      0.87       422

Train F3: 0.951

['yes', 'no']
[[411  11]
 [111 759]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.87      0.71        95

avg / total       0.60      0.87      0.71        95

Test  F3: 0.836

['yes', 'no']
[[ 83  12]
 [ 55 173]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=67, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:24:23 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:24:38 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 3
Random Seeds:	randForClassifier=420	randForSplit=198	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.97      0.86       414

avg / total       0.77      0.97      0.86       414

Train F3: 0.947

['yes', 'no']
[[402  12]
 [118 760]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.70      0.88      0.78       103

avg / total       0.70      0.88      0.78       103

Test  F3: 0.861

['yes', 'no']
[[ 91  12]
 [ 39 181]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=420, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:24:39 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:25:04 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=123	randForSplit=624	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.96      0.88       415

avg / total       0.81      0.96      0.88       415

Train F4: 0.949

['yes', 'no']
[[398  17]
 [ 93 784]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.69      0.91      0.78       102

avg / total       0.69      0.91      0.78       102

Test  F4: 0.895

['yes', 'no']
[[ 93   9]
 [ 42 179]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=123, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:25:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:25:16 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=55	randForSplit=39	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.97      0.86       427

avg / total       0.77      0.97      0.86       427

Train F4: 0.957

['yes', 'no']
[[415  12]
 [126 739]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.69      0.91      0.78        90

avg / total       0.69      0.91      0.78        90

Test  F4: 0.894

['yes', 'no']
[[ 82   8]
 [ 37 196]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=55, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:25:17 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:25:26 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=10	randForSplit=459	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.97      0.86       404

avg / total       0.77      0.97      0.86       404

Train F4: 0.954

['yes', 'no']
[[391  13]
 [114 774]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.76      0.91      0.83       113

avg / total       0.76      0.91      0.83       113

Test  F4: 0.901

['yes', 'no']
[[103  10]
 [ 33 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=10, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:25:27 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:25:34 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=546	randForSplit=412	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.98      0.87       412

avg / total       0.78      0.98      0.87       412

Train F4: 0.962

['yes', 'no']
[[402  10]
 [111 769]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.70      0.92      0.80       105

avg / total       0.70      0.92      0.80       105

Test  F4: 0.907

['yes', 'no']
[[ 97   8]
 [ 42 176]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=546, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:25:36 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:25:44 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 4
Random Seeds:	randForClassifier=623	randForSplit=886	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.96      0.87       418

avg / total       0.79      0.96      0.87       418

Train F4: 0.952

['yes', 'no']
[[403  15]
 [109 765]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.87      0.76        99

avg / total       0.67      0.87      0.76        99

Test  F4: 0.854

['yes', 'no']
[[ 86  13]
 [ 42 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=623, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:25:45 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:26:02 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 5
Random Seeds:	randForClassifier=571	randForSplit=347	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.87       403

avg / total       0.78      0.97      0.87       403

Train F5: 0.964

['yes', 'no']
[[392  11]
 [109 780]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.71      0.89      0.79       114

avg / total       0.71      0.89      0.79       114

Test  F5: 0.878

['yes', 'no']
[[101  13]
 [ 41 168]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=571, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:26:03 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:26:12 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 5
Random Seeds:	randForClassifier=713	randForSplit=717	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       423

avg / total       0.79      0.97      0.87       423

Train F5: 0.961

['yes', 'no']
[[410  13]
 [109 760]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.87      0.75        94

avg / total       0.66      0.87      0.75        94

Test  F5: 0.861

['yes', 'no']
[[ 82  12]
 [ 43 186]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=713, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:26:14 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:26:26 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 5
Random Seeds:	randForClassifier=245	randForSplit=106	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.86       418

avg / total       0.78      0.97      0.86       418

Train F5: 0.957

['yes', 'no']
[[404  14]
 [117 757]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.87      0.76        99

avg / total       0.68      0.87      0.76        99

Test  F5: 0.859

['yes', 'no']
[[ 86  13]
 [ 41 183]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=245, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:26:27 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:26:44 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=519	randForSplit=608	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.96      0.87       419

avg / total       0.78      0.96      0.87       419

Train F6: 0.958

['yes', 'no']
[[404  15]
 [111 762]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.68      0.92      0.78        98

avg / total       0.68      0.92      0.78        98

Test  F6: 0.910

['yes', 'no']
[[ 90   8]
 [ 43 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=519, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:26:45 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:26:58 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=635	randForSplit=560	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.77      0.97      0.86       417

avg / total       0.77      0.97      0.86       417

Train F6: 0.967

['yes', 'no']
[[406  11]
 [119 756]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.89      0.75       100

avg / total       0.65      0.89      0.75       100

Test  F6: 0.881

['yes', 'no']
[[ 89  11]
 [ 48 175]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=635, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:27:00 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:27:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=237	randForSplit=69	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       436

avg / total       0.79      0.97      0.87       436

Train F6: 0.964

['yes', 'no']
[[423  13]
 [110 746]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.91      0.69        81

avg / total       0.56      0.91      0.69        81

Test  F6: 0.898

['yes', 'no']
[[ 74   7]
 [ 58 184]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=237, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:27:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:27:34 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=372	randForSplit=976	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.96      0.88       409

avg / total       0.81      0.96      0.88       409

Train F6: 0.956

['yes', 'no']
[[393  16]
 [ 93 790]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.86      0.75       108

avg / total       0.67      0.86      0.75       108

Test  F6: 0.854

['yes', 'no']
[[ 93  15]
 [ 46 169]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=372, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:27:35 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:27:56 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=416	randForSplit=220	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.71      0.98      0.82       410

avg / total       0.71      0.98      0.82       410

Train F6: 0.970

['yes', 'no']
[[402   8]
 [165 717]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.90      0.71       107

avg / total       0.58      0.90      0.71       107

Test  F6: 0.884

['yes', 'no']
[[ 96  11]
 [ 69 147]]

### Best Pipeline Parameters:
classifier__alpha: 0.05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.05, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=416, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:27:57 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:28:07 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=321	randForSplit=432	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.72      0.97      0.83       420

avg / total       0.72      0.97      0.83       420

Train F6: 0.958

['yes', 'no']
[[406  14]
 [157 715]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.95      0.73        97

avg / total       0.59      0.95      0.73        97

Test  F6: 0.933

['yes', 'no']
[[ 92   5]
 [ 64 162]]

### Best Pipeline Parameters:
classifier__alpha: 0.05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.05, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=321, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:28:08 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:28:15 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=135	randForSplit=659	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.97      0.81       425

avg / total       0.69      0.97      0.81       425

Train F6: 0.957

['yes', 'no']
[[411  14]
 [184 683]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.96      0.75        92

avg / total       0.62      0.96      0.75        92

Test  F6: 0.943

['yes', 'no']
[[ 88   4]
 [ 54 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.05, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=135, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:28:17 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:28:38 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=426	randForSplit=440	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.72      0.97      0.82       407

avg / total       0.72      0.97      0.82       407

Train F6: 0.961

['yes', 'no']
[[395  12]
 [157 728]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.89      0.76       110

avg / total       0.66      0.89      0.76       110

Test  F6: 0.882

['yes', 'no']
[[ 98  12]
 [ 51 162]]

### Best Pipeline Parameters:
classifier__alpha: 0.05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.05, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=426, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:28:39 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:28:44 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=999	randForSplit=797	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.72      0.97      0.83       407

avg / total       0.72      0.97      0.83       407

Train F6: 0.957

['yes', 'no']
[[393  14]
 [152 733]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.69      0.98      0.81       110

avg / total       0.69      0.98      0.81       110

Test  F6: 0.971

['yes', 'no']
[[108   2]
 [ 49 164]]

### Best Pipeline Parameters:
classifier__alpha: 0.05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.05, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=999, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:28:46 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:32:06 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=367	randForSplit=523	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.97      0.80       429

avg / total       0.69      0.97      0.80       429

Train F6: 0.955

['yes', 'no']
[[414  15]
 [189 674]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.55      0.95      0.70        88

avg / total       0.55      0.95      0.70        88

Test  F6: 0.936

['yes', 'no']
[[ 84   4]
 [ 68 167]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=367, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:32:08 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:32:18 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=731	randForSplit=397	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.64      0.99      0.78       411

avg / total       0.64      0.99      0.78       411

Train F6: 0.972

['yes', 'no']
[[405   6]
 [223 658]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.95      0.71       106

avg / total       0.56      0.95      0.71       106

Test  F6: 0.935

['yes', 'no']
[[101   5]
 [ 78 139]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=731, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:32:19 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:32:26 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=897	randForSplit=112	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.62      0.99      0.76       420

avg / total       0.62      0.99      0.76       420

Train F6: 0.970

['yes', 'no']
[[414   6]
 [252 620]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.97      0.66        97

avg / total       0.51      0.97      0.66        97

Test  F6: 0.946

['yes', 'no']
[[ 94   3]
 [ 92 134]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=897, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:32:27 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:32:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=5	randForSplit=555	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.99      0.75       413

avg / total       0.60      0.99      0.75       413

Train F6: 0.973

['yes', 'no']
[[409   4]
 [272 607]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.51      0.96      0.66       104

avg / total       0.51      0.96      0.66       104

Test  F6: 0.939

['yes', 'no']
[[100   4]
 [ 97 122]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=5, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:32:34 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:32:41 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=219	randForSplit=820	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.58      0.99      0.73       402

avg / total       0.58      0.99      0.73       402

Train F6: 0.969

['yes', 'no']
[[397   5]
 [291 599]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.55      0.97      0.70       115

avg / total       0.55      0.97      0.70       115

Test  F6: 0.946

['yes', 'no']
[[111   4]
 [ 90 118]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=219, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:32:43 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:33:11 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=315	randForSplit=156	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.97      0.80       402

avg / total       0.69      0.97      0.80       402

Train F6: 0.955

['yes', 'no']
[[388  14]
 [177 713]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.93      0.78       115

avg / total       0.66      0.93      0.78       115

Test  F6: 0.920

['yes', 'no']
[[107   8]
 [ 54 154]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=315, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 12:33:12 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:48:15 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=315	randForSplit=144	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.70      0.95      0.81       419

avg / total       0.70      0.95      0.81       419

Train F6: 0.941

['yes', 'no']
[[398  21]
 [169 704]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.69      0.90      0.78        98

avg / total       0.69      0.90      0.78        98

Test  F6: 0.891

['yes', 'no']
[[ 88  10]
 [ 39 186]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=315, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1354	develop
+0.0875	mutant
+0.0762	tissu
+0.0643	embryo
+0.0610	heart
+0.0593	dissect
+0.0546	kidney
+0.0542	genotyp
+0.0534	design
+0.0530	wild
+0.0526	defect
+0.0520	defici
+0.0513	hybrid
+0.0512	pool
+0.0510	e16
+0.0500	muscl
+0.0498	overal
+0.0491	liver
+0.0488	consist
+0.0485	brain

### Top negative features (20)
-0.0531	primari
-0.0552	sort
-0.0560	fac
-0.0574	line
-0.0590	differenti
-0.0603	pre
-0.0603	c2c12
-0.0621	activ
-0.0627	stem
-0.0675	infect
-0.0678	respons
-0.0707	follow
-0.0708	cultur
-0.0732	transgen
-0.0810	treatment
-0.0928	treat
-0.0982	fibroblast
-0.0996	mef
-0.1248	induc
-0.2636	cell

### Vectorizer:   Number of Features: 4351
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'leav', u'lectin', u'led', u'lef', u'left', u'leica', u'len', u'length', u'lens', u'lentivir']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 39
E-GEOD-6323
E-GEOD-39619
E-GEOD-1482
E-MTAB-27
E-BAIR-4

### False negatives: 10
E-ERAD-71
E-MTAB-867
E-ERAD-520
E-ERAD-272
E-GEOD-14605

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Wed Sep 20 12:48:17 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:48:45 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=315	randForSplit=955	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.75       411

avg / total       0.60      0.98      0.75       411

Train F6: 0.966

['yes', 'no']
[[404   7]
 [267 614]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.54      0.92      0.68       106

avg / total       0.54      0.92      0.68       106

Test  F6: 0.898

['yes', 'no']
[[ 97   9]
 [ 83 134]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=315, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1292	develop
+0.1064	tissu
+0.0941	gene
+0.0769	genotyp
+0.0744	mutant
+0.0744	embryo
+0.0733	heart
+0.0698	overal
+0.0697	wild
+0.0678	design
+0.0636	dissect
+0.0612	mice
+0.0602	insight
+0.0596	brain
+0.0594	adult
+0.0582	litterm
+0.0577	seq
+0.0569	male
+0.0565	liver
+0.0527	kidney

### Top negative features (20)
-0.0483	primari
-0.0487	c2c12
-0.0491	stimul
-0.0552	pre
-0.0561	cultur
-0.0578	fac
-0.0597	follow
-0.0599	transgen
-0.0689	sort
-0.0699	activ
-0.0742	stem
-0.0755	treatment
-0.0775	infect
-0.0777	respons
-0.0781	differenti
-0.0853	fibroblast
-0.0933	treat
-0.1049	mef
-0.1395	induc
-0.2894	cell

### Vectorizer:   Number of Features: 4341
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lesion', u'let', u'lethal', u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukemogenesi', u'leukocyt', u'level']

Last 10 features: [u'zebrafish', u'zero', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich']

### False positives: 83
E-GEOD-3700
E-GEOD-62210
E-TIGR-109
E-GEOD-9545
E-GEOD-2882

### False negatives: 9
E-ERAD-169
E-GEOD-34210
E-ERAD-520
E-ERAD-352
E-ERAD-72

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           411	Yes count:       106
No  count:     1098	No  count:           881	No  count:       217
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Wed Sep 20 12:48:46 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 12:49:09 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 6
Random Seeds:	randForClassifier=315	randForSplit=716	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.72      0.95      0.82       399

avg / total       0.72      0.95      0.82       399

Train F6: 0.942

['yes', 'no']
[[379  20]
 [151 742]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.86      0.74       118

avg / total       0.65      0.86      0.74       118

Test  F6: 0.857

['yes', 'no']
[[102  16]
 [ 56 149]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=315, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1144	develop
+0.0911	tissu
+0.0882	mutant
+0.0849	wild
+0.0802	mice
+0.0755	embryo
+0.0748	dissect
+0.0633	pool
+0.0619	differ
+0.0617	overal
+0.0595	litterm
+0.0593	genotyp
+0.0585	male
+0.0556	liver
+0.0556	ko
+0.0548	brain
+0.0543	design
+0.0532	heart
+0.0532	seq
+0.0530	femal

### Top negative features (20)
-0.0505	stimul
-0.0528	primari
-0.0533	vitro
-0.0577	c2c12
-0.0589	fac
-0.0600	sort
-0.0657	respons
-0.0675	stem
-0.0699	transgen
-0.0716	infect
-0.0719	differenti
-0.0732	cultur
-0.0742	follow
-0.0754	activ
-0.0764	treatment
-0.0997	mef
-0.1079	fibroblast
-0.1198	treat
-0.1213	induc
-0.2979	cell

### Vectorizer:   Number of Features: 4414
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'leptin', u'lesion', u'let', u'lethal', u'leucin']

Last 10 features: [u'young', u'yy1', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 56
E-GEOD-30140
E-BAIR-1
E-GEOD-8128
E-MTAB-2777
E-BAIR-3

### False negatives: 16
E-GEOD-51960
E-GEOD-11484
E-GEOD-61582
E-GEOD-11040
E-GEOD-27451

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           399	Yes count:       118
No  count:     1098	No  count:           893	No  count:       205
Percent Yes:    32%	Percent Yes:         30%	Percent Yes:     36%
### End Time Wed Sep 20 12:49:10 2017

### Start Time Tue Sep 19 19:50:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=441	randForSplit=609	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.98      0.78       407

avg / total       0.65      0.98      0.78       407

Train F2: 0.890

['yes', 'no']
[[400   7]
 [218 667]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.95      0.74       110

avg / total       0.60      0.95      0.74       110

Test  F2: 0.855

['yes', 'no']
[[105   5]
 [ 69 144]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=441, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[0.001, 0.01, 0.1, 1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.1028	develop
+0.0917	knockout
+0.0726	tissu
+0.0708	mous
+0.0641	mutant
+0.0633	liver
+0.0549	genotyp
+0.0548	overal
+0.0546	design
+0.0532	defect
+0.0501	brain
+0.0477	gene
+0.0472	litterm
+0.0469	development
+0.0459	exhibit
+0.0458	project
+0.0458	correspond
+0.0457	pool
+0.0456	dnase
+0.0448	dissect

### Top negative features (20)
-0.0482	line
-0.0483	fac
-0.0503	untreat
-0.0507	unclear
-0.0512	c2c12
-0.0513	infect
-0.0529	hour
-0.0537	follow
-0.0540	ani
-0.0543	inject
-0.0557	transgen
-0.0580	sort
-0.0586	cultur
-0.0620	primari
-0.0694	stem
-0.1089	treat
-0.1152	induc
-0.1213	fibroblast
-0.1354	mef
-0.1720	cell

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a10', u'a2', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'lcm', u'lcmv', u'ld', u'ldp', u'lead', u'lean', u'learn', u'leav', u'lectin', u'led']

Last 10 features: [u'yy1', u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 69
E-MTAB-4079
E-GEOD-6223
E-GEOD-70819
E-GEOD-60596
E-ERAD-201

### False negatives: 5
E-GEOD-43517
E-ERAD-72
E-MTAB-3707
E-ERAD-169
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           407	Yes count:       110
No  count:     1098	No  count:           885	No  count:       213
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     34%
### End Time Tue Sep 19 19:51:02 2017
### End Time Tue Sep 19 19:54:37 2017

Fitting 5 folds for each of 108 candidates, totalling 540 fits
### Start Time Tue Sep 19 19:57:19 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=155	randForSplit=934	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.98      0.89       403

avg / total       0.81      0.98      0.89       403

Train F2: 0.942

['yes', 'no']
[[396   7]
 [ 95 794]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.62      0.92      0.74       114

avg / total       0.62      0.92      0.74       114

Test  F2: 0.840

['yes', 'no']
[[105   9]
 [ 64 145]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-05
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=155, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-05, 0.0001, 0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2', 'l1']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0774	develop
+0.0658	embryon day
+0.0641	tissu
+0.0625	mutant
+0.0612	pool
+0.0611	liver
+0.0522	dissect
+0.0516	knockout
+0.0508	mous heart
+0.0508	genotyp
+0.0507	male
+0.0496	kidney
+0.0495	brain
+0.0494	postnat
+0.0494	development
+0.0493	mice studi
+0.0488	litterm
+0.0469	correspond
+0.0465	femal
+0.0463	profil mous

### Top negative features (20)
-0.0487	cell treat
-0.0498	inject
-0.0509	hour
-0.0528	transgen
-0.0528	embryon
-0.0530	unclear
-0.0560	primari
-0.0564	treatment
-0.0577	follow
-0.0602	sort
-0.0780	stem
-0.0789	stem cell
-0.0927	cell
-0.0983	induc
-0.1031	treat
-0.1088	fibroblast
-0.1112	fibroblast mef
-0.1187	mous embryon
-0.1200	embryon fibroblast
-0.1251	mef

### Vectorizer:   Number of Features: 16857
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr', u'aberr gene', u'aberr transcript']

Middle 10 features: [u'krt14 cre', u'kruppel', u'kruppel like', u'ksom', u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 64
E-GEOD-1482
E-GEOD-13691
E-GEOD-769
E-GEOD-2130
E-GEOD-66088

### False negatives: 9
E-GEOD-61367
E-GEOD-9913
E-GEOD-59777
E-ERAD-433
E-GEOD-45278

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 19:58:45 2017

Fitting 5 folds for each of 54 candidates, totalling 270 fits
### Start Time Tue Sep 19 20:03:01 2017
NOTE: BEST
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=440	randForSplit=448	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.82      0.98      0.89       419

avg / total       0.82      0.98      0.89       419

Train F2: 0.946

['yes', 'no']
[[412   7]
 [ 90 783]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.65      0.90      0.75        98

avg / total       0.65      0.90      0.75        98

Test  F2: 0.833

['yes', 'no']
[[ 88  10]
 [ 48 177]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 1e-06
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=1e-06, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=440, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1, 1]
classifier__eta0:[1e-06, 1e-05, 0.0001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1), (1, 2)]

### Top positive features (20)
+0.0889	develop
+0.0712	tissu
+0.0644	mutant
+0.0578	pool
+0.0535	postnat
+0.0525	mutant embryo
+0.0523	brain
+0.0515	genotyp
+0.0498	liver
+0.0497	dissect
+0.0486	extens
+0.0484	embryon day
+0.0476	design
+0.0471	kidney
+0.0459	e12
+0.0457	defici mice
+0.0452	litterm
+0.0448	development
+0.0442	knockout
+0.0441	wild type

### Top negative features (20)
-0.0482	activ
-0.0483	inject
-0.0508	untreat
-0.0510	transgen
-0.0512	cell treat
-0.0517	sort
-0.0552	unclear
-0.0585	treatment
-0.0590	stem
-0.0602	stem cell
-0.0603	primari
-0.0637	follow
-0.0982	induc
-0.1024	embryon fibroblast
-0.1043	treat
-0.1107	fibroblast mef
-0.1202	fibroblast
-0.1203	mous embryon
-0.1372	mef
-0.1613	cell

### Vectorizer:   Number of Features: 16902
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'ksr', u'ksr medium', u'kurimoto', u'kurimoto et', u'l1', u'l1 adipocyt', u'l5178i', u'l5178i cell', u'la', u'la jolla']

Last 10 features: [u'zipper', u'zona', u'zone', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot']

### False positives: 48
E-MARS-18
E-GEOD-3621
E-GEOD-54678
E-GEOD-57990
E-MTAB-3106

### False negatives: 10
E-GEOD-21860
E-GEOD-22125
E-MTAB-1404
E-ERAD-283
E-GEOD-7342

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:03:45 2017

### End Time Tue Sep 19 20:08:24 2017
### End Time Tue Sep 19 20:12:42 2017

Fitting 5 folds for each of 40 candidates, totalling 200 fits
### Start Time Tue Sep 19 20:13:22 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=308	randForSplit=566	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.80      0.96      0.87       420

avg / total       0.80      0.96      0.87       420

Train F2: 0.926

['yes', 'no']
[[405  15]
 [101 771]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.89      0.76        97

avg / total       0.66      0.89      0.76        97

Test  F2: 0.830

['yes', 'no']
[[ 86  11]
 [ 44 182]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=308, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05, 0.0001, 0.001, 0.01]
classifier__eta0:[1e-06, 1e-05, 0.0001, 0.001, 0.01]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3644	develop
+0.3073	tissu
+0.2563	liver
+0.2430	mutant
+0.2406	heart
+0.2390	postnat
+0.2240	knockout
+0.2125	litterm
+0.2048	kidney
+0.2047	dissect
+0.2019	development
+0.1998	wild
+0.1925	design
+0.1863	overal
+0.1841	e12
+0.1787	consist
+0.1724	bud
+0.1669	pool
+0.1662	genotyp
+0.1621	box

### Top negative features (20)
-0.1679	myoblast
-0.1920	treatment
-0.1925	stimul
-0.1960	infect
-0.2020	fac
-0.2047	unclear
-0.2064	differenti
-0.2181	activ
-0.2271	c2c12
-0.2337	inject
-0.2361	primari
-0.2579	treat
-0.2590	sort
-0.2698	transgen
-0.2764	follow
-0.2922	stem
-0.3963	induc
-0.4142	cell
-0.5183	fibroblast
-0.5604	mef

### Vectorizer:   Number of Features: 4417
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'ablat', u'abnorm']

Middle 10 features: [u'life', u'lifespan', u'lifetim', u'ligand', u'ligas', u'ligat', u'light', u'like', u'likewis', u'lim']

Last 10 features: [u'zero', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 44
E-GEOD-13691
E-GEOD-60780
E-GEOD-59848
E-GEOD-59463
E-MTAB-1569

### False negatives: 11
E-GEOD-59777
E-MTAB-5020
E-ERAD-237
E-GEOD-8969
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           420	Yes count:        97
No  count:     1098	No  count:           872	No  count:       226
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Tue Sep 19 20:13:41 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:15:31 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=728	randForSplit=842	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.60      0.98      0.74       403

avg / total       0.60      0.98      0.74       403

Train F2: 0.867

['yes', 'no']
[[393  10]
 [261 628]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.99      0.73       114

avg / total       0.58      0.99      0.73       114

Test  F2: 0.869

['yes', 'no']
[[113   1]
 [ 81 128]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=728, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1071	develop
+0.0769	tissu
+0.0738	mutant
+0.0695	knockout
+0.0644	defect
+0.0641	heart
+0.0625	exhibit
+0.0607	postnat
+0.0597	design
+0.0581	liver
+0.0575	kidney
+0.0566	conclus
+0.0560	pool
+0.0553	litterm
+0.0539	e12
+0.0536	overal
+0.0535	gene
+0.0532	genotyp
+0.0499	wild
+0.0493	development

### Top negative features (20)
-0.0441	fac
-0.0443	treatment
-0.0445	activ
-0.0455	hour
-0.0465	c2c12
-0.0486	infect
-0.0487	line
-0.0522	unclear
-0.0526	treat
-0.0552	transgen
-0.0567	primari
-0.0588	inject
-0.0608	sort
-0.0621	respons
-0.0770	follow
-0.0789	stem
-0.0955	cell
-0.1010	induc
-0.1263	fibroblast
-0.1423	mef

### Vectorizer:   Number of Features: 4349
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lead', u'lean', u'learn', u'leav', u'led', u'lef', u'left', u'leica', u'len', u'length']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 81
E-MTAB-2266
E-GEOD-12309
E-ERAD-201
E-GEOD-29632
E-CBIL-39

### False negatives: 1
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           403	Yes count:       114
No  count:     1098	No  count:           889	No  count:       209
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:15:37 2017

Interesting 60% precision, high recall
Try some more repetitions
Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:17:59 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=492	randForSplit=650	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.68      0.96      0.79       401

avg / total       0.68      0.96      0.79       401

Train F2: 0.884

['yes', 'no']
[[383  18]
 [180 711]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       116

avg / total       0.61      0.89      0.72       116

Test  F2: 0.814

['yes', 'no']
[[103  13]
 [ 66 141]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=492, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0987	develop
+0.0793	tissu
+0.0739	mutant
+0.0634	liver
+0.0617	mice
+0.0572	exhibit
+0.0563	genotyp
+0.0548	knockout
+0.0546	brain
+0.0542	appar
+0.0542	postnat
+0.0533	gene
+0.0520	conclus
+0.0515	development
+0.0512	litter
+0.0511	litterm
+0.0506	dissect
+0.0504	wild
+0.0491	defect
+0.0480	correspond

### Top negative features (20)
-0.0474	myoblast
-0.0497	untreat
-0.0500	fac
-0.0522	c2c12
-0.0523	hour
-0.0524	line
-0.0538	transgen
-0.0546	unclear
-0.0558	activ
-0.0582	follow
-0.0591	inject
-0.0606	sort
-0.0634	primari
-0.0647	ani
-0.0851	stem
-0.1005	cell
-0.1084	treat
-0.1116	induc
-0.1320	mef
-0.1414	fibroblast

### Vectorizer:   Number of Features: 4380
First 10 features: [u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat']

Middle 10 features: [u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukocyt', u'level', u'leverag', u'leydig', u'li', u'libitum']

Last 10 features: [u'zero', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 66
E-GEOD-20570
E-GEOD-62758
E-GEOD-57133
E-TABM-304
E-GEOD-55247

### False negatives: 13
E-ERAD-71
E-GEOD-8360
E-GEOD-15795
E-ERAD-381
E-GEOD-15794

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           401	Yes count:       116
No  count:     1098	No  count:           891	No  count:       207
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Tue Sep 19 20:18:05 2017

Fitting 5 folds for each of 12 candidates, totalling 60 fits
### Start Time Tue Sep 19 20:18:58 2017
Data dir: /Users/jak/work/gxd_htLearning/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=362	randForSplit=345	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.76      0.98      0.86       409

avg / total       0.76      0.98      0.86       409

Train F2: 0.926

['yes', 'no']
[[401   8]
 [128 755]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.64      0.90      0.75       108

avg / total       0.64      0.90      0.75       108

Test  F2: 0.830

['yes', 'no']
[[ 97  11]
 [ 55 160]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=362, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.3412	develop
+0.2805	tissu
+0.2515	liver
+0.2455	mutant
+0.2253	knockout
+0.2087	moe430
+0.2085	male
+0.2082	genotyp
+0.2067	postnat
+0.2031	litterm
+0.1961	pool
+0.1926	appar
+0.1916	kidney
+0.1912	dissect
+0.1888	femal
+0.1877	embryo
+0.1829	brain
+0.1776	development
+0.1678	gene
+0.1619	heart

### Top negative features (20)
-0.1963	myoblast
-0.1989	infect
-0.2000	treatment
-0.2001	unclear
-0.2026	embryon
-0.2031	differenti
-0.2040	fac
-0.2089	c2c12
-0.2458	cultur
-0.2464	inject
-0.2467	sort
-0.2503	transgen
-0.2544	treat
-0.2597	follow
-0.2740	primari
-0.3155	stem
-0.3595	cell
-0.4668	induc
-0.4799	fibroblast
-0.5304	mef

### Vectorizer:   Number of Features: 4312
First 10 features: [u'a1', u'a2', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin', u'lesion', u'let', u'lethal', u'leucin', u'leukaemia']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 55
E-MTAB-4349
E-GEOD-42389
E-GEOD-74317
E-GEOD-54924
E-MTAB-1569

### False negatives: 11
E-ERAD-278
E-GEOD-45278
E-TABM-280
E-ERAD-272
E-MTAB-3976

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Tue Sep 19 20:19:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:25:43 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=203	randForSplit=241	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.79      0.97      0.87       408

avg / total       0.79      0.97      0.87       408

Train F2: 0.924

['yes', 'no']
[[394  14]
 [106 778]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.67      0.87      0.76       109

avg / total       0.67      0.87      0.76       109

Test  F2: 0.823

['yes', 'no']
[[ 95  14]
 [ 46 168]]

### Best Pipeline Parameters:
classifier__alpha: 0.001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=203, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:25:44 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:08 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=50	randForSplit=953	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.83      0.97      0.89       426

avg / total       0.83      0.97      0.89       426

Train F2: 0.935

['yes', 'no']
[[412  14]
 [ 87 779]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.59      0.90      0.71        91

avg / total       0.59      0.90      0.71        91

Test  F2: 0.813

['yes', 'no']
[[ 82   9]
 [ 58 174]]

### Best Pipeline Parameters:
classifier__alpha: 0.0001
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=50, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.0001]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:09 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:26:33 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=614	randForSplit=50	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.81      0.97      0.88       415

avg / total       0.81      0.97      0.88       415

Train F2: 0.931

['yes', 'no']
[[401  14]
 [ 93 784]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.66      0.81      0.73       102

avg / total       0.66      0.81      0.73       102

Test  F2: 0.779

['yes', 'no']
[[ 83  19]
 [ 42 179]]

### Best Pipeline Parameters:
classifier__alpha: 1e-05
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1e-05, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=614, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1e-05]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:26:34 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Wed Sep 20 11:27:34 2017
NOTE: Looking Good
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=799	randForSplit=847	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.78      0.97      0.87       406

avg / total       0.78      0.97      0.87       406

Train F2: 0.928

['yes', 'no']
[[395  11]
 [109 777]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.71      0.93      0.80       111

avg / total       0.71      0.93      0.80       111

Test  F2: 0.874

['yes', 'no']
[[103   8]
 [ 42 170]]

### Best Pipeline Parameters:
classifier__alpha: 0.01
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=799, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:27:35 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Wed Sep 20 11:28:00 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=428	randForSplit=34	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.95      0.80       415

avg / total       0.69      0.95      0.80       415

Train F2: 0.888

['yes', 'no']
[[396  19]
 [174 703]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.58      0.93      0.71       102

avg / total       0.58      0.93      0.71       102

Test  F2: 0.829

['yes', 'no']
[[ 95   7]
 [ 70 151]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=428, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:28:02 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### End Time Wed Sep 20 11:28:40 2017
### End Time Wed Sep 20 11:34:22 2017

Fitting 5 folds for each of 3 candidates, totalling 15 fits
### Start Time Wed Sep 20 11:34:50 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_noencode,	Beta: 2
Random Seeds:	randForClassifier=47	randForSplit=91	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.97      0.78       408

avg / total       0.65      0.97      0.78       408

Train F2: 0.886

['yes', 'no']
[[397  11]
 [212 672]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.61      0.89      0.72       109

avg / total       0.61      0.89      0.72       109

Test  F2: 0.815

['yes', 'no']
[[ 97  12]
 [ 62 152]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=47, shuffle=True, verbose=0,
       warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
CountVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents='unicode', token_pattern=u'(?i)\\b([a-z_]\\w+)\\b',
        tokenizer=None, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant', 'optimal', 'invscaling']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### End Time Wed Sep 20 11:34:52 2017
### End Time Wed Sep 20 11:42:38 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:21:47 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=510	randForSplit=627	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.65      0.98      0.78       419

avg / total       0.65      0.98      0.78       419

Train F4: 0.952

['yes', 'no']
[[411   8]
 [225 648]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.53      0.93      0.67        98

avg / total       0.53      0.93      0.67        98

Test  F4: 0.889

['yes', 'no']
[[ 91   7]
 [ 82 143]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=510, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1354	develop
+0.0890	liver
+0.0867	mutant
+0.0827	tissu
+0.0767	organism_part_ef
+0.0741	dissect
+0.0686	design
+0.0680	embryo
+0.0672	overal
+0.0648	mous
+0.0625	kidney
+0.0615	age_ef
+0.0613	hybrid
+0.0612	genotyp
+0.0612	litterm
+0.0607	wild
+0.0597	mice
+0.0580	male
+0.0561	heart
+0.0554	limb

### Top negative features (20)
-0.0533	c2c12
-0.0551	sort
-0.0576	fac
-0.0588	transgen
-0.0603	cultur
-0.0618	respons
-0.0619	pre
-0.0658	follow
-0.0661	stem
-0.0674	differenti
-0.0732	treatment
-0.0767	infect
-0.0768	cell_type_ef
-0.0794	compound_ef
-0.0999	treat
-0.1043	mef
-0.1069	time_ef
-0.1130	fibroblast
-0.1298	induc
-0.2561	cell

### Vectorizer:   Number of Features: 4458
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'lcmv', u'ldlr', u'ldp', u'lead', u'lean', u'learn', u'leav', u'led', u'lef', u'left']

Last 10 features: [u'zebrafish', u'zero', u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 82
E-GEOD-25072
E-GEOD-14424
E-GEOD-5332
E-GEOD-36336
E-MTAB-2520

### False negatives: 7
E-CBIL-23
E-ERAD-169
E-ERAD-520
E-MTAB-3321
E-GEOD-61582

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           419	Yes count:        98
No  count:     1098	No  count:           873	No  count:       225
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     30%
### End Time Thu Sep 21 10:21:49 2017

Fitting 5 folds for each of 2 candidates, totalling 10 fits
### Start Time Thu Sep 21 10:22:55 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=240	randForSplit=801	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.98      0.81       425

avg / total       0.69      0.98      0.81       425

Train F4: 0.953

['yes', 'no']
[[415  10]
 [190 677]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.52      0.92      0.67        92

avg / total       0.52      0.92      0.67        92

Test  F4: 0.884

['yes', 'no']
[[ 85   7]
 [ 77 154]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.01
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=240, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.01, 0.1]
classifier__eta0:[0.01]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.1171	develop
+0.0762	mice
+0.0747	seq
+0.0735	heart
+0.0730	tissu
+0.0729	dissect
+0.0726	age_ef
+0.0721	organism_part_ef
+0.0719	male
+0.0712	mutant
+0.0684	liver
+0.0641	embryo
+0.0605	design
+0.0601	wild
+0.0575	genotyp
+0.0570	femal
+0.0565	affymetrix
+0.0546	e12
+0.0545	testi
+0.0531	brain

### Top negative features (20)
-0.0564	dose_ef
-0.0579	treatment_ef
-0.0607	follow
-0.0633	activ
-0.0638	stem
-0.0639	c2c12
-0.0665	fac
-0.0668	treatment
-0.0688	respons
-0.0770	infect
-0.0779	cell_type_ef
-0.0882	compound_ef
-0.0915	differenti
-0.0933	cultur
-0.0986	treat
-0.1049	mef
-0.1079	fibroblast
-0.1098	time_ef
-0.1126	induc
-0.2707	cell

### Vectorizer:   Number of Features: 4506
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'lectin', u'led', u'left', u'len', u'length', u'lens', u'lentivir', u'lentivirus', u'lep', u'leptin']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 77
E-GEOD-13031
E-GEOD-22307
E-RZPD-1
E-GEOD-11400
E-TIGR-105

### False negatives: 7
E-GEOD-32511
E-GEOD-51932
E-GEOD-15795
E-GEOD-12618
E-GEOD-6589

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           425	Yes count:        92
No  count:     1098	No  count:           867	No  count:       231
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     28%
### End Time Thu Sep 21 10:22:57 2017

Fitting 5 folds for each of 24 candidates, totalling 120 fits
### Start Time Thu Sep 21 10:24:37 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=678	randForSplit=928	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.58      0.98      0.73       398

avg / total       0.58      0.98      0.73       398

Train F4: 0.947

['yes', 'no']
[[392   6]
 [279 615]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.60      0.97      0.74       119

avg / total       0.60      0.97      0.74       119

Test  F4: 0.940

['yes', 'no']
[[116   3]
 [ 78 126]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=678, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1, 1]
classifier__eta0:[0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0687	develop
+0.0437	tissu
+0.0422	mutant
+0.0371	age_ef
+0.0366	gene
+0.0349	dissect
+0.0343	male
+0.0335	stage
+0.0329	wild
+0.0328	organism_part_ef
+0.0324	overal
+0.0321	mice
+0.0321	specif
+0.0307	liver
+0.0302	embryo
+0.0296	genet
+0.0294	brain
+0.0283	heart
+0.0278	testi
+0.0274	postnat

### Top negative features (20)
-0.0271	pre
-0.0275	sort
-0.0279	transgen
-0.0290	differenti
-0.0317	fac
-0.0321	follow
-0.0341	activ
-0.0348	stem
-0.0359	cultur
-0.0362	respons
-0.0374	infect
-0.0396	compound_ef
-0.0396	treatment
-0.0408	cell_type_ef
-0.0414	fibroblast
-0.0484	treat
-0.0495	mef
-0.0502	time_ef
-0.0609	induc
-0.1368	cell

### Vectorizer:   Number of Features: 4427
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'leukaemia', u'leukem', u'leukemia', u'leukemogenesi', u'leukocyt', u'level', u'leverag', u'li', u'libitum', u'librari']

Last 10 features: [u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 78
E-GEOD-6046
E-GEOD-42389
E-GEOD-46686
E-GEOD-60318
E-GEOD-34069

### False negatives: 3
E-GEOD-61582
E-GEOD-50119
E-GEOD-10246

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           398	Yes count:       119
No  count:     1098	No  count:           894	No  count:       204
Percent Yes:    32%	Percent Yes:         30%	Percent Yes:     36%
### End Time Thu Sep 21 10:24:49 2017

Fitting 5 folds for each of 32 candidates, totalling 160 fits
### Start Time Thu Sep 21 10:26:06 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=805	randForSplit=533	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.61      0.98      0.75       409

avg / total       0.61      0.98      0.75       409

Train F4: 0.944

['yes', 'no']
[[400   9]
 [258 625]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.56      0.97      0.71       108

avg / total       0.56      0.97      0.71       108

Test  F4: 0.931

['yes', 'no']
[[105   3]
 [ 84 131]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=805, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0171	develop
+0.0129	wild
+0.0104	tissu
+0.0091	mice
+0.0087	heart
+0.0087	age_ef
+0.0083	organism_part_ef
+0.0082	dissect
+0.0076	gene
+0.0075	male
+0.0075	kidney
+0.0070	embryo
+0.0070	type
+0.0069	liver
+0.0068	hybrid
+0.0067	brain
+0.0067	design
+0.0066	mous
+0.0066	genotyp
+0.0066	pool

### Top negative features (20)
-0.0058	primari
-0.0058	follow
-0.0059	fac
-0.0061	transgen
-0.0065	activ
-0.0067	c2c12
-0.0068	pre
-0.0069	dose_ef
-0.0081	infect
-0.0082	differenti
-0.0082	cell_type_ef
-0.0082	stem
-0.0100	treatment
-0.0104	treat
-0.0104	compound_ef
-0.0104	time_ef
-0.0117	mef
-0.0118	fibroblast
-0.0139	induc
-0.0301	cell

### Vectorizer:   Number of Features: 4430
First 10 features: [u'a2', u'aa', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat', u'abnorm']

Middle 10 features: [u'lethal', u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukemogenesi', u'leukocyt', u'level', u'leverag', u'lh']

Last 10 features: [u'yolk', u'young', u'yy1', u'zebrafish', u'zinc', u'zn', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 84
E-GEOD-591
E-GEOD-2039
E-GEOD-6223
E-GEOD-15318
E-GEOD-60596

### False negatives: 3
E-ERAD-283
E-GEOD-34210
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Thu Sep 21 10:26:20 2017

Fitting 5 folds for each of 32 candidates, totalling 160 fits
### Start Time Thu Sep 21 10:30:02 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=481	randForSplit=371	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.99      0.81       415

avg / total       0.69      0.99      0.81       415

Train F4: 0.961

['yes', 'no']
[[409   6]
 [185 692]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.55      0.92      0.69       102

avg / total       0.55      0.92      0.69       102

Test  F4: 0.886

['yes', 'no']
[[ 94   8]
 [ 78 143]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=481, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 2)]

### Top positive features (20)
+0.0165	develop
+0.0106	tissu
+0.0105	age_ef
+0.0102	wild
+0.0101	wild type
+0.0098	mutant
+0.0096	organism_part_ef
+0.0090	mice
+0.0083	dissect
+0.0083	seq
+0.0079	embryo
+0.0076	liver
+0.0074	rna seq
+0.0073	design
+0.0070	genotyp
+0.0070	genotype_ef
+0.0070	gene
+0.0065	testi
+0.0065	litterm
+0.0065	male

### Top negative features (20)
-0.0067	c2c12
-0.0071	treatment_ef
-0.0072	follow
-0.0072	activ
-0.0073	sort
-0.0076	respons
-0.0084	infect
-0.0085	stem cell
-0.0091	compound_ef
-0.0092	mous embryon
-0.0093	stem
-0.0093	treatment
-0.0094	embryon fibroblast
-0.0096	treat
-0.0098	cell_type_ef
-0.0113	time_ef
-0.0116	mef
-0.0122	fibroblast
-0.0134	induc
-0.0302	cell

### Vectorizer:   Number of Features: 17451
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'aad', u'abdomin', u'abdomin obes']

Middle 10 features: [u'ko vs', u'ko wild', u'ko wt', u'kras', u'krasg12d', u'krasg12d activ', u'krt14', u'krt14 cre', u'kruppel', u'kruppel like']

Last 10 features: [u'zone', u'zone neural', u'zone svz', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 78
E-MEXP-1018
E-MTAB-2561
E-ERAD-197
E-GEOD-5583
E-GEOD-49200

### False negatives: 8
E-MTAB-3321
E-MTAB-3707
E-ERAD-169
E-ERAD-283
E-CBIL-23

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           415	Yes count:       102
No  count:     1098	No  count:           877	No  count:       221
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     31%
### End Time Thu Sep 21 10:30:40 2017

Fitting 5 folds for each of 32 candidates, totalling 160 fits
### Start Time Thu Sep 21 10:35:52 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=876	randForSplit=852	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.61      0.97      0.75       411

avg / total       0.61      0.97      0.75       411

Train F4: 0.938

['yes', 'no']
[[399  12]
 [255 626]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.52      0.93      0.66       106

avg / total       0.52      0.93      0.66       106

Test  F4: 0.891

['yes', 'no']
[[ 99   7]
 [ 93 124]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=876, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.001, 0.01, 0.1, 1]
classifier__eta0:[0.0001, 0.001, 0.01, 0.1]
classifier__learning_rate:['constant', 'optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.0130	develop
+0.0123	wild
+0.0122	wild type
+0.0100	age_ef
+0.0096	tissu
+0.0094	mice
+0.0091	dissect
+0.0081	organism_part_ef
+0.0078	mous
+0.0077	heart
+0.0075	liver
+0.0074	mutant
+0.0074	design
+0.0073	type
+0.0071	genotype_ef
+0.0070	gene
+0.0069	overal
+0.0069	embryo
+0.0068	mammalian
+0.0065	overal design

### Top negative features (20)
-0.0063	fac
-0.0064	embryon fibroblast mef
-0.0067	fibroblast mef
-0.0068	activ
-0.0068	infect
-0.0069	follow
-0.0084	treatment
-0.0086	stem cell
-0.0090	stem
-0.0090	compound_ef
-0.0097	mous embryon fibroblast
-0.0100	embryon fibroblast
-0.0104	time_ef
-0.0105	mous embryon
-0.0108	cell_type_ef
-0.0124	treat
-0.0129	fibroblast
-0.0136	induc
-0.0156	mef
-0.0282	cell

### Vectorizer:   Number of Features: 25069
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'aa4 purifi cell', u'abdomin', u'abdomin obes', u'abdomin obes overlap', u'abdomin obes perturb']

Middle 10 features: [u'known essenti', u'known express', u'known function', u'known gene', u'known housekeep', u'known housekeep gene', u'known howev', u'known howev lack', u'known investig', u'known investig molecular']

Last 10 features: [u'zone neural', u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus coloni form', u'zooepidemicus strong', u'zooepidemicus strong suggest', u'zurich', u'zygot', u'zygot genom', u'zygot genom activ']

### False positives: 93
E-GEOD-64076
E-GEOD-29300
E-GEOD-10881
E-GEOD-15119
E-GEOD-69952

### False negatives: 7
E-GEOD-36026
E-GEOD-39524
E-GEOD-17141
E-ERAD-283
E-GEOD-15795

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           411	Yes count:       106
No  count:     1098	No  count:           881	No  count:       217
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     32%
### End Time Thu Sep 21 10:37:00 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:41:46 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=308	randForSplit=141	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.56      0.98      0.71       396

avg / total       0.56      0.98      0.71       396

Train F4: 0.942

['yes', 'no']
[[390   6]
 [309 587]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.57      0.98      0.72       121

avg / total       0.57      0.98      0.72       121

Test  F4: 0.944

['yes', 'no']
[[119   2]
 [ 89 113]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=308, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0761	develop
+0.0470	mutant
+0.0467	wild
+0.0432	tissu
+0.0391	design
+0.0388	embryo
+0.0355	overal
+0.0337	genotyp
+0.0336	mice
+0.0330	age_ef
+0.0329	dissect
+0.0320	type
+0.0315	organism_part_ef
+0.0309	affymetrix
+0.0305	mous
+0.0305	male
+0.0303	liver
+0.0301	genet
+0.0290	gene
+0.0289	stage

### Top negative features (20)
-0.0280	follow
-0.0292	sort
-0.0300	transgen
-0.0328	fac
-0.0329	pre
-0.0331	c2c12
-0.0335	differenti
-0.0344	infect
-0.0349	respons
-0.0354	stem
-0.0357	cultur
-0.0391	compound_ef
-0.0410	treatment
-0.0433	cell_type_ef
-0.0478	mef
-0.0492	time_ef
-0.0516	treat
-0.0528	fibroblast
-0.0678	induc
-0.1404	cell

### Vectorizer:   Number of Features: 4467
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'laser', u'late', u'latenc', u'later', u'lats2', u'lavag', u'layer', u'lc', u'lck', u'lcm']

Last 10 features: [u'young', u'yy1', u'zfp36', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 89
E-CBIL-37
E-GEOD-63272
E-SMDB-1853
E-GEOD-64004
E-GEOD-78701

### False negatives: 2
E-GEOD-20639
E-GEOD-61582

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           396	Yes count:       121
No  count:     1098	No  count:           896	No  count:       202
Percent Yes:    32%	Percent Yes:         30%	Percent Yes:     37%
### End Time Thu Sep 21 10:41:47 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:42:23 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=955	randForSplit=914	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.57      0.99      0.72       414

avg / total       0.57      0.99      0.72       414

Train F4: 0.945

['yes', 'no']
[[408   6]
 [305 573]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.50      0.98      0.66       103

avg / total       0.50      0.98      0.66       103

Test  F4: 0.928

['yes', 'no']
[[101   2]
 [101 119]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=955, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0628	develop
+0.0541	wild
+0.0459	mutant
+0.0416	mice
+0.0406	tissu
+0.0372	embryo
+0.0349	design
+0.0341	age_ef
+0.0327	overal
+0.0321	type
+0.0319	male
+0.0316	dissect
+0.0314	heart
+0.0313	affymetrix
+0.0309	transcript
+0.0306	organism_part_ef
+0.0299	genotyp
+0.0293	seq
+0.0291	liver
+0.0290	defici

### Top negative features (20)
-0.0303	pre
-0.0308	follow
-0.0312	fac
-0.0312	differenti
-0.0319	activ
-0.0328	transgen
-0.0334	c2c12
-0.0374	cultur
-0.0374	stem
-0.0385	treatment_ef
-0.0392	compound_ef
-0.0393	infect
-0.0409	cell_type_ef
-0.0458	treatment
-0.0468	mef
-0.0489	treat
-0.0503	time_ef
-0.0509	fibroblast
-0.0676	induc
-0.1395	cell

### Vectorizer:   Number of Features: 4536
First 10 features: [u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil', u'abl', u'abl1']

Middle 10 features: [u'langhorn', u'languag', u'larg', u'larger', u'largest', u'larva', u'laser', u'late', u'later', u'lats2']

Last 10 features: [u'yy1', u'zfp36', u'zinc', u'zipper', u'zn', u'zona', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 101
E-GEOD-60499
E-GEOD-25257
E-GEOD-3621
E-GEOD-528
E-GEOD-34907

### False negatives: 2
E-GEOD-20639
E-ERAD-520

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           414	Yes count:       103
No  count:     1098	No  count:           878	No  count:       220
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     31%
### End Time Thu Sep 21 10:42:24 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:42:51 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=856	randForSplit=909	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.59      0.98      0.74       414

avg / total       0.59      0.98      0.74       414

Train F4: 0.944

['yes', 'no']
[[406   8]
 [282 596]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.50      0.97      0.66       103

avg / total       0.50      0.97      0.66       103

Test  F4: 0.919

['yes', 'no']
[[100   3]
 [102 118]]

### Best Pipeline Parameters:
classifier__alpha: 0.1
classifier__eta0: 0.001
classifier__learning_rate: 'constant'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='constant', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=856, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[0.1]
classifier__eta0:[0.001]
classifier__learning_rate:['constant']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0769	develop
+0.0509	wild
+0.0472	tissu
+0.0434	mutant
+0.0422	embryo
+0.0384	overal
+0.0366	organism_part_ef
+0.0357	mice
+0.0350	age_ef
+0.0345	genotyp
+0.0324	dissect
+0.0316	gene
+0.0308	design
+0.0295	seq
+0.0293	male
+0.0292	liver
+0.0285	mous
+0.0284	brain
+0.0284	hybrid
+0.0273	heart

### Top negative features (20)
-0.0304	follow
-0.0307	transgen
-0.0308	pre
-0.0314	sort
-0.0332	c2c12
-0.0337	fac
-0.0339	respons
-0.0343	cultur
-0.0346	differenti
-0.0365	stem
-0.0389	treatment
-0.0393	infect
-0.0408	cell_type_ef
-0.0479	mef
-0.0485	compound_ef
-0.0487	treat
-0.0522	fibroblast
-0.0533	time_ef
-0.0633	induc
-0.1447	cell

### Vectorizer:   Number of Features: 4403
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'abdomin', u'aberr', u'abil', u'abl']

Middle 10 features: [u'lavag', u'layer', u'lc', u'lcm', u'lcmv', u'ld', u'ldp', u'lead', u'lean', u'learn']

Last 10 features: [u'yy1', u'zinc', u'zipper', u'zona', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 102
E-MEXP-2161
E-GEOD-28790
E-CBIL-16
E-GEOD-59739
E-GEOD-31115

### False negatives: 3
E-GEOD-12618
E-ERAD-520
E-GEOD-51932

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           414	Yes count:       103
No  count:     1098	No  count:           878	No  count:       220
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     31%
### End Time Thu Sep 21 10:42:52 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:44:24 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=521	randForSplit=170	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.54      0.99      0.70       415

avg / total       0.54      0.99      0.70       415

Train F4: 0.940

['yes', 'no']
[[409   6]
 [345 532]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.47      0.98      0.64       102

avg / total       0.47      0.98      0.64       102

Test  F4: 0.922

['yes', 'no']
[[100   2]
 [112 109]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=521, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0151	develop
+0.0103	mutant
+0.0094	tissu
+0.0091	mice
+0.0081	embryo
+0.0080	overal
+0.0080	organism_part_ef
+0.0079	design
+0.0079	heart
+0.0077	age_ef
+0.0075	wild
+0.0073	genotyp
+0.0069	pool
+0.0069	dissect
+0.0069	gene
+0.0069	genet
+0.0066	liver
+0.0065	male
+0.0063	litterm
+0.0062	affymetrix

### Top negative features (20)
-0.0055	activ
-0.0059	cultur
-0.0059	follow
-0.0059	fac
-0.0067	c2c12
-0.0067	transgen
-0.0071	stem
-0.0072	pre
-0.0080	infect
-0.0081	respons
-0.0081	differenti
-0.0082	cell_type_ef
-0.0083	compound_ef
-0.0085	treatment
-0.0102	time_ef
-0.0108	mef
-0.0109	treat
-0.0115	fibroblast
-0.0128	induc
-0.0300	cell

### Vectorizer:   Number of Features: 4536
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'aberr', u'abil']

Middle 10 features: [u'laser', u'late', u'latenc', u'latent', u'later', u'lavag', u'layer', u'lc', u'lcm', u'lcmv']

Last 10 features: [u'zfp36', u'zinc', u'zipper', u'zn', u'zone', u'zooepidemicus', u'zt20', u'zt6', u'zurich', u'zygot']

### False positives: 112
E-GEOD-5583
E-GEOD-23421
E-SMDB-3543
E-GEOD-5500
E-GEOD-62210

### False negatives: 2
E-ERAD-381
E-ERAD-283

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           415	Yes count:       102
No  count:     1098	No  count:           877	No  count:       221
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     31%
### End Time Thu Sep 21 10:44:25 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:44:42 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=794	randForSplit=533	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.55      0.99      0.70       409

avg / total       0.55      0.99      0.70       409

Train F4: 0.947

['yes', 'no']
[[406   3]
 [338 545]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.50      0.97      0.66       108

avg / total       0.50      0.97      0.66       108

Test  F4: 0.922

['yes', 'no']
[[105   3]
 [103 112]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 1)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=794, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 1)]

### Top positive features (20)
+0.0171	develop
+0.0129	wild
+0.0104	tissu
+0.0090	mice
+0.0087	heart
+0.0086	age_ef
+0.0083	organism_part_ef
+0.0081	dissect
+0.0075	gene
+0.0075	kidney
+0.0075	male
+0.0070	embryo
+0.0069	liver
+0.0069	type
+0.0068	hybrid
+0.0067	brain
+0.0066	design
+0.0066	genotyp
+0.0065	pool
+0.0065	mous

### Top negative features (20)
-0.0058	primari
-0.0058	follow
-0.0059	fac
-0.0062	transgen
-0.0066	activ
-0.0067	c2c12
-0.0068	pre
-0.0069	dose_ef
-0.0081	infect
-0.0082	cell_type_ef
-0.0082	differenti
-0.0082	stem
-0.0100	treatment
-0.0104	treat
-0.0104	compound_ef
-0.0105	time_ef
-0.0117	mef
-0.0118	fibroblast
-0.0139	induc
-0.0302	cell

### Vectorizer:   Number of Features: 4430
First 10 features: [u'a2', u'aa', u'aa4', u'abdomin', u'aberr', u'abil', u'abl', u'abl1', u'ablat', u'abnorm']

Middle 10 features: [u'lethal', u'leucin', u'leukaemia', u'leukem', u'leukemia', u'leukemogenesi', u'leukocyt', u'level', u'leverag', u'lh']

Last 10 features: [u'yolk', u'young', u'yy1', u'zebrafish', u'zinc', u'zn', u'zone', u'zooepidemicus', u'zurich', u'zygot']

### False positives: 103
E-GEOD-591
E-GEOD-21013
E-GEOD-2039
E-GEOD-4936
E-GEOD-6223

### False negatives: 3
E-ERAD-283
E-GEOD-34210
E-ERAD-352

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           409	Yes count:       108
No  count:     1098	No  count:           883	No  count:       215
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     33%
### End Time Thu Sep 21 10:44:43 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:45:21 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=580	randForSplit=16	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.63      0.98      0.76       425

avg / total       0.63      0.98      0.76       425

Train F4: 0.945

['yes', 'no']
[[415  10]
 [248 619]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.45      0.91      0.61        92

avg / total       0.45      0.91      0.61        92

Test  F4: 0.862

['yes', 'no']
[[ 84   8]
 [101 130]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 2)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=580, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 2)]

### Top positive features (20)
+0.0153	develop
+0.0107	wild
+0.0106	wild type
+0.0104	mice
+0.0103	mutant
+0.0093	age_ef
+0.0092	tissu
+0.0086	gene
+0.0085	design
+0.0083	overal
+0.0079	genotyp
+0.0078	male
+0.0077	liver
+0.0077	type
+0.0075	litterm
+0.0075	overal design
+0.0075	kidney
+0.0074	experi overal
+0.0071	organism_part_ef
+0.0070	heart

### Top negative features (20)
-0.0065	hour
-0.0066	fibroblast mef
-0.0067	respons
-0.0076	follow
-0.0077	c2c12
-0.0077	differenti
-0.0078	infect
-0.0085	compound_ef
-0.0093	treatment
-0.0093	embryon fibroblast
-0.0099	stem cell
-0.0104	cell_type_ef
-0.0106	mous embryon
-0.0108	stem
-0.0112	time_ef
-0.0114	treat
-0.0122	fibroblast
-0.0133	induc
-0.0140	mef
-0.0310	cell

### Vectorizer:   Number of Features: 17798
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aad', u'abdomin', u'abdomin obes', u'aberr']

Middle 10 features: [u'label reaction', u'label target', u'label use', u'labexpid_ef', u'labexpid_ef readtype_description_ef', u'laboratori', u'laboratori develop', u'laboratori direct', u'laboratori mous', u'laboratori understand']

Last 10 features: [u'zooepidemicus', u'zooepidemicus coloni', u'zooepidemicus strong', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zygot', u'zygot genom']

### False positives: 101
E-MTAB-2995
E-GEOD-6614
E-GEOD-33878
E-MEXP-1190
E-GEOD-15715

### False negatives: 8
E-GEOD-32511
E-ERAD-278
E-ERAD-272
E-ERAD-520
E-ERAD-237

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           425	Yes count:        92
No  count:     1098	No  count:           867	No  count:       231
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     28%
### End Time Thu Sep 21 10:45:24 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:46:01 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=817	randForSplit=892	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.69      0.97      0.81       421

avg / total       0.69      0.97      0.81       421

Train F4: 0.951

['yes', 'no']
[[410  11]
 [180 691]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.53      0.91      0.67        96

avg / total       0.53      0.91      0.67        96

Test  F4: 0.869

['yes', 'no']
[[ 87   9]
 [ 78 149]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=817, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.0158	develop
+0.0115	wild type
+0.0114	wild
+0.0099	mice
+0.0096	age_ef
+0.0090	tissu
+0.0085	gene
+0.0081	mutant
+0.0081	dissect
+0.0080	heart
+0.0075	organism_part_ef
+0.0073	genotype_ef
+0.0072	litterm
+0.0071	embryo
+0.0070	pool
+0.0068	liver
+0.0068	design
+0.0068	stage
+0.0066	affymetrix
+0.0065	brain

### Top negative features (20)
-0.0064	primari
-0.0066	follow
-0.0068	c2c12
-0.0069	activ
-0.0069	differenti
-0.0076	infect
-0.0085	stem cell
-0.0086	treatment
-0.0087	compound_ef
-0.0092	stem
-0.0093	mous embryon fibroblast
-0.0096	embryon fibroblast
-0.0101	mous embryon
-0.0101	time_ef
-0.0106	mef
-0.0107	cell_type_ef
-0.0123	treat
-0.0125	fibroblast
-0.0146	induc
-0.0291	cell

### Vectorizer:   Number of Features: 24795
First 10 features: [u'a1', u'a10', u'a2', u'a9', u'aa', u'aa4', u'aa4 purifi', u'aa4 purifi cell', u'abdomin', u'abdomin obes']

Middle 10 features: [u'landscap polycomb', u'landscap polycomb repress', u'lane', u'lane illumina', u'lane illumina genom', u'lane illumina hiseq', u'langerhan', u'langhorn', u'langhorn nimr', u'langhorn nimr data']

Last 10 features: [u'zone svz', u'zt20', u'zt6', u'zt6 zt20', u'zurich', u'zurich anim', u'zurich anim acclimat', u'zygot', u'zygot genom', u'zygot genom activ']

### False positives: 78
E-GEOD-69166
E-GEOD-54924
E-GEOD-60361
E-GEOD-13691
E-BAIR-3

### False negatives: 9
E-GEOD-32511
E-GEOD-61367
E-GEOD-9913
E-MTAB-3707
E-GEOD-45278

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           421	Yes count:        96
No  count:     1098	No  count:           871	No  count:       227
Percent Yes:    32%	Percent Yes:         32%	Percent Yes:     29%
### End Time Thu Sep 21 10:46:05 2017

Fitting 5 folds for each of 1 candidates, totalling 5 fits
### Start Time Thu Sep 21 10:46:25 2017
Data dir: /Users/jak/work/gxd_htLearning/Data/Data_expFactors,	Beta: 4
Random Seeds:	randForClassifier=337	randForSplit=888	
### Metrics: Training Set
             precision    recall  f1-score   support

  Train yes       0.59      0.98      0.74       402

avg / total       0.59      0.98      0.74       402

Train F4: 0.939

['yes', 'no']
[[392  10]
 [272 618]]

### Metrics: Test Set
             precision    recall  f1-score   support

  Test  yes       0.54      0.96      0.69       115

avg / total       0.54      0.96      0.69       115

Test  F4: 0.915

['yes', 'no']
[[110   5]
 [ 93 115]]

### Best Pipeline Parameters:
classifier__alpha: 1
classifier__eta0: 0.0001
classifier__learning_rate: 'optimal'
classifier__loss: 'log'
classifier__penalty: 'l2'
vectorizer__max_df: 0.98
vectorizer__min_df: 2
vectorizer__ngram_range: (1, 3)

### GridSearch Pipeline:
classifier:
SGDClassifier(alpha=1, average=False, class_weight='balanced', epsilon=0.1,
       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=337, shuffle=True,
       verbose=0, warm_start=False)

scaler:
MaxAbsScaler(copy=True)

vectorizer:
TfidfVectorizer(analyzer=u'word', binary=False, decode_error='replace',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.98, max_features=None, min_df=2,
        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words='english', strip_accents='unicode', sublinear_tf=False,
        token_pattern=u'(?i)\\b([a-z_]\\w+)\\b', tokenizer=None,
        use_idf=True, vocabulary=None)

### Parameter Options Tried:
classifier__alpha:[1]
classifier__eta0:[0.0001]
classifier__learning_rate:['optimal']
classifier__loss:['log']
classifier__penalty:['l2']
vectorizer__max_df:[0.98]
vectorizer__min_df:[2]
vectorizer__ngram_range:[(1, 3)]

### Top positive features (20)
+0.0145	develop
+0.0127	tissu
+0.0096	gene
+0.0093	wild
+0.0092	mice
+0.0091	wild type
+0.0080	dissect
+0.0078	affymetrix
+0.0076	overal
+0.0076	design
+0.0075	genotyp
+0.0075	liver
+0.0073	age_ef
+0.0072	heart
+0.0072	overal design
+0.0070	experi overal
+0.0070	experi overal design
+0.0070	organism_part_ef
+0.0070	mutant
+0.0066	embryo

### Top negative features (20)
-0.0064	fac
-0.0066	c2c12
-0.0067	activ
-0.0067	embryon fibroblast
-0.0072	differenti
-0.0076	infect
-0.0078	treatment_ef
-0.0080	pre
-0.0085	treatment
-0.0089	compound_ef
-0.0091	follow
-0.0091	fibroblast
-0.0095	stem cell
-0.0099	stem
-0.0100	cell_type_ef
-0.0106	time_ef
-0.0115	mef
-0.0125	treat
-0.0127	induc
-0.0335	cell

### Vectorizer:   Number of Features: 24836
First 10 features: [u'a10', u'a2', u'a9', u'aa', u'aa4', u'abdomin', u'abdomin obes', u'abdomin obes overlap', u'abdomin obes perturb', u'aberr']

Middle 10 features: [u'la jolla ca', u'lab', u'label', u'label accord', u'label accord standard', u'label affymetrix', u'label assay', u'label biotin', u'label cdna', u'label complementari']

Last 10 features: [u'zooepidemicus coloni', u'zooepidemicus coloni form', u'zooepidemicus strong', u'zooepidemicus strong suggest', u'zurich', u'zurich anim', u'zurich anim acclimat', u'zygot', u'zygot genom', u'zygot genom activ']

### False positives: 93
E-GEOD-68155
E-MTAB-2067
E-GEOD-5332
E-GEOD-15694
E-GEOD-38988

### False negatives: 5
E-ERAD-283
E-GEOD-8360
E-ERAD-352
E-ERAD-231
E-GEOD-61367

### Train Test Split Report, test % = 0.20
All Samples:   1615	Training Samples:   1292	Test Samples:    323
Yes count:      517	Yes count:           402	Yes count:       115
No  count:     1098	No  count:           890	No  count:       208
Percent Yes:    32%	Percent Yes:         31%	Percent Yes:     35%
### End Time Thu Sep 21 10:46:30 2017

