
# Definitions of good Pipelines to compare.
# Define a variable "pipelines" that is the list of Pipelines.

import sys
sys.path.append('..')
import numpy
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import StandardScaler, MaxAbsScaler
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC, LinearSVC
import sklearnHelperLib as ppLib 
#-----------------------

global pipelines
pipelines = [ \
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.00001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.0000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.00000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.000000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.0000000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.00000000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
    Pipeline( [
	('vectorizer', TfidfVectorizer( analyzer='word',
			strip_accents='unicode', decode_error='replace',
			token_pattern=u'(?i)\\b([a-z_]\w+)\\b',
			stop_words="english",
			max_df=0.98, min_df=2,
			ngram_range=(1,2),
			),
	),
	('scaler'    ,StandardScaler(copy=True,with_mean=False,with_std=True)),
	#('scaler'    , MaxAbsScaler(copy=True)),
	('classifier', LinearSVC(verbose=0,
			loss='hinge', penalty='l2',
			C=0.000000000001,
			max_iter=200,
			class_weight='balanced',) ),
	] ),
]
#-----------------------
